{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a basic PySpark application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup S3 bucket locations and roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sagemaker_logger = logging.getLogger(\"sagemaker\")\n",
    "sagemaker_logger.setLevel(logging.INFO)\n",
    "sagemaker_logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Running a basic PySpark application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-01-22 05:08:16--  https://s3-us-west-2.amazonaws.com/sparkml-mleap/data/abalone/abalone.csv\n",
      "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.236.168\n",
      "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.236.168|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 191873 (187K) [binary/octet-stream]\n",
      "Saving to: ‘./data/abalone.csv’\n",
      "\n",
      "./data/abalone.csv  100%[===================>] 187.38K   317KB/s    in 0.6s    \n",
      "\n",
      "2021-01-22 05:08:18 (317 KB/s) - ‘./data/abalone.csv’ saved [191873/191873]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fetch the dataset from the SageMaker bucket\n",
    "!mkdir -p ./data\n",
    "!wget https://s3-us-west-2.amazonaws.com/sparkml-mleap/data/abalone/abalone.csv -O ./data/abalone.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the PySpark script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./code/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./code/preprocess.py\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    OneHotEncoder,\n",
    "    StringIndexer,\n",
    "    VectorAssembler,\n",
    "    VectorIndexer,\n",
    ")\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "\n",
    "\n",
    "def csv_line(data):\n",
    "    r = ','.join(str(d) for d in data[1])\n",
    "    return str(data[0]) + \",\" + r\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n",
    "    parser.add_argument(\"--s3_input_bucket\", type=str, help=\"s3 input bucket\")\n",
    "    parser.add_argument(\"--s3_input_key_prefix\", type=str, help=\"s3 input key prefix\")\n",
    "    parser.add_argument(\"--s3_output_bucket\", type=str, help=\"s3 output bucket\")\n",
    "    parser.add_argument(\"--s3_output_key_prefix\", type=str, help=\"s3 output key prefix\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"PySparkApp\").getOrCreate()\n",
    "\n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\"mapred.output.committer.class\",\n",
    "                                                      \"org.apache.hadoop.mapred.FileOutputCommitter\")\n",
    "\n",
    "    # Defining the schema corresponding to the input data. The input data does not contain the headers\n",
    "    schema = StructType([StructField(\"sex\", StringType(), True),\n",
    "                         StructField(\"length\", DoubleType(), True),\n",
    "                         StructField(\"diameter\", DoubleType(), True),\n",
    "                         StructField(\"height\", DoubleType(), True),\n",
    "                         StructField(\"whole_weight\", DoubleType(), True),\n",
    "                         StructField(\"shucked_weight\", DoubleType(), True),\n",
    "                         StructField(\"viscera_weight\", DoubleType(), True),\n",
    "                         StructField(\"shell_weight\", DoubleType(), True),\n",
    "                         StructField(\"rings\", DoubleType(), True)])\n",
    "\n",
    "    # Downloading the data from S3 into a Dataframe\n",
    "    total_df = spark.read.csv(('s3://' + os.path.join(args.s3_input_bucket, args.s3_input_key_prefix,\n",
    "                                                   'abalone.csv')), header=False, schema=schema)\n",
    "\n",
    "    #StringIndexer on the sex column which has categorical value\n",
    "    sex_indexer = StringIndexer(inputCol=\"sex\", outputCol=\"indexed_sex\")\n",
    "\n",
    "    #one-hot-encoding is being performed on the string-indexed sex column (indexed_sex)\n",
    "    sex_encoder = OneHotEncoder(inputCol=\"indexed_sex\", outputCol=\"sex_vec\")\n",
    "\n",
    "    #vector-assembler will bring all the features to a 1D vector for us to save easily into CSV format\n",
    "    assembler = VectorAssembler(inputCols=[\"sex_vec\",\n",
    "                                           \"length\",\n",
    "                                           \"diameter\",\n",
    "                                           \"height\",\n",
    "                                           \"whole_weight\",\n",
    "                                           \"shucked_weight\",\n",
    "                                           \"viscera_weight\",\n",
    "                                           \"shell_weight\"],\n",
    "                                outputCol=\"features\")\n",
    "\n",
    "    # The pipeline comprises of the steps added above\n",
    "    pipeline = Pipeline(stages=[sex_indexer, sex_encoder, assembler])\n",
    "\n",
    "    # This step trains the feature transformers\n",
    "    model = pipeline.fit(total_df)\n",
    "\n",
    "    # This step transforms the dataset with information obtained from the previous fit\n",
    "    transformed_total_df = model.transform(total_df)\n",
    "\n",
    "    # Split the overall dataset into 80-20 training and validation\n",
    "    (train_df, validation_df) = transformed_total_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "    # Convert the train dataframe to RDD to save in CSV format and upload to S3\n",
    "    train_rdd = train_df.rdd.map(lambda x: (x.rings, x.features))\n",
    "    train_lines = train_rdd.map(csv_line)\n",
    "    train_lines.saveAsTextFile('s3://' + os.path.join(args.s3_output_bucket, args.s3_output_key_prefix, 'train'))\n",
    "\n",
    "    # Convert the validation dataframe to RDD to save in CSV format and upload to S3\n",
    "    validation_rdd = validation_df.rdd.map(lambda x: (x.rings, x.features))\n",
    "    validation_lines = validation_rdd.map(csv_line)\n",
    "    validation_lines.saveAsTextFile('s3://' + os.path.join(args.s3_output_bucket, args.s3_output_key_prefix, 'validation'))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the SageMaker Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating processing-job with name sm-spark-2021-01-22-05-11-54-716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sm-spark-2021-01-22-05-11-54-716\n",
      "Inputs:  [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-cn-north-1-876820548815/sm-spark-2021-01-22-05-11-54-716/input/code/preprocess.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'output-1', 'S3Output': {'S3Uri': 's3://sagemaker-cn-north-1-876820548815/sagemaker/spark-preprocess-demo/2021-01-22-05-11-54/spark_event_logs', 'LocalPath': '/opt/ml/processing/spark-events/', 'S3UploadMode': 'Continuous'}}]\n",
      "......................................................................!"
     ]
    }
   ],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "# Upload the raw input dataset to a unique S3 location\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "prefix = \"sagemaker/spark-preprocess-demo/{}\".format(timestamp_prefix)\n",
    "input_prefix_abalone = \"{}/input/raw/abalone\".format(prefix)\n",
    "input_preprocessed_prefix_abalone = \"{}/input/preprocessed/abalone\".format(prefix)\n",
    "\n",
    "sagemaker_session.upload_data(path='./data/abalone.csv', bucket=bucket, key_prefix=input_prefix_abalone)\n",
    "\n",
    "# Run the processing job\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark\",\n",
    "    framework_version=\"2.4\",\n",
    "    role=role,\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=1200,\n",
    ")\n",
    "\n",
    "spark_processor.run(\n",
    "    submit_app=\"./code/preprocess.py\",\n",
    "    arguments=[\"--s3_input_bucket\", bucket,\n",
    "               \"--s3_input_key_prefix\", input_prefix_abalone,\n",
    "               \"--s3_output_bucket\", bucket,\n",
    "               \"--s3_output_key_prefix\", input_preprocessed_prefix_abalone],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, prefix),\n",
    "    logs=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Data Processing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 rows from s3://sagemaker-cn-north-1-876820548815/sagemaker/spark-preprocess-demo/2021-01-22-05-11-54/input/preprocessed/abalone/train/\n",
      "2021-01-22 05:17:18          0 _SUCCESS\n",
      "2021-01-22 05:17:18     177556 part-00000\n",
      "5.0,0.0,0.0,0.275,0.195,0.07,0.08,0.031,0.0215,0.025\n",
      "6.0,0.0,0.0,0.29,0.21,0.075,0.275,0.113,0.0675,0.035\n",
      "5.0,0.0,0.0,0.29,0.225,0.075,0.14,0.0515,0.0235,0.04\n",
      "7.0,0.0,0.0,0.305,0.225,0.07,0.1485,0.0585,0.0335,0.045\n",
      "7.0,0.0,0.0,0.305,0.23,0.08,0.156,0.0675,0.0345,0.048\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 5 rows from s3://{}/{}/train/\".format(bucket, input_preprocessed_prefix_abalone))\n",
    "!aws s3 ls s3://$bucket/$input_preprocessed_prefix_abalone/train/\n",
    "\n",
    "!aws s3 cp --quiet s3://$bucket/$input_preprocessed_prefix_abalone/train/part-00000 - | head -n5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## View the Spark UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pulling spark history server image...\n",
      "docker command: docker pull 671472414489.dkr.ecr.cn-north-1.amazonaws.com.cn/sagemaker-spark-processing:2.4-cpu\n",
      "image pulled: 671472414489.dkr.ecr.cn-north-1.amazonaws.com.cn/sagemaker-spark-processing:2.4-cpu\n",
      "History server terminated\n",
      "Starting history server...\n",
      "History server failed to start. Please run 'docker logs history_server' to see logs\n"
     ]
    }
   ],
   "source": [
    "spark_processor.start_history_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_processor.terminate_history_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Specify additional python and jar file dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./code/hello_py_spark_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./code/hello_py_spark_app.py\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "# Import local module to test spark-submit--py-files dependencies\n",
    "import hello_py_spark_udfs as udfs\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Hello World, this is PySpark!\")\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"inputs and outputs\")\n",
    "    parser.add_argument(\"--input\", type=str, help=\"path to input data\")\n",
    "    parser.add_argument(\"--output\", required=False, type=str, help=\"path to output data\")\n",
    "    args = parser.parse_args()\n",
    "    spark = SparkSession.builder.appName(\"SparkTestApp\").getOrCreate()\n",
    "    sqlContext = SQLContext(spark.sparkContext)\n",
    "\n",
    "    # Load test data set\n",
    "    inputPath = args.input\n",
    "    outputPath = args.output\n",
    "    salesDF = spark.read.json(inputPath)\n",
    "    salesDF.printSchema()\n",
    "\n",
    "    salesDF.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "    # Define a UDF that doubles an integer column\n",
    "    # The UDF function is imported from local module to test spark-submit--py-files dependencies\n",
    "    double_udf_int = udf(udfs.double_x, IntegerType())\n",
    "\n",
    "    # Save transformed data set to disk\n",
    "    salesDF.select(\"date\", \"sale\", double_udf_int(\"sale\").alias(\"sale_double\")).write.json(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./code/hello_py_spark_udfs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./code/hello_py_spark_udfs.py\n",
    "def double_x(x):\n",
    "    return x + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying dependency from local path ./code/hello_py_spark_udfs.py to tmpdir /tmp/tmphnqo6_b8\n",
      "Uploading dependencies from tmpdir /tmp/tmphnqo6_b8 to S3 s3://sagemaker-cn-north-1-876820548815/sm-spark-udfs-2021-01-22-05-31-48-206/input/py-files\n",
      "Creating processing-job with name sm-spark-udfs-2021-01-22-05-31-48-206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sm-spark-udfs-2021-01-22-05-31-48-206\n",
      "Inputs:  [{'InputName': 'py-files', 'S3Input': {'S3Uri': 's3://sagemaker-cn-north-1-876820548815/sm-spark-udfs-2021-01-22-05-31-48-206/input/py-files', 'LocalPath': '/opt/ml/processing/input/py-files', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-cn-north-1-876820548815/sm-spark-udfs-2021-01-22-05-31-48-206/input/code/hello_py_spark_app.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  []\n",
      "..................................................................!"
     ]
    }
   ],
   "source": [
    "# Define job input/output URIs\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "prefix = \"sagemaker/spark-preprocess-demo/{}\".format(timestamp_prefix)\n",
    "input_prefix_sales = \"{}/input/sales\".format(prefix)\n",
    "output_prefix_sales = \"{}/output/sales\".format(prefix)\n",
    "input_s3_uri = \"s3://{}/{}\".format(bucket, input_prefix_sales)\n",
    "output_s3_uri = \"s3://{}/{}\".format(bucket, output_prefix_sales)\n",
    "\n",
    "sagemaker_session.upload_data(path=\"./data/data.jsonl\", bucket=bucket, key_prefix=input_prefix_sales)\n",
    "\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-udfs\",\n",
    "    framework_version=\"2.4\",\n",
    "    role=role,\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=1200,\n",
    ")\n",
    "\n",
    "spark_processor.run(\n",
    "    submit_app=\"./code/hello_py_spark_app.py\",\n",
    "    submit_py_files=[\"./code/hello_py_spark_udfs.py\"],\n",
    "    arguments=[\"--input\", input_s3_uri, \"--output\", output_s3_uri],\n",
    "    logs=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output files in s3://sagemaker-cn-north-1-876820548815/sagemaker/spark-preprocess-demo/2021-01-22-05-31-48/output/sales\n",
      "2021-01-22 05:36:56          0 _SUCCESS\n",
      "2021-01-22 05:36:56      51313 part-00000-ea69e7fb-431d-4ada-9552-30268e848dc1-c000.json\n"
     ]
    }
   ],
   "source": [
    "print('Output files in {}'.format(output_s3_uri))\n",
    "!aws s3 ls $output_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"date\":\"2020-01-04\",\"sale\":283,\"sale_double\":566}\n",
      "{\"date\":\"2020-01-06\",\"sale\":140,\"sale_double\":280}\n",
      "{\"date\":\"2020-01-05\",\"sale\":820,\"sale_double\":1640}\n",
      "{\"date\":\"2020-01-04\",\"sale\":452,\"sale_double\":904}\n",
      "{\"date\":\"2020-01-06\",\"sale\":495,\"sale_double\":990}\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --quiet $output_s3_uri/part-00000-ea69e7fb-431d-4ada-9552-30268e848dc1-c000.json - | head -n5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Run a Java/Scala Spark application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating processing-job with name sm-spark-java-2021-01-22-06-10-17-930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sm-spark-java-2021-01-22-06-10-17-930\n",
      "Inputs:  [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-cn-north-1-876820548815/sm-spark-java-2021-01-22-06-10-17-930/input/code/spark-test-app.jar', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  []\n",
      ".............................................................!"
     ]
    }
   ],
   "source": [
    "from sagemaker.spark.processing import SparkJarProcessor\n",
    "\n",
    "# Upload the raw input dataset to S3\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "prefix = \"sagemaker/spark-preprocess-demo/{}\".format(timestamp_prefix)\n",
    "input_prefix_sales = \"{}/input/sales\".format(prefix)\n",
    "output_prefix_sales = \"{}/output/sales\".format(prefix)\n",
    "input_s3_uri = \"s3://{}/{}\".format(bucket, input_prefix_sales)\n",
    "output_s3_uri = \"s3://{}/{}\".format(bucket, output_prefix_sales)\n",
    "\n",
    "sagemaker_session.upload_data(path=\"./data/data.jsonl\", bucket=bucket, key_prefix=input_prefix_sales)\n",
    "\n",
    "spark_processor = SparkJarProcessor(\n",
    "    base_job_name=\"sm-spark-java\",\n",
    "    framework_version=\"2.4\",\n",
    "    role=role,\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=1200,\n",
    ")\n",
    "\n",
    "spark_processor.run(\n",
    "    submit_app=\"./code/spark-test-app.jar\",\n",
    "    submit_class=\"com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp\",\n",
    "    arguments=[\"--input\", input_s3_uri, \"--output\", output_s3_uri],\n",
    "    logs=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Data Processing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output files in s3://sagemaker-cn-north-1-876820548815/sagemaker/spark-preprocess-demo/2021-01-22-06-10-17/output/sales\n",
      "2021-01-22 06:15:02          0 _SUCCESS\n",
      "2021-01-22 06:15:01      51313 part-00000-71e850dd-dbd6-4159-9292-d037276a17ef-c000.json\n"
     ]
    }
   ],
   "source": [
    "print('Output files in {}'.format(output_s3_uri))\n",
    "!aws s3 ls $output_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"date\":\"2020-01-01\",\"sale\":5,\"sale_double\":10}\n",
      "{\"date\":\"2020-01-01\",\"sale\":7,\"sale_double\":14}\n",
      "{\"date\":\"2020-01-01\",\"sale\":15,\"sale_double\":30}\n",
      "{\"date\":\"2020-01-01\",\"sale\":15,\"sale_double\":30}\n",
      "{\"date\":\"2020-01-01\",\"sale\":23,\"sale_double\":46}\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --quiet $output_s3_uri/part-00000-71e850dd-dbd6-4159-9292-d037276a17ef-c000.json - | head -n5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Specifying additional Spark configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating processing-job with name sm-spark-2021-01-22-07-03-43-145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sm-spark-2021-01-22-07-03-43-145\n",
      "Inputs:  [{'InputName': 'conf', 'S3Input': {'S3Uri': 's3://sagemaker-cn-north-1-876820548815/sm-spark-2021-01-22-07-03-43-145/input/conf/configuration.json', 'LocalPath': '/opt/ml/processing/input/conf', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-cn-north-1-876820548815/sm-spark-2021-01-22-07-03-43-145/input/code/preprocess.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  []\n",
      ".......................................................................!"
     ]
    }
   ],
   "source": [
    "# Upload the raw input dataset to a unique S3 location\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "prefix = \"sagemaker/spark-preprocess-demo/{}\".format(timestamp_prefix)\n",
    "input_prefix_abalone = \"{}/input/raw/abalone\".format(prefix)\n",
    "input_preprocessed_prefix_abalone = \"{}/input/preprocessed/abalone\".format(prefix)\n",
    "\n",
    "sagemaker_session.upload_data(path=\"./data/abalone.csv\", bucket=bucket, key_prefix=input_prefix_abalone)\n",
    "\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark\",\n",
    "    framework_version=\"2.4\",\n",
    "    role=role,\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=1200,\n",
    ")\n",
    "\n",
    "configuration = [{\n",
    "    \"Classification\": \"spark-defaults\",\n",
    "    \"Properties\": {\"spark.executor.memory\": \"2g\", \"spark.executor.cores\": \"1\"},\n",
    "}]\n",
    "\n",
    "spark_processor.run(\n",
    "    submit_app=\"./code/preprocess.py\",\n",
    "    arguments=[\"--s3_input_bucket\", bucket,\n",
    "               \"--s3_input_key_prefix\", input_prefix_abalone,\n",
    "               \"--s3_output_bucket\", bucket,\n",
    "               \"--s3_output_key_prefix\", input_preprocessed_prefix_abalone],\n",
    "    configuration=configuration,\n",
    "    logs=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 5 rows from s3://{}/{}/train/\".format(bucket, input_preprocessed_prefix_abalone))\n",
    "!aws s3 ls s3://$bucket/$input_preprocessed_prefix_abalone/train/\n",
    "\n",
    "!aws s3 cp --quiet s3://$bucket/$input_preprocessed_prefix_abalone/train/part-00000 - | head -n5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
