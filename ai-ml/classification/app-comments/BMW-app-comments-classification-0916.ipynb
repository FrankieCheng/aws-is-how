{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对于2018年CIC2rd的反馈数据进行文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install dependency and initial session and S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws-cn:iam::876820548815:role/Sagemaker-Bootcamp-SageMakerExecutionRole-Z3VF78G260T1\n",
      "ray-ai-ml-bjs\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "from random import shuffle\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role) # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf\n",
    "\n",
    "bucket = 'ray-ai-ml-bjs' #sess.default_bucket() # Replace with your own bucket name if needed\n",
    "print(bucket)\n",
    "prefix = 'classification/blazingtext/bmwticket' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already up-to-date: jieba in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.42.1)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: paddlepaddle-tiny in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.6.1)\n",
      "Requirement already satisfied: requests>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from paddlepaddle-tiny) (2.23.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from paddlepaddle-tiny) (1.14.0)\n",
      "Requirement already satisfied: objgraph in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from paddlepaddle-tiny) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from paddlepaddle-tiny) (1.16.4)\n",
      "Requirement already satisfied: decorator in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from paddlepaddle-tiny) (4.4.2)\n",
      "Requirement already satisfied: graphviz in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from paddlepaddle-tiny) (0.14.1)\n",
      "Requirement already satisfied: protobuf>=3.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from paddlepaddle-tiny) (3.12.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.0.0->paddlepaddle-tiny) (2020.4.5.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.0.0->paddlepaddle-tiny) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.0.0->paddlepaddle-tiny) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.0.0->paddlepaddle-tiny) (3.0.4)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.1.0->paddlepaddle-tiny) (46.1.3.post20200330)\n"
     ]
    }
   ],
   "source": [
    "!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple jieba --upgrade\n",
    "!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple paddlepaddle-tiny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare classification category\n",
    "- All return '\\n' convert to `_` for example `LSC\\n远程指令` convert to `LSC_远程指令`\n",
    "- Convert alphabet to Uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'APP显示问题': '1001', 'APP版本问题': '1002', 'APP登陆问题': '1003', 'APP车辆信息显示问题': '1004', 'BON': '1005', 'CARLIFE': '1006', 'CARPLAY': '1007', 'ETC': '1008', 'GPS': '1009', 'IBA用户手册': '1010', 'LSC': '1011', 'LSC_远程指令': '1012', 'PIN码': '1013', 'POI': '1014', 'QQ音乐': '1015', 'RSU': '1016', 'RTTI': '1017', 'SIM卡相关': '1019', 'WIFI': '1020', '个性化设置': '1021', '二手车预激活问题': '1022', '会员': '1023', '其他': '1024', '即时充电': '1025', '喜马拉雅': '1026', '地图自动更新服务': '1027', '天猫精灵': '1028', '实时路况功能': '1029', '意见建议': '1030', '数字钥匙': '1031', '智慧停车': '1032', '更新BMW服务': '1033', '满意': '1034', '节日祝福': '1035', '行程摘要': '1036', '行车摘要': '1037', '车机端登录': '1038', '远程3D视图': '1039', '远程指令': '1040', '远程指令_LSC': '1041', '远程服务': '1042', '远程软件升级': '1043', '违章代缴': '1044', '预开通激活': '1045', '预激活开通': '1046'}\n"
     ]
    }
   ],
   "source": [
    "index_to_label = {} #1001:APP显示问题\n",
    "label_to_index = {} #APP显示问题:1001\n",
    "with open(\"app-comments-classes.txt\") as f:\n",
    "    for i,label in enumerate(f.readlines()):\n",
    "        ll = label.strip().split(',')\n",
    "        index_to_label[ll[0]] = ll[1].upper()\n",
    "        label_to_index[ll[1].upper()] = ll[0]\n",
    "print(label_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the input data\n",
    "- parse the 'category detail' and 'comment'\n",
    "- convert the 'category detail' as format `1001_!_APP显示问题_!_问题描述`\n",
    "- Convert alphabet to Uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  category detail                                            comment\n",
      "0             LSC                                      我的互联不更新了是什么原因\n",
      "1             LSC                                           定位不准，乱定位\n",
      "2            WIFI                                   开通互联驾驶后，手机怎么连接车辆\n",
      "3           预激活开通                                     我怎么是开通不了互联驾驶功能\n",
      "4            远程服务                      开通云端互联后车辆无法执行通风指令 且车辆里程等信息无显示\n",
      "5             LSC                                          系统好几天不更新了\n",
      "6             LSC                                            无法定位车辆！\n",
      "7             LSC  你好，为什么在互联驾驶这个功能中定位不了我车了，显示说我在车上的GPS关闭了，但是我检查过了...\n",
      "8           个性化设置                                        云端互联欢迎词修改不了\n",
      "9          远程3D视图                                         远程3D视图看不了了\n",
      "['1011_!_LSC_!_我的互联不更新了是什么原因', '1011_!_LSC_!_定位不准，乱定位', '1020_!_WIFI_!_开通互联驾驶后，手机怎么连接车辆'] \n",
      "\n",
      "['已经激活云端互联。但是不能连接车', '汽车公里数不和车同步', '突然之间导航没有任何声音了'] \n",
      "\n",
      "Excel Sheet to CSV done - csv_data_df:  4171  , input_with_category:  3502  , input_without_category:  669 \n",
      "\n",
      "data offset counts: \n",
      "LSC          2065\n",
      "远程服务          460\n",
      "其他            457\n",
      "远程3D视图        119\n",
      "意见建议           50\n",
      "预激活开通          46\n",
      "CARPLAY        39\n",
      "APP显示问题        36\n",
      "QQ音乐           27\n",
      "RSU            26\n",
      "远程指令           23\n",
      "WIFI           19\n",
      "数字钥匙           19\n",
      "行程摘要           18\n",
      "POI            10\n",
      "车机端登录           9\n",
      "PIN码            9\n",
      "天猫精灵            9\n",
      "CARLIFE         7\n",
      "个性化设置           6\n",
      "LSC_远程指令        6\n",
      "RTTI            5\n",
      "预开通激活           5\n",
      "即时充电            4\n",
      "BON             3\n",
      "GPS             2\n",
      "智慧停车            2\n",
      "远程软件升级          2\n",
      "APP版本问题         2\n",
      "APP车辆信息显示问题      2\n",
      "\n",
      " sentiment_input:  4171 preview 10 items\n",
      "['我的互联不更新了是什么原因', '定位不准，乱定位', '开通互联驾驶后，手机怎么连接车辆', '我怎么是开通不了互联驾驶功能', '开通云端互联后车辆无法执行通风指令 且车辆里程等信息无显示', '系统好几天不更新了', '无法定位车辆！', '你好，为什么在互联驾驶这个功能中定位不了我车了，显示说我在车上的GPS关闭了，但是我检查过了，也开着啊什么问题呢', '云端互联欢迎词修改不了', '远程3D视图看不了了']\n"
     ]
    }
   ],
   "source": [
    "excel_data_df = pd.read_excel('20200916_in-app-comments_EE-CN-12.xlsx', sheet_name='in-app comment', usecols=['category detail', 'comment'], index_col=None)\n",
    "#print(excel_data_df[0:5])\n",
    "\n",
    "raw_app_comments_file = 'raw-app-comments-0916.csv'\n",
    "excel_data_df.to_csv(raw_app_comments_file, index=False)\n",
    "sentiment_input = []\n",
    "input_with_category = []\n",
    "input_without_category = []\n",
    "data_offset_counts = {}\n",
    "\n",
    "csv_data_df = pd.read_csv(raw_app_comments_file, delimiter=',', index_col=None)\n",
    "print(csv_data_df[0:10])\n",
    "\n",
    "for index, data in csv_data_df.iterrows():\n",
    "    category_str = data['category detail']\n",
    "    comment_str = data['comment']\n",
    "    if category_str=='' or pd.isnull(category_str):\n",
    "        input_without_category.append(str(comment_str))\n",
    "    else:\n",
    "        if category_str==\"远程指令\\nLSC\" or category_str==\"LSC\\n远程指令\":\n",
    "            category_str = '_'.join(category_str.split('\\n'))\n",
    "            #print(category_str, comment_str)\n",
    "        category_str = category_str.upper()\n",
    "        category_index = label_to_index[category_str]\n",
    "        input_with_category.append(str(category_index) + '_!_' + str(category_str) + '_!_' + str(comment_str))\n",
    "        data_offset_counts[category_str] = data_offset_counts.get(category_str, 0) + 1\n",
    "    sentiment_input.append(comment_str)\n",
    "\n",
    "print(input_with_category[0:3],'\\n')\n",
    "print(input_without_category[0:3],'\\n')\n",
    "print('Excel Sheet to CSV done - csv_data_df: ',len(csv_data_df), ' , input_with_category: ', len(input_with_category), ' , input_without_category: ', len(input_without_category), '\\n')\n",
    "\n",
    "print('data offset counts: ')\n",
    "items = list(data_offset_counts.items())\n",
    "items.sort(key=lambda x: x[1], reverse=True)\n",
    "for i in range(30):\n",
    "    word, count = items[i]\n",
    "    print(\"{:<10}{:>7}\".format(word, count))\n",
    "\n",
    "\n",
    "print('\\n','sentiment_input: ', len(sentiment_input), 'preview 10 items')\n",
    "print(sentiment_input[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "喜马拉雅FM可以像QQ音乐一样直接在车上应用么？还是只能在CARPLAY上使用 - sentiment: NEUTRAL - sentiment_score: {\"Positive\": 0.010348147712647915, \"Negative\": 0.0909975990653038, \"Neutral\": 0.8986495733261108, \"Mixed\": 4.764520781463943e-06}\n",
      "\n",
      "你好，为什么在互联驾驶这个功能中定位不了我车了，显示说我在车上的GPS关闭了，但是我检查过了，也开着啊什么问题呢 - sentiment: NEGATIVE - sentiment_score: {\"Positive\": 0.0004973431350663304, \"Negative\": 0.9937689900398254, \"Neutral\": 0.005685559939593077, \"Mixed\": 4.810316386283375e-05}\n",
      "\n",
      "我喜欢使用互联驾驶这个功能中路线规划 - sentiment: POSITIVE - sentiment_score: {\"Positive\": 0.9950544834136963, \"Negative\": 5.2080289606237784e-05, \"Neutral\": 0.004892547149211168, \"Mixed\": 8.196818157557573e-07}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session = boto3.Session(profile_name='global', region_name='us-east-1')\n",
    "comprehend_client = session.client('comprehend')\n",
    "sample_list = ['喜马拉雅FM可以像QQ音乐一样直接在车上应用么？还是只能在CARPLAY上使用', '你好，为什么在互联驾驶这个功能中定位不了我车了，显示说我在车上的GPS关闭了，但是我检查过了，也开着啊什么问题呢', '我喜欢使用互联驾驶这个功能中路线规划']\n",
    "for s_input in sample_list:\n",
    "    sentiment_response = comprehend_client.detect_sentiment(Text=s_input, LanguageCode='zh')\n",
    "    sentiment_str = sentiment_response['Sentiment']\n",
    "    sentiment_score = json.dumps(sentiment_response['SentimentScore'])\n",
    "    print(s_input + ' - sentiment: ' + sentiment_str + ' - sentiment_score: ' + sentiment_score + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_detect_sentiment \n",
    "sentiment_result_file = 'sentiment_result.csv'\n",
    "\n",
    "if os.path.isfile(sentiment_result_file):\n",
    "    os.remove(sentiment_result_file)\n",
    "\n",
    "with open(sentiment_result_file, 'a+') as sentiment_f:\n",
    "    sentiment_f.write('comment' + '|' + 'sentiment' + '|' + 'sentiment_score' + '\\n')\n",
    "\n",
    "sentiment_chunks = [sentiment_input[x:x+25] for x in range(0, len(sentiment_input), 25)]\n",
    "for chunk in sentiment_chunks:\n",
    "    sentiment_response = comprehend_client.batch_detect_sentiment(TextList=chunk, LanguageCode='zh')\n",
    "    #print(sentiment_response['ResultList'])\n",
    "    with open(sentiment_result_file, 'a+') as sentiment_f:\n",
    "        s_input_index = 0\n",
    "        for s_input in chunk:\n",
    "          sentiment_str = sentiment_response['ResultList'][s_input_index]['Sentiment']\n",
    "          sentiment_score = json.dumps(sentiment_response['ResultList'][s_input_index]['SentimentScore'])\n",
    "          sentiment_f.write(s_input + '|' + sentiment_str + '|' + sentiment_score + '\\n')\n",
    "          s_input_index +=1\n",
    "sentiment_f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         comment sentiment  \\\n",
      "0                  我的互联不更新了是什么原因  NEGATIVE   \n",
      "1                       定位不准，乱定位  NEGATIVE   \n",
      "2               开通互联驾驶后，手机怎么连接车辆   NEUTRAL   \n",
      "3                 我怎么是开通不了互联驾驶功能  NEGATIVE   \n",
      "4  开通云端互联后车辆无法执行通风指令 且车辆里程等信息无显示  NEGATIVE   \n",
      "\n",
      "                                     sentiment_score  \n",
      "0  {\"Positive\": 0.00021503579046111554, \"Negative...  \n",
      "1  {\"Positive\": 0.00015880828141234815, \"Negative...  \n",
      "2  {\"Positive\": 0.0008933115750551224, \"Negative\"...  \n",
      "3  {\"Positive\": 0.0003960980975534767, \"Negative\"...  \n",
      "4  {\"Positive\": 0.00016022840281948447, \"Negative...  \n",
      "Analysis sentiment done for total item:  4172\n"
     ]
    }
   ],
   "source": [
    "sentiment_data_df = pd.read_csv(sentiment_result_file, delimiter='|', index_col=None)\n",
    "print(sentiment_data_df.head())\n",
    "\n",
    "print('Analysis sentiment done for total item: ', len(sentiment_data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005_!_RS_!_客户罗先生（车主黄先生的家人）致电，表示其i豪华型车辆，之前已经接到互联驾驶通知协议手机号更改成功的通知，但目前使用新协议手机号登陆云端互联APP后，首页不显示远程控制选项了（更改协议手机号之前可以正常使用），针对此问题，烦请互联驾驶部门跟进处理。 签订互联驾驶协议时登记的车主姓名：黄先生 协议手机号码：（原协议手机号） VIN：LBV5S3104HSN87123 登陆云端互联密码：990308ljq 手机型号&版本：IOS 最新 云端互联APP版本：最新 联系人：罗先生 联系电话：\n",
      "1002_!_Carplay_!_ya_gao 2018-11-01 09:35: 客户杨先生通过在线客户平台反馈，表示在2018/9/1在淮安润宝行店内购买的 530Li 尊享型 豪华套装车辆，客户表示互联驾驶已经开通但是还是没有Apple Carplay的选项，已建议客户发送邮件，针对客户问题烦请互联驾驶人员跟进处理。 联系人/协议手机号码： 车主/联系人：杨先生 车架号：LBVKY5109JSP87501\n",
      "1002_!_Carplay_!_ya_gao 2018-11-01 10:36:2018/11/1 7:53:14客户李先生（车主刘力先生的朋友）致电，表示其（2018.09.29）在（唐山中宝）购买了（525LI M），（10.09）登陆BMW云端互联APP，遇到（CARPLAY无显示、无法使用）问题，已建议客户发送截图至互联驾驶邮箱，针对此问题烦请跟进处理。未转接。协议手机号码：+86\n"
     ]
    }
   ],
   "source": [
    "import logging,os,jieba\n",
    "#!wget https://cdc-code.s3.cn-north-1.amazonaws.com.cn/chineseStopWords.txt\n",
    "def get_stopwords(StopWordFileName):\n",
    "    logging.basicConfig(format='%(asctime)s:%(levelname)s:%(message)s',level=logging.INFO)  \n",
    "      #加载停用词表 \n",
    "    stopword_set = set()\n",
    "    with open(StopWordFileName,'r',encoding=\"utf-8\") as stopwords:\n",
    "        for stopword in stopwords: \n",
    "            stopword_set.add(stopword.strip(\"\\n\"))  \n",
    "    return stopword_set\n",
    "    \n",
    "def clear_timestamp(mystr):\n",
    "    patterns = [r\"\\w{3} \\w{3} \\d{2} \\d{1,2}:\\d{1,2}:\\d{1,2} \\d{4}\\s*\",    #sun aug 19 13:02:10 2018\n",
    "        r\"\\w{3}, \\d{2} \\w{3} \\d{4} \\d{1,2}:\\d{1,2}:\\d{1,2} \\w{2}\\s*\",     #Sun, 19 Aug 2018 13:02:08 ET\n",
    "        r\"\\d{4}-\\d{1,2}-\\d{1,2} \\d{1,2}:\\d{1,2}:\\s*\",                       #2018-11-01 09:35:\n",
    "        r\"\\d{4}/\\d{1,2}/\\d{1,2}\\s*\",                                    #2018/9/1\n",
    "        r\"\\d{1,2}/\\d{1,2}/\\d{4}\\s*\",                                    #9/1/2018\n",
    "        r\"\\d{4}.\\d{1,2}.\\d{0,2}\\s*\",                                    #2018.9.1\n",
    "        r\"\\d{1,2}.\\d{1,2}.\\d{4}\\s*\",                                    #9.1.2018\n",
    "        r\"\\d{4}-\\d{1,2}-\\d{1,2} \\d{1,2}:\\d{1,2}:\\d{4}/\\d{1,2}/\\d{1,2}\\s*\",      #2018-11-01 11:18:2018/10/31\n",
    "        r\"\\d{1,2}:\\d{1,2}:\\d{4}/\\d{1,2}/\\d{1,2}\\s*\",      #2018-11-01 11:18:2018/10/31 21:09:08\n",
    "        r\"\\d{1,2}:\\d{1,2}:\\d{1,2}\\s*(AM|PM|am|pm)\\s*\",        #4:00:58 PM\n",
    "        r\"\\d{1,2}:\\d{1,2}:\\d{1,2}\\s*\",                                     #21:09:08\n",
    "        r\"\\d{1,2}:\\d{1,2}\\s*\",                                     #21:09:08\n",
    "        r\"(\\d{4})年(\\d{1,2})月(\\d{1,2})日\\s*\",                 #2018年10月5日\n",
    "        r\"(\\d{2,4})年\\s*\",                 #2018年\n",
    "        r\"(\\d{4})年(\\d{1,2})月\\s*\",                 #2018年10月\n",
    "        r\":\\s*([\\da-zA_Z]+\\/)+([a-zA-Z0-9\\.]+)\"                     #URL\n",
    "        ]\n",
    "\n",
    "    s = mystr\n",
    "\n",
    "    for p in patterns:\n",
    "        s = re.sub(p,'', s)\n",
    "\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "def clear_email_phone_colon(mystr):\n",
    "    patterns = [r\"1[0-9]{10}\", #mobile\n",
    "        r\"(\\(0\\d{2}\\) \\d{8})|(\\(0\\d{3}\\) \\d{7})|(\\(0\\d{3}\\)-\\d{8}$)|(\\(0\\d{2}\\)\\d{8})|(\\(0\\d{3}\\)\\d{7})|(\\(0\\d{3}\\)\\d{8}$)\",                       #phone\n",
    "        r\"(0\\d{2}-\\d{8})|(0\\d{3}-\\d{7})|(0\\d{3}-\\d{8}$)|(\\d{8})\",                       #phone\n",
    "        r\"\\w+([-+.]\\w+)*@\\w+([-.]\\w+)*\\.\\w+([-.]\\w+)*\"    #email\n",
    "        ]\n",
    "\n",
    "    s = mystr\n",
    "\n",
    "    for p in patterns:\n",
    "        s = re.sub(p,'', s)\n",
    "\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "# Debug code\n",
    "file  = 'stop-words-test.txt'\n",
    "with open(file) as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "labels = []\n",
    "for line in lines:\n",
    "    s = clear_email_phone_colon(line)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (20.2.3)\n",
      "Requirement already satisfied: datetime in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (4.3)\n",
      "Requirement already satisfied: pytz in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datetime) (2019.3)\n",
      "Requirement already satisfied: zope.interface in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datetime) (5.1.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from zope.interface->datetime) (46.1.3.post20200330)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg\n",
    "\n",
    "def with_stop_words_paddle_pseg(mystr, word_counts):\n",
    "    #get stopwords\n",
    "    stopwords = get_stopwords('chinesestopwords_test.txt')\n",
    "    #启用停用词过滤\n",
    "    no_phone = clear_email_phone_colon(mystr)\n",
    "    no_timestamp = clear_timestamp(no_phone)\n",
    "    fenci = re.sub(r\"[\\s+\\.\\!\\/_,$%^*()?;；:-【】+\\\"\\']+|[+——一！，;:：。？、~@#￥%……&*（）]+\", \"\", no_timestamp)\n",
    "    words = pseg.cut(fenci,use_paddle=True) #paddle模式+posseg\n",
    "    filled_words = set()\n",
    "    for word, flag in words:\n",
    "        #print('raw %s %s' % (word, flag))\n",
    "        # 人名, 地名，机构，方位名词, 量词, 代词, 时间, 副词\n",
    "        if (flag == 'nr' or flag == 'ns' or flag == 'PER' \n",
    "            or flag == 'LOC' or flag == 'ORG' or flag == 'f' or flag == 'r' or flag == 'q'\n",
    "            or flag == 't' or flag == 'TIME' or flag == 'd'):\n",
    "            #print('%s %s' % (word, flag))\n",
    "            continue\n",
    "        else:\n",
    "            if word not in stopwords:          #不在停用词表中\n",
    "                if len(word) == 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    word_counts[word] = word_counts.get(word, 0) + 1\n",
    "                filled_words.add(word)\n",
    "    return filled_words\n",
    "\n",
    "\n",
    "def without_stop_words_paddle_pseg(mystr, word_counts):\n",
    "    #get stopwords\n",
    "    stopwords = get_stopwords('chinesestopwords_test.txt')\n",
    "    #启用停用词过滤\n",
    "    no_phone = clear_email_phone_colon(mystr)\n",
    "    no_timestamp = clear_timestamp(no_phone)\n",
    "    fenci = re.sub(r\"[\\s+\\.\\!\\/_,$%^*()?;；:-【】+\\\"\\']+|[+——一！，;:：。？、~@#￥%……&*（）]+\", \"\", no_timestamp)\n",
    "    words = pseg.cut(fenci,use_paddle=True) #paddle模式+posseg\n",
    "    filled_words = set()\n",
    "    for word, flag in words:\n",
    "        #print('raw %s %s' % (word, flag))\n",
    "        # 人名, 地名，机构，方位名词, 量词, 代词, 时间, 副词\n",
    "        if (flag == 'nr' or flag == 'ns' or flag == 'PER' \n",
    "            or flag == 'LOC' or flag == 'ORG' or flag == 'f' or flag == 'r' or flag == 'q'\n",
    "            or flag == 't' or flag == 'TIME' or flag == 'd'):\n",
    "            #print('%s %s' % (word, flag))\n",
    "            continue\n",
    "        else:\n",
    "            if len(word) == 1:\n",
    "                continue\n",
    "            else:\n",
    "                word_counts[word] = word_counts.get(word, 0) + 1\n",
    "        filled_words.add(word)\n",
    "    return filled_words\n",
    "\n",
    "def without_stop_words_cut(mystr, word_counts): \n",
    "    #get stopwords\n",
    "    stopwords = get_stopwords('chinesestopwords_test.txt')\n",
    "    #启用停用词过滤\n",
    "    no_phone = clear_email_phone_colon(mystr)\n",
    "    no_timestamp = clear_timestamp(no_phone)\n",
    "    fenci = re.sub(r\"[\\s+\\.\\!\\/_,$%^*()?;；:-【】+\\\"\\']+|[+——一！，;:：。？、~@#￥%……&*（）]+\", \"\", no_timestamp)\n",
    "    words = jieba.cut(fenci,cut_all=False)\n",
    "    filled_words = set()\n",
    "    for word in words:\n",
    "        #print('raw %s %s' % (word, flag))\n",
    "        if len(word) == 1:\n",
    "            continue\n",
    "        else:\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "        filled_words.add(word)\n",
    "    return filled_words\n",
    "\n",
    "\n",
    "def with_stop_words_cut(mystr, word_counts): \n",
    "    #get stopwords\n",
    "    stopwords = get_stopwords('chinesestopwords_test.txt')\n",
    "    #启用停用词过滤\n",
    "    no_phone = clear_email_phone_colon(mystr)\n",
    "    no_timestamp = clear_timestamp(no_phone)\n",
    "    fenci = re.sub(r\"[\\s+\\.\\!\\/_,$%^*()?;；:-【】+\\\"\\']+|[+——一！，;:：。？、~@#￥%……&*（）]+\", \"\", no_timestamp)\n",
    "    words = jieba.cut(fenci,cut_all=False)\n",
    "    filled_words = set()\n",
    "    for word in words:\n",
    "        #print('raw %s %s' % (word, flag))\n",
    "        # 人名, 地名，机构，方位名词, 量词, 代词, 时间, 副词\n",
    "        if word not in stopwords:          #不在停用词表中\n",
    "            if len(word) == 1:\n",
    "                continue\n",
    "            else:\n",
    "                word_counts[word] = word_counts.get(word, 0) + 1\n",
    "            filled_words.add(word)\n",
    "    return filled_words\n",
    "\n",
    "\n",
    "def without_stop_words_paddle(mystr, word_counts): \n",
    "    #get stopwords\n",
    "    stopwords = get_stopwords('chinesestopwords_test.txt')\n",
    "    #启用停用词过滤\n",
    "    no_phone = clear_email_phone_colon(mystr)\n",
    "    no_timestamp = clear_timestamp(no_phone)\n",
    "    fenci = re.sub(r\"[\\s+\\.\\!\\/_,$%^*()?;；:-【】+\\\"\\']+|[+——一！，;:：。？、~@#￥%……&*（）]+\", \"\", no_timestamp)\n",
    "    words = jieba.cut(fenci,use_paddle=True) #paddle模式\n",
    "    filled_words = set()\n",
    "    for word in words:\n",
    "        #print('raw %s %s' % (word, flag))\n",
    "        if len(word) == 1:\n",
    "            continue\n",
    "        else:\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "        filled_words.add(word)\n",
    "    return filled_words\n",
    "\n",
    "\n",
    "def with_stop_words_cut_paddle(mystr, word_counts):   \n",
    "    #get stopwords\n",
    "    stopwords = get_stopwords('chinesestopwords_test.txt')\n",
    "    #启用停用词过滤\n",
    "    no_phone = clear_email_phone_colon(mystr)\n",
    "    no_timestamp = clear_timestamp(no_phone)\n",
    "    fenci = re.sub(r\"[\\s+\\.\\!\\/_,$%^*()?;；:-【】+\\\"\\']+|[+——一！，;:：。？、~@#￥%……&*（）]+\", \"\", no_timestamp)\n",
    "    words = jieba.cut(fenci,cut_all=False,use_paddle=True) #paddle模式\n",
    "    filled_words = set()\n",
    "    for word in words:\n",
    "        #print('raw %s %s' % (word, flag))\n",
    "        # 人名, 地名，机构，方位名词, 量词, 代词, 时间, 副词\n",
    "        if word not in stopwords:          #不在停用词表中\n",
    "            if len(word) == 1:\n",
    "                continue\n",
    "            else:\n",
    "                word_counts[word] = word_counts.get(word, 0) + 1\n",
    "            filled_words.add(word)\n",
    "    return filled_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paddle enabled successfully......\n",
      "2020-10-13 05:22:14,144:DEBUG:Paddle enabled successfully......\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data processing time 9\n",
      "[['__label__其他', '手机', '自动', '云端', '默认', '启动', '打开', '关闭', '互联'], ['__label__喜马拉雅', '只能', '喜马拉雅', '音乐样', '车上'], ['__label__LSC', '不到', '查询', '状态', '更新', '车辆', '互联'], ['__label__LSC', '云端', '更新', '位置', '互联'], ['__label__远程服务', '解锁']]\n",
      "更新           1350\n",
      "车辆           1185\n",
      "定位            825\n",
      "互联            630\n",
      "信息            626\n",
      "显示            597\n",
      "远程            501\n",
      "云端            401\n",
      "功能            396\n",
      "位置            274\n",
      "手机            250\n",
      "状态            237\n",
      "驾驶            160\n",
      "数据            149\n",
      "成功            142\n",
      "解锁            137\n",
      "软件            133\n",
      "车门            131\n",
      "打开            131\n",
      "系统            122\n",
      "连接            121\n",
      "车子            116\n",
      "发送            105\n",
      "实时            104\n",
      "刷新             98\n",
      "通风             95\n",
      "解决             87\n",
      "准确             86\n",
      "启动             86\n",
      "里程             85\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "#单机并行分词\n",
    "#jieba.enable_parallel(8)\n",
    "#paddle模式，精确匹配需要关闭parallel\n",
    "jieba.disable_parallel()\n",
    "#启动paddle模式\n",
    "jieba.enable_paddle()\n",
    "\n",
    "begin = datetime.datetime.now()\n",
    "    \n",
    "labels = []\n",
    "counts = {}                 #计数{word，frequency}\n",
    "for line in input_with_category:\n",
    "    label = []\n",
    "    line = line.split('_!_')\n",
    "    if line[0].strip('\"').isdigit() and index_to_label[line[0].strip('\"')]:\n",
    "        label_code = index_to_label[line[0].strip('\"')]\n",
    "        label.append('__label__' + label_code)\n",
    "        \n",
    "        words_after_jieba = with_stop_words_cut_paddle(line[2], counts)\n",
    "        \n",
    "        label.extend(words_after_jieba)\n",
    "        #print(label)\n",
    "        labels.append(label)\n",
    "\n",
    "shuffle(labels)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print('data processing time %d' %(end - begin).seconds)\n",
    "print(labels[0:5])\n",
    "\n",
    "items = list(counts.items())\n",
    "items.sort(key=lambda x: x[1], reverse=True)\n",
    "for i in range(30):\n",
    "    word, count = items[i]\n",
    "    print(\"{:<10}{:>7}\".format(word, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'classification/blazingtext/bmwticket'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train_data = labels[0:int(len(labels)*0.8)]\n",
    "t_validation_data = labels[int(len(labels)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t_train_data[0:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "t_train_file = 'tt.train'\n",
    "t_validation_file = 'tt.validation'\n",
    "\n",
    "with open(t_train_file, 'w') as csvoutfile:\n",
    "    csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "    csv_writer.writerows(t_train_data)\n",
    "    \n",
    "with open(t_validation_file, 'w') as csvoutfile:\n",
    "    csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "    csv_writer.writerows(t_validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.5 ms, sys: 0 ns, total: 46.5 ms\n",
      "Wall time: 187 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "t_train_channel = prefix + '/train'\n",
    "t_validation_channel = prefix + '/validation'\n",
    "\n",
    "sess.upload_data(path='tt.train', bucket=bucket, key_prefix=t_train_channel)\n",
    "sess.upload_data(path='tt.validation', bucket=bucket, key_prefix=t_validation_channel)\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, t_train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket, t_validation_channel)\n",
    "\n",
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-13 05:22:24,002:WARNING:'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker BlazingText container: 390948362332.dkr.ecr.cn-north-1.amazonaws.com.cn/blazingtext:latest (cn-north-1)\n"
     ]
    }
   ],
   "source": [
    "region_name = boto3.Session().region_name\n",
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-13 05:22:24,020:WARNING:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "t_bt_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.c4.4xlarge',\n",
    "                                         train_volume_size = 120,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)\n",
    "t_bt_model.set_hyperparameters(\n",
    "    mode=\"supervised\",\n",
    "    epochs=20,\n",
    "    min_count=2,\n",
    "    learning_rate=0.1,\n",
    "    vector_dim=10,\n",
    "    early_stopping=True,\n",
    "    patience=4,\n",
    "    min_epochs=5,\n",
    "    word_ngrams=1,\n",
    "    embedding=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-13 05:22:24,027:WARNING:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "2020-10-13 05:22:24,028:WARNING:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "t_train_data = sagemaker.inputs.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/plain', s3_data_type='S3Prefix')\n",
    "t_validation_data = sagemaker.inputs.s3_input(s3_validation_data, distribution='FullyReplicated', \n",
    "                             content_type='text/plain', s3_data_type='S3Prefix')\n",
    "t_data_channels = {'train': t_train_data, 'validation': t_validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-13 05:58:20,165:INFO:Creating training-job with name: blazingtext-2020-10-13-05-58-20-165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-13 05:58:20 Starting - Starting the training job...\n",
      "2020-10-13 05:58:23 Starting - Launching requested ML instances......\n",
      "2020-10-13 05:59:24 Starting - Preparing the instances for training......\n",
      "2020-10-13 06:00:48 Downloading - Downloading input data\n",
      "2020-10-13 06:00:48 Training - Downloading the training image..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[10/13/2020 06:01:02 WARNING 140380736489280] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[10/13/2020 06:01:02 WARNING 140380736489280] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[10/13/2020 06:01:02 INFO 140380736489280] nvidia-smi took: 0.0252020359039 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[10/13/2020 06:01:02 INFO 140380736489280] Running single machine CPU BlazingText training using supervised mode.\u001b[0m\n",
      "\u001b[34m[10/13/2020 06:01:02 INFO 140380736489280] Processing /opt/ml/input/data/train/tt.train . File size: 0 MB\u001b[0m\n",
      "\u001b[34m[10/13/2020 06:01:02 INFO 140380736489280] Processing /opt/ml/input/data/validation/tt.validation . File size: 0 MB\u001b[0m\n",
      "\u001b[34mRead 0M words\u001b[0m\n",
      "\u001b[34mNumber of words:  773\u001b[0m\n",
      "\u001b[34mLoading validation data from /opt/ml/input/data/validation/tt.validation\u001b[0m\n",
      "\u001b[34mLoaded validation data.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 11\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.777618\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0132  Progress: 86.80%  Million Words/sec: 2.30 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 17\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.777618\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: -0.0000  Progress: 100.02%  Million Words/sec: 1.55 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 20\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.784792\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 1.54 #####\n",
      "\u001b[0m\n",
      "\u001b[34mTraining finished.\u001b[0m\n",
      "\u001b[34mAverage throughput in Million words/sec: 1.54\u001b[0m\n",
      "\u001b[34mTotal training time in seconds: 0.25\n",
      "\u001b[0m\n",
      "\u001b[34m#train_accuracy: 0.87\u001b[0m\n",
      "\u001b[34mNumber of train examples: 2801\n",
      "\u001b[0m\n",
      "\u001b[34m#validation_accuracy: 0.7848\u001b[0m\n",
      "\u001b[34mNumber of validation examples: 701\u001b[0m\n",
      "\n",
      "2020-10-13 06:01:19 Uploading - Uploading generated training model\n",
      "2020-10-13 06:01:19 Completed - Training job completed\n",
      "Training seconds: 43\n",
      "Billable seconds: 43\n"
     ]
    }
   ],
   "source": [
    "t_bt_model.fit(inputs=t_data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-13 06:03:27,192:WARNING:Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "2020-10-13 06:03:27,193:INFO:Creating model with name: blazingtext-2020-10-13-05-58-20-165\n",
      "2020-10-13 06:03:27,733:INFO:Creating endpoint with name blazingtext-2020-10-13-05-58-20-165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "t_text_classifier = t_bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.c5.large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total predict:  669  ,more than  0.8  confidence:  372\n"
     ]
    }
   ],
   "source": [
    "predict_category_result_file = 'predict_category_result.csv'\n",
    "low_confidence = [] # confidence<confidence_thredhold\n",
    "confidence_thredhold = 0.8\n",
    "\n",
    "if os.path.isfile(predict_category_result_file):\n",
    "    os.remove(predict_category_result_file)\n",
    "\n",
    "with open(predict_category_result_file, 'a+') as predict_f:\n",
    "    predict_f.write('category' + '|' + 'comment' + '|' + 'confidence' + '\\n')\n",
    "\n",
    "# #单机并行分词\n",
    "# jieba.enable_parallel(8)\n",
    "# #paddle模式，精确匹配需要关闭parallel\n",
    "# #jieba.disable_parallel()\n",
    "# #启动paddle模式\n",
    "# jieba.enable_paddle()\n",
    "\n",
    "for sentences in input_without_category:\n",
    "    counts = {} \n",
    "    # using the same nltk tokenizer that we used during data preparation for training\n",
    "    tokenized_sentences = [' '.join(with_stop_words_cut(sentences, counts))]\n",
    "    #print('sentences: ', sentences, ' , jieba: ', tokenized_sentences)\n",
    "\n",
    "    #payload = {\"instances\" : tokenized_sentences, \"configuration\": {\"k\": 2}}\n",
    "    payload = {\"instances\" : tokenized_sentences}\n",
    "\n",
    "    t_response = t_text_classifier.predict(json.dumps(payload))\n",
    "\n",
    "    t_predictions = json.loads(t_response)\n",
    "    #print(json.dumps(t_predictions, indent=2))\n",
    "    predict_prob = t_predictions[0]['prob'][0]\n",
    "    predict_category = t_predictions[0]['label'][0]\n",
    "    if predict_prob < confidence_thredhold:\n",
    "        low_confidence.append(sentences + '|' + predict_category + '|' + str(predict_prob) + '\\n')\n",
    "    \n",
    "    with open(predict_category_result_file, 'a+') as predict_f:\n",
    "        predict_f.write(sentences + '|' + predict_category + '|' + str(predict_prob) + '\\n')\n",
    "\n",
    "predict_f.close\n",
    "print('total predict: ', len(input_without_category), ' , > ', confidence_thredhold, ' confidence: ', len(input_without_category) - len(low_confidence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-13 07:28:39,347:INFO:Deleting endpoint configuration with name: blazingtext-2020-10-13-05-58-20-165\n",
      "2020-10-13 07:28:39,424:INFO:Deleting endpoint with name: blazingtext-2020-10-13-05-58-20-165\n"
     ]
    }
   ],
   "source": [
    "t_text_classifier.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
