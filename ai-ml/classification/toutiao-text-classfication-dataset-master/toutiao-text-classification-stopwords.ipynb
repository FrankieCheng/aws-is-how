{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对中文新闻题目进行分类\n",
    "\n",
    "本示例使用头条客户端抓取的新闻题目分类，演示如何用Amazon Sagemaker内置算法BlazingText对新闻标题进行分类。\n",
    "\n",
    "原数据集下载地址：https://github.com/skdjfla/toutiao-text-classfication-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws-cn:iam::876820548815:role/service-role/AmazonSageMaker-ExecutionRole-20200520T151303\n",
      "ray-ai-ml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "from random import shuffle\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role) # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf\n",
    "\n",
    "bucket = 'ray-ai-ml' # Replace with your own bucket name if needed\n",
    "print(bucket)\n",
    "prefix = 'classification/blazingtext/supervised/toutiao' #Replace with the prefix under which you want to store the data if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: jieba in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.42.1)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100', '民生']\n",
      "['101', '文化']\n",
      "['102', '娱乐']\n",
      "['103', '体育']\n",
      "['104', '财经']\n",
      "['106', '房产']\n",
      "['107', '汽车']\n",
      "['108', '教育']\n",
      "['109', '科技']\n",
      "['110', '军事']\n",
      "['112', '旅游']\n",
      "['113', '国际']\n",
      "['114', '证券']\n",
      "['115', '农业']\n",
      "['116', '电竞']\n",
      "{'100': '民生', '101': '文化', '102': '娱乐', '103': '体育', '104': '财经', '106': '房产', '107': '汽车', '108': '教育', '109': '科技', '110': '军事', '112': '旅游', '113': '国际', '114': '证券', '115': '农业', '116': '电竞'}\n"
     ]
    }
   ],
   "source": [
    "index_to_label = {} \n",
    "with open(\"classes.txt\") as f:\n",
    "    for i,label in enumerate(f.readlines()):\n",
    "        ll = label.strip().split(',')\n",
    "        index_to_label[ll[0]] = ll[1]\n",
    "        print(ll)\n",
    "print(index_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.871 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['__label__军事', '929', '波音', '俄', '大客机', '中', '787', '年', '对标', '宽体', '首飞', '2025'], ['__label__汽车', '谍照', '路试', '新款', '道奇'], ['__label__教育', '语文', '挂', '年', '142', '力荐', '保管', '董卿', '成语', '墙上', '打印', '背熟', '1500', '孩子'], ['__label__科技', '跨境', '运动', '下波', '造富', '商会', '电'], ['__label__体育', '常规赛', '火箭', '难', '惨不忍睹', '复苏', '戈登', '对比', '数据', '季后赛']]\n"
     ]
    }
   ],
   "source": [
    "# define stopwords\n",
    "def get_stopwords():  \n",
    "    #加载停用词表  \n",
    "    stopword_set = set()  \n",
    "    with open(\"zhstopwords.txt\",'r',encoding=\"utf-8\") as stopwords:  \n",
    "        for stopword in stopwords:  \n",
    "            stopword_set.add(stopword.strip(\"\\n\"))  \n",
    "    return stopword_set\n",
    "\n",
    "# Parse chinese with stopwords\n",
    "def parse_zh_words(read_file_path):\n",
    "    \n",
    "    #get stopwords\n",
    "    stopword_set = get_stopwords()\n",
    "#     i = 0\n",
    "#     for x in stopword_set:\n",
    "#         if i > 10:\n",
    "#             break\n",
    "#         print(x)\n",
    "#         i += 1\n",
    "    \n",
    "    with open(read_file_path) as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    labels = []\n",
    "    for line in lines:\n",
    "        label = []\n",
    "        line = line.split('_!_')\n",
    "        label_code = index_to_label[line[1]]\n",
    "        label.append('__label__' + label_code)\n",
    "        line[3] = re.sub(r\"[\\s+\\.\\!\\/_,$%^*()?;；:-【】+\\\"\\']+|[+——一！，;:：。？、~@#￥%……&*（）]+\", \"\", line[3])\n",
    "        words = jieba.cut(line[3],cut_all=False)\n",
    "    \n",
    "        filled_words = set()\n",
    "        for word in words:\n",
    "            if word not in stopword_set:\n",
    "                filled_words.add(word)\n",
    "\n",
    "        label.extend(filled_words)\n",
    "        #print(label)\n",
    "        labels.append(label)\n",
    "    \n",
    "    shuffle(labels)\n",
    "    return labels\n",
    "    \n",
    "file  = 'toutiao_cat_data.txt'#'toutiao_cat_data.txt'\n",
    "labels = parse_zh_words(file)\n",
    "    \n",
    "shuffle(labels)\n",
    "print(labels[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'classification/blazingtext/supervised/toutiao'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train_data = labels[0:int(len(labels)*0.8)]\n",
    "t_validation_data = labels[int(len(labels)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['__label__国际', '时', '不好', '意义', '驱逐', '对方', '外交官', '两国关系'],\n",
       " ['__label__财经', '生物科技', '股有', '股中'],\n",
       " ['__label__财经', '茅台镇', '喝', '茅台酒', '老百姓'],\n",
       " ['__label__房产', '买房子', '去', '莱山区', '更好', '开发区', '烟台'],\n",
       " ['__label__电竞', '遇挫', '模仿', '抄袭', '游戏', '频频', '自主', '腾讯', '开发'],\n",
       " ['__label__农业',\n",
       "  '有人',\n",
       "  '农村',\n",
       "  '封杀',\n",
       "  '惨',\n",
       "  '成线',\n",
       "  '走出',\n",
       "  '明星',\n",
       "  '霸屏',\n",
       "  '大',\n",
       "  '却',\n",
       "  '小花',\n",
       "  '多年'],\n",
       " ['__label__汽车', '利用', '农村', '喇叭', '活动', '交警', '大', '教育', '交通安全', '临漳'],\n",
       " ['__label__汽车',\n",
       "  '几招',\n",
       "  '车身',\n",
       "  '抖动',\n",
       "  '汽车',\n",
       "  '决不能',\n",
       "  '忽视',\n",
       "  '放出',\n",
       "  '危险',\n",
       "  '教',\n",
       "  '找到',\n",
       "  '信号',\n",
       "  '这是',\n",
       "  '原因'],\n",
       " ['__label__体育', '地上', '球员', '在场', '足球', '唾沫', '都', '高强度', '运动', '吐', '运动员'],\n",
       " ['__label__文化', '考述', '毛氏', '刻本', '汲古阁', '特色', '价值'],\n",
       " ['__label__电竞', '游', '总', '换部', '玩手', '像样', '不爽', '手机'],\n",
       " ['__label__军事', '时', '美国', '投', '没用', '直', '原子弹', '越南战争', '胜利'],\n",
       " ['__label__房产', '不涨', '房价', '涨工资', '涨']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_train_data[0:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "t_train_file = 'tt.train'\n",
    "t_validation_file = 'tt.validation'\n",
    "\n",
    "with open(t_train_file, 'w') as csvoutfile:\n",
    "    csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "    csv_writer.writerows(t_train_data)\n",
    "    \n",
    "with open(t_validation_file, 'w') as csvoutfile:\n",
    "    csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "    csv_writer.writerows(t_validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 294 ms, sys: 67.2 ms, total: 361 ms\n",
      "Wall time: 681 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "t_train_channel = prefix + '/train'\n",
    "t_validation_channel = prefix + '/validation'\n",
    "\n",
    "sess.upload_data(path='tt.train', bucket=bucket, key_prefix=t_train_channel)\n",
    "sess.upload_data(path='tt.validation', bucket=bucket, key_prefix=t_validation_channel)\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, t_train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket, t_validation_channel)\n",
    "\n",
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker BlazingText container: 387376663083.dkr.ecr.cn-northwest-1.amazonaws.com.cn/blazingtext:latest (cn-northwest-1)\n"
     ]
    }
   ],
   "source": [
    "region_name = boto3.Session().region_name\n",
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_bt_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.c4.4xlarge',\n",
    "                                         train_volume_size = 30,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)\n",
    "t_bt_model.set_hyperparameters(mode=\"supervised\",\n",
    "                            epochs=10,\n",
    "                            min_count=2,\n",
    "                            learning_rate=0.05,\n",
    "                            vector_dim=10,\n",
    "                            early_stopping=True,\n",
    "                            patience=4,\n",
    "                            min_epochs=5,\n",
    "                            word_ngrams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train_data = sagemaker.inputs.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/plain', s3_data_type='S3Prefix')\n",
    "t_validation_data = sagemaker.inputs.s3_input(s3_validation_data, distribution='FullyReplicated', \n",
    "                             content_type='text/plain', s3_data_type='S3Prefix')\n",
    "t_data_channels = {'train': t_train_data, 'validation': t_validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-19 07:30:34 Starting - Starting the training job...\n",
      "2020-06-19 07:30:38 Starting - Launching requested ML instances......\n",
      "2020-06-19 07:31:43 Starting - Preparing the instances for training......\n",
      "2020-06-19 07:32:56 Downloading - Downloading input data...\n",
      "2020-06-19 07:33:31 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[06/19/2020 07:33:31 WARNING 140138826430272] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[06/19/2020 07:33:31 WARNING 140138826430272] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[06/19/2020 07:33:31 INFO 140138826430272] nvidia-smi took: 0.0251858234406 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[06/19/2020 07:33:31 INFO 140138826430272] Running single machine CPU BlazingText training using supervised mode.\u001b[0m\n",
      "\u001b[34m[06/19/2020 07:33:31 INFO 140138826430272] Processing /opt/ml/input/data/train/tt.train . File size: 20 MB\u001b[0m\n",
      "\u001b[34m[06/19/2020 07:33:31 INFO 140138826430272] Processing /opt/ml/input/data/validation/tt.validation . File size: 5 MB\u001b[0m\n",
      "\u001b[34mRead 2M words\u001b[0m\n",
      "\u001b[34mNumber of words:  79788\u001b[0m\n",
      "\u001b[34mLoading validation data from /opt/ml/input/data/validation/tt.validation\u001b[0m\n",
      "\u001b[34mLoaded validation data.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 2\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0327  Progress: 34.54%  Million Words/sec: 20.48 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 3\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0292  Progress: 41.69%  Million Words/sec: 20.61 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 4\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0256  Progress: 48.84%  Million Words/sec: 20.70 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0220  Progress: 55.98%  Million Words/sec: 20.77 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 5\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.860318\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0178  Progress: 64.49%  Million Words/sec: 17.54 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 6\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.860684\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0129  Progress: 74.17%  Million Words/sec: 15.90 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 7\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.861611\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0086  Progress: 82.79%  Million Words/sec: 14.34 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 8\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.862147\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0042  Progress: 91.55%  Million Words/sec: 13.61 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 9\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.86182\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0013  Progress: 97.43%  Million Words/sec: 12.92 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 10\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.862055\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 2 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 12.59 #####\u001b[0m\n",
      "\u001b[34mTraining finished.\u001b[0m\n",
      "\u001b[34mAverage throughput in Million words/sec: 12.59\u001b[0m\n",
      "\u001b[34mTotal training time in seconds: 2.38\n",
      "\u001b[0m\n",
      "\u001b[34m#train_accuracy: 0.9934\u001b[0m\n",
      "\u001b[34mNumber of train examples: 306150\n",
      "\u001b[0m\n",
      "\u001b[34m#validation_accuracy: 0.8621\u001b[0m\n",
      "\u001b[34mNumber of validation examples: 76538\u001b[0m\n",
      "\n",
      "2020-06-19 07:33:58 Uploading - Uploading generated training model\n",
      "2020-06-19 07:33:58 Completed - Training job completed\n",
      "Training seconds: 62\n",
      "Billable seconds: 62\n"
     ]
    }
   ],
   "source": [
    "t_bt_model.fit(inputs=t_data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "t_text_classifier = t_bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.m4.2xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"prob\": [\n",
      "      0.9997926354408264\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__\\u79d1\\u6280\"\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__label__科技']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = \"亚马逊云计算Q1营收过百亿\"#\"宝马推出新车型/亚马逊云计算Q1营收过百亿/美国航空母舰开往伊朗波斯湾/北京迎来黄金周小高峰/C罗纳尔多还是梅西/邓超加入春晚/新冠肺炎全球蔓延急需疫苗/谁是最可爱的人\"\n",
    "\n",
    "# using the same nltk tokenizer that we used during data preparation for training\n",
    "tokenized_sentences = [' '.join(jieba.cut(sentences,cut_all=False))]\n",
    "\n",
    "payload = {\"instances\" : tokenized_sentences}\n",
    "\n",
    "# payload = {\"instances\" : tokenized_sentences,\n",
    "#           \"configuration\": {\"k\": 2}}\n",
    "\n",
    "t_response = t_text_classifier.predict(json.dumps(payload))\n",
    "\n",
    "t_predictions = json.loads(t_response)\n",
    "print(json.dumps(t_predictions, indent=2))\n",
    "t_predictions[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tears down the SageMaker endpoint and endpoint configuration\n",
    "t_text_classifier.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
