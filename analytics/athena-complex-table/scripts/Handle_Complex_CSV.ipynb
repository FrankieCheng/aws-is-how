{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-structured data and Nested Struct handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import awswrangler for easy access to glue catalog and Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting awswrangler\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/aa/19/859c65e265fc38fa29afbc6f9766a12df1b51d020d68ee3fcc548eae610c/awswrangler-2.1.0-py3-none-any.whl (150 kB)\n",
      "\u001b[K     |████████████████████████████████| 150 kB 49.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: boto3<2.0.0,>=1.12.49 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awswrangler) (1.16.19)\n",
      "Collecting redshift-connector~=2.0.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/2f/73/fe9ca0306d0fa5a56b8ae65b2b7cb11849f70e15d0efd9c2222ed01c507f/redshift_connector-2.0.872-py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 161 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pandas<1.2.0,>=1.1.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c3/e2/00cacecafbab071c787019f00ad84ca3185952f6bb9bca9550ed83870d4d/pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.5 MB 56.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: botocore<2.0.0,>=1.15.49 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awswrangler) (1.19.19)\n",
      "Requirement already satisfied: numpy<1.20.0,>=1.18.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awswrangler) (1.18.1)\n",
      "Collecting pg8000~=1.16.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/47/7b/0770c960a1e5407abd9cccbc51b21503ae05887718030b1b21e25c2b5e8a/pg8000-1.16.6-py3-none-any.whl (24 kB)\n",
      "Collecting pyarrow~=2.0.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.7 MB 63.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pymysql<0.11.0,>=0.9.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/1a/ea/dd9c81e2d85efd03cfbf808736dd055bd9ea1a78aea9968888b1055c3263/PyMySQL-0.10.1-py2.py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 434 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3<2.0.0,>=1.12.49->awswrangler) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3<2.0.0,>=1.12.49->awswrangler) (0.3.3)\n",
      "Collecting scramp>=1.2.0<1.3.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/0a/86/7ef1b93e8f453f297303e98869451e544588e8d76f2dd73ad17e8dabc5fc/scramp-1.2.0-py3-none-any.whl (6.3 kB)\n",
      "Collecting pytz>=2020.1<2020.2\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/12/f8/ff09af6ff61a3efaad5f61ba5facdf17e7722c4393f7d8a66674d2dbd29f/pytz-2020.4-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[K     |████████████████████████████████| 509 kB 39.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.7.0<4.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from redshift-connector~=2.0.0->awswrangler) (4.8.2)\n",
      "Requirement already satisfied: lxml>=4.2.5<4.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from redshift-connector~=2.0.0->awswrangler) (4.6.1)\n",
      "Collecting requests>=2.23.0<2.24.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 6.4 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas<1.2.0,>=1.1.0->awswrangler) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<2.0.0,>=1.15.49->awswrangler) (1.25.10)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from beautifulsoup4>=4.7.0<4.8.0->redshift-connector~=2.0.0->awswrangler) (1.9.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.23.0<2.24.0->redshift-connector~=2.0.0->awswrangler) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.23.0<2.24.0->redshift-connector~=2.0.0->awswrangler) (2.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.23.0<2.24.0->redshift-connector~=2.0.0->awswrangler) (3.0.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas<1.2.0,>=1.1.0->awswrangler) (1.14.0)\n",
      "Installing collected packages: scramp, pytz, requests, redshift-connector, pandas, pg8000, pyarrow, pymysql, awswrangler\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2019.3\n",
      "    Uninstalling pytz-2019.3:\n",
      "      Successfully uninstalled pytz-2019.3\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.22.0\n",
      "    Uninstalling requests-2.22.0:\n",
      "      Successfully uninstalled requests-2.22.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.1\n",
      "    Uninstalling pandas-1.0.1:\n",
      "      Successfully uninstalled pandas-1.0.1\n",
      "Successfully installed awswrangler-2.1.0 pandas-1.1.5 pg8000-1.16.6 pyarrow-2.0.0 pymysql-0.10.1 pytz-2020.4 redshift-connector-2.0.872 requests-2.25.1 scramp-1.2.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple awswrangler\n",
    "import awswrangler as wr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample: Using code to create the glue catalog and athena query\n",
    "Here I show the demo how to handle the json array with structure schema\n",
    "- You can run crawler to create the table\n",
    "- You can also use the code to create the table\n",
    "  - You can also create a Parquet Table (Metadata Only) in the AWS Glue Catalog. \n",
    "  - Or you can create the external table on Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the external table (AWS Glue Catalog table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B                             C\n",
      "0  foo  bar   {'C1': 'dummyC1', 'C2': 10}\n",
      "1  foo  bar  {'C1': 'dummyC11', 'C2': 11}\n",
      "Athena query 7a4d9553-e828-449b-b1dd-0b9de02fea93 result: SUCCEEDED\n",
      "string\n",
      "string\n",
      "struct<c1:string,c2:int>\n",
      "string\n",
      "int\n",
      "string\n",
      "string\n",
      "struct<c1:string,c2:int>\n",
      "string\n",
      "int\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'paths': ['s3://ray-glue-streaming/catalog_test/parquet/1696c6cf44d248099b82679f66300dce.snappy.parquet'],\n",
       " 'partitions_values': {}}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_bucket = 'ray-glue-streaming'\n",
    "s3_csv_prefix = 'catalog_test/complextable/' \n",
    "s3_json_prefix = 'catalog_test/json/'\n",
    "s3_parquet_prefix = 'catalog_test/parquet/'\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "json_file_path = 's3://{}/{}'.format(s3_bucket, s3_json_prefix)\n",
    "parquet_file_path = 's3://{}/{}'.format(s3_bucket, s3_parquet_prefix)\n",
    "\n",
    "data = {\"A\": \"foo\", \"B\": \"bar\", \"C\":[{\"C1\":\"dummyC1\", \"C2\":10},{\"C1\":\"dummyC11\", \"C2\":11}]}\n",
    "df = pd.DataFrame(data)\n",
    "print(df.head())\n",
    "\n",
    "# Option1: Athena create external table\n",
    "# reference doc: https://aws.amazon.com/cn/blogs/big-data/create-tables-in-amazon-athena-from-nested-json-and-mappings-using-jsonserde/\n",
    "query = r'''CREATE EXTERNAL TABLE IF NOT EXISTS `sampledb`.`json_structure` (\n",
    "      `A` string, \n",
    "      `B` string, \n",
    "      `C` struct<c1:string,c2:int>\n",
    "      )\n",
    "    ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\n",
    "    WITH SERDEPROPERTIES ('ignore.malformed.json' = 'true', 'paths'='A,B,C')\n",
    "    STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' \n",
    "    OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
    "    LOCATION '{}'\n",
    "    '''\n",
    "query = query.format(json_file_path)\n",
    "    \n",
    "query_exec_id = wr.athena.start_query_execution(sql=query, database=\"sampledb\")\n",
    "wr.athena.wait_query(query_execution_id=query_exec_id)\n",
    "res = wr.athena.get_query_execution(query_execution_id=query_exec_id)\n",
    "print(\"Athena query {} result: {}\".format(query_exec_id, res[\"Status\"][\"State\"]))\n",
    "\n",
    "# Upload to S3: Option1: Save to temporay file and Use the S3 client uplaod to S3\n",
    "df.to_json(\"json_info.json\", orient=\"records\", lines=True)\n",
    "s3_client.upload_file(Filename='json_info.json', Bucket=s3_bucket, Key=s3_json_prefix+\"json_info.json\")\n",
    "# json.load Not well support array structure\n",
    "# with open('json_info.json') as f:\n",
    "#   parsed = json.load(f)\n",
    "# print(parsed)\n",
    "\n",
    "# Upload to S3: Option2: use the aws-data-wrangler https://aws-data-wrangler.readthedocs.io/en/stable/\n",
    "wr.s3.to_json(\n",
    "        df=df,\n",
    "        path=json_file_path+'wr_s3_json_info.json',\n",
    "        orient=\"records\", \n",
    "        lines=True\n",
    "    )\n",
    "\n",
    "#Option2: wr.catalog.create_parquet_table, however, to_parquet right now can not well support structure data\n",
    "wr.catalog.create_parquet_table(database=\"sampledb\", table=\"complextable_parquet\", path=parquet_file_path, \n",
    "                                columns_types={\"A\": \"string\", \"B\": \"string\", \"C\":\"struct<C1:string,C2:int>\"}\n",
    "                               )\n",
    "wr.s3.to_parquet(\n",
    "    df, path=parquet_file_path, dataset=True, database=\"sampledb\", table=\"complextable_parquet\", \n",
    "    dtype={\"A\": \"string\", \"B\": \"string\", \"C\":\"struct<C1:string,C2:int>\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Athena Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     a    b                             c\n",
      "0  foo  bar   {'c1': 'dummyC1', 'c2': 10}\n",
      "1  foo  bar  {'c1': 'dummyC11', 'c2': 11}\n",
      "0  foo  bar   {'c1': 'dummyC1', 'c2': 10}\n",
      "1  foo  bar  {'c1': 'dummyC11', 'c2': 11}\n",
      "0.19921875\n",
      "     a        c1  c2\n",
      "0  foo   dummyC1  10\n",
      "1  foo  dummyC11  11\n",
      "0  foo   dummyC1  10\n",
      "1  foo  dummyC11  11\n",
      "0.19921875\n",
      "     a    b                         c\n",
      "0  foo  bar  {'c1': None, 'c2': None}\n",
      "1  foo  bar  {'c1': None, 'c2': None}\n",
      "2  foo  bar  {'c1': None, 'c2': None}\n",
      "3  foo  bar  {'c1': None, 'c2': None}\n",
      "4.638671875\n"
     ]
    }
   ],
   "source": [
    "query = r'''SELECT * FROM json_structure limit 10\n",
    "    '''\n",
    "df = wr.athena.read_sql_query(sql=query, database=\"sampledb\")\n",
    "print(df.head())\n",
    "scanned_bytes = df.query_metadata[\"Statistics\"][\"DataScannedInBytes\"]\n",
    "print(scanned_bytes/1024)\n",
    "\n",
    "query = r'''\n",
    "SELECT A as A,\n",
    "       C.C1 as C1, \n",
    "       C.C2 as C2\n",
    "FROM json_structure\n",
    "limit 10\n",
    "'''\n",
    "df = wr.athena.read_sql_query(sql=query, database=\"sampledb\")\n",
    "print(df.head())\n",
    "# Check how many date athen scanned to get the result\n",
    "scanned_bytes = df.query_metadata[\"Statistics\"][\"DataScannedInBytes\"]\n",
    "print(scanned_bytes/1024)\n",
    "\n",
    "query = r'''SELECT * FROM complextable_parquet limit 10\n",
    "    '''\n",
    "df = wr.athena.read_sql_query(sql=query, database=\"sampledb\")\n",
    "print(df.head())\n",
    "# Check how many date athen scanned to get the result\n",
    "scanned_bytes = df.query_metadata[\"Statistics\"][\"DataScannedInBytes\"]\n",
    "print(scanned_bytes/1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample: Handle complex csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the csv file from S3\n",
    "```python\n",
    "raw_board_csv_df = pd.read_csv(s3.open(file_path,mode='rb'), index_col=None, nrows=1, skipinitialspace=True, delim_whitespace=True)\n",
    "raw_board_csv_df = wr.s3.read_csv(file_path, index_col=None, nrows=1, skipinitialspace=True, delim_whitespace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: s3fs in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.4.2)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from s3fs) (0.6.2)\n",
      "Requirement already satisfied: botocore>=1.12.91 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from s3fs) (1.19.19)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (1.25.10)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.12.91->s3fs) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Reading remote files\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#reading-remote-files\n",
    "# !pip install -i https://pypi.tuna.tsinghua.edu.cn/simple s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://ray-glue-streaming/catalog_test/complextable/MN63459620201110165647.csv\n",
      "    BARCODE  INDEX        DATE    S.TIME    E.TIME  CYCLE                 JOB  \\\n",
      "0  M4634596  24878  11/10/2020  16:56:38  16:56:46      8  A5E41637164-04-TOP   \n",
      "1  M4634596  24878  11/10/2020  16:56:38  16:56:46      8  A5E41637164-04-TOP   \n",
      "2  M4634596  24878  11/10/2020  16:56:38  16:56:46      8  A5E41637164-04-TOP   \n",
      "3  M4634596  24878  11/10/2020  16:56:38  16:56:46      8  A5E41637164-04-TOP   \n",
      "4  M4634596  24878  11/10/2020  16:56:38  16:56:46      8  A5E41637164-04-TOP   \n",
      "\n",
      "  RESULT USER   LOTINFO MACHINE  SIDE  \\\n",
      "0   GOOD   SV  KOHYOUNG       T   NaN   \n",
      "1   GOOD   SV  KOHYOUNG       T   NaN   \n",
      "2   GOOD   SV  KOHYOUNG       T   NaN   \n",
      "3   GOOD   SV  KOHYOUNG       T   NaN   \n",
      "4   GOOD   SV  KOHYOUNG       T   NaN   \n",
      "\n",
      "                              aggrate_component_info  \n",
      "0  {'ComponentID': '1:00', 'Volume_percentage_': ...  \n",
      "1  {'ComponentID': '1:00', 'Volume_percentage_': ...  \n",
      "2  {'ComponentID': '1:C600_04', 'Volume_percentag...  \n",
      "3  {'ComponentID': '1:00', 'Volume_percentage_': ...  \n",
      "4  {'ComponentID': '1:00', 'Volume_percentage_': ...  \n",
      "raw_data_json_df size:  (2584, 13)\n",
      "{'ComponentID': '1:00', 'Volume_percentage_': 118.49600000000001, 'Height_um_': 137.565, 'Area_percentage_': 103.366, 'OffsetX_percentage_': 0.43, 'OffsetY_percentage_': -5.175, 'Volume_um3_': 54887270, 'Area_um2_': 398990, 'Result': 'GOOD', 'PinNumber': nan, 'PadVerification': nan, 'Shape': '56.4um', 'Library_Name': 'PART0', 'Vol_Min_percentage_': 45, 'Vol_Max_percentage_': 200, 'Height_Low_um_': 60, 'Height_High_um_': 230, 'Area_Min_percentage_': 60, 'Area_Max_percentage_': 170, 'OffsetX_Error_mm_': 0.18, 'OffsetY_Error_mm_': 0.18}\n",
      "upload to S3 s3://ray-glue-streaming/catalog_test/complex_json/MN63459620201110165647.csv.json\n",
      "s3://ray-glue-streaming/catalog_test/complextable/MN63485020201110164344.csv\n",
      "    BARCODE  INDEX        DATE    S.TIME    E.TIME  CYCLE                 JOB  \\\n",
      "0  MN634850  24857  11/10/2020  16:43:32  16:43:40      8  A5E41637164-04-TOP   \n",
      "1  MN634850  24857  11/10/2020  16:43:32  16:43:40      8  A5E41637164-04-TOP   \n",
      "2  MN634850  24857  11/10/2020  16:43:32  16:43:40      8  A5E41637164-04-TOP   \n",
      "3  MN634850  24857  11/10/2020  16:43:32  16:43:40      8  A5E41637164-04-TOP   \n",
      "4  MN634850  24857  11/10/2020  16:43:32  16:43:40      8  A5E41637164-04-TOP   \n",
      "\n",
      "  RESULT USER   LOTINFO  MACHINE  \\\n",
      "0   FAIL   SV  KOHYOUNG      NaN   \n",
      "1   FAIL   SV  KOHYOUNG      NaN   \n",
      "2   FAIL   SV  KOHYOUNG      NaN   \n",
      "3   FAIL   SV  KOHYOUNG      NaN   \n",
      "4   FAIL   SV  KOHYOUNG      NaN   \n",
      "\n",
      "                              aggrate_component_info  \n",
      "0  {'ComponentID': '1:', 'Volume_percentage_': 12...  \n",
      "1  {'ComponentID': '1:', 'Volume_percentage_': 10...  \n",
      "2  {'ComponentID': '1:C600_04', 'Volume_percentag...  \n",
      "3  {'ComponentID': '1:', 'Volume_percentage_': 10...  \n",
      "4  {'ComponentID': '1:', 'Volume_percentage_': 98...  \n",
      "raw_data_json_df size:  (2584, 12)\n",
      "{'ComponentID': '1:', 'Volume_percentage_': 120.896, 'Height_um_': 141.464, 'Area_percentage_': 102.553, 'OffsetX_percentage_': 0.562, 'OffsetY_percentage_': -10.390999999999998, 'Volume_um3_': 55998940, 'Area_um2_': 395853, 'Result': 'GOOD', 'PinNumber': nan, 'PadVerification': nan, 'Shape': '65.8um', 'Library_Name': 'PART0', 'Vol_Min_percentage_': 45, 'Vol_Max_percentage_': 200, 'Height_Low_um_': 60, 'Height_High_um_': 230, 'Area_Min_percentage_': 60, 'Area_Max_percentage_': 170, 'OffsetX_Error_mm_': 0.18, 'OffsetY_Error_mm_': 0.18, 'Unnamed_21': nan}\n",
      "upload to S3 s3://ray-glue-streaming/catalog_test/complex_json/MN63485020201110164344.csv.json\n",
      "s3://ray-glue-streaming/catalog_test/complextable/MN63558220201110115418.csv\n",
      "    BARCODE  INDEX        DATE    S.TIME    E.TIME  CYCLE                 JOB  \\\n",
      "0  MN635582  24566  11/10/2020  11:54:09  11:54:15      6  A5E41637164-04-BOT   \n",
      "1  MN635582  24566  11/10/2020  11:54:09  11:54:15      6  A5E41637164-04-BOT   \n",
      "2  MN635582  24566  11/10/2020  11:54:09  11:54:15      6  A5E41637164-04-BOT   \n",
      "3  MN635582  24566  11/10/2020  11:54:09  11:54:15      6  A5E41637164-04-BOT   \n",
      "4  MN635582  24566  11/10/2020  11:54:09  11:54:15      6  A5E41637164-04-BOT   \n",
      "\n",
      "  RESULT USER   LOTINFO  MACHINE  \\\n",
      "0   FAIL   SV  KOHYOUNG      NaN   \n",
      "1   FAIL   SV  KOHYOUNG      NaN   \n",
      "2   FAIL   SV  KOHYOUNG      NaN   \n",
      "3   FAIL   SV  KOHYOUNG      NaN   \n",
      "4   FAIL   SV  KOHYOUNG      NaN   \n",
      "\n",
      "                              aggrate_component_info  \n",
      "0  {'ComponentID': '1:', 'Volume_percentage_': 10...  \n",
      "1  {'ComponentID': '1:', 'Volume_percentage_': 10...  \n",
      "2  {'ComponentID': '1:R615_04', 'Volume_percentag...  \n",
      "3  {'ComponentID': '1:R615_04', 'Volume_percentag...  \n",
      "4  {'ComponentID': '1:X161_04', 'Volume_percentag...  \n",
      "raw_data_json_df size:  (1792, 12)\n",
      "{'ComponentID': '1:', 'Volume_percentage_': 105.85700000000001, 'Height_um_': 125.15299999999999, 'Area_percentage_': 101.49799999999999, 'OffsetX_percentage_': -0.04, 'OffsetY_percentage_': 0.47, 'Volume_um3_': 31230310, 'Area_um2_': 249537, 'Result': 'GOOD', 'PinNumber': nan, 'PadVerification': nan, 'Shape': '45.6um', 'Library_Name': 'PART2', 'Vol_Min_percentage_': 45, 'Vol_Max_percentage_': 190, 'Height_Low_um_': 60, 'Height_High_um_': 230, 'Area_Min_percentage_': 60, 'Area_Max_percentage_': 170, 'OffsetX_Error_mm_': 0.18, 'OffsetY_Error_mm_': 0.18, 'Unnamed_21': nan}\n",
      "upload to S3 s3://ray-glue-streaming/catalog_test/complex_json/MN63558220201110115418.csv.json\n"
     ]
    }
   ],
   "source": [
    "import boto\n",
    "import json\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "    List objects in an S3 bucket.\n",
    "\n",
    "    :param bucket: Name of the S3 bucket.\n",
    "    :param prefix: Only fetch objects whose key starts with\n",
    "        this prefix (optional).\n",
    "    :param suffix: Only fetch objects whose keys end with\n",
    "        this suffix (optional).\n",
    "\"\"\"\n",
    "def get_all_s3_objects(bucket, prefix='', suffix=''):\n",
    "    \"\"\"Get a list of all keys in an S3 bucket.\"\"\"\n",
    "    paginator = s3_client.get_paginator(\"list_objects_v2\")\n",
    "    \n",
    "    kwargs = {'Bucket': bucket}\n",
    "    if isinstance(prefix, str):\n",
    "        prefixes = (prefix, )\n",
    "    else:\n",
    "        prefixes = prefix\n",
    "        \n",
    "    for key_prefix in prefixes:\n",
    "        kwargs[\"Prefix\"] = key_prefix\n",
    "\n",
    "        for page in paginator.paginate(**kwargs):\n",
    "            try:\n",
    "                contents = page[\"Contents\"]\n",
    "            except KeyError:\n",
    "                break\n",
    "\n",
    "            for obj in contents:\n",
    "                key = obj[\"Key\"]\n",
    "                if key.endswith(suffix) and key != prefix:\n",
    "                    yield obj\n",
    "\n",
    "def get_all_s3_keys(bucket, prefix=\"\", suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Generate the keys in an S3 bucket.\n",
    "\n",
    "    :param bucket: Name of the S3 bucket.\n",
    "    :param prefix: Only fetch keys that start with this prefix (optional).\n",
    "    :param suffix: Only fetch keys that end with this suffix (optional).\n",
    "    \"\"\"\n",
    "    for obj in get_all_s3_objects(bucket, prefix, suffix):\n",
    "        yield obj[\"Key\"]\n",
    "\n",
    "\"\"\"\n",
    "    conver the csv to json as \n",
    "    {barcode=xxx, index=xxx, ... job= xxx, component=[{Component ID=aaa, Volume=bbb, Height=ccc ...}, {Component ID=aaa1, Volume=bbb1, Height=ccc1 ...}, ....]}\n",
    "\"\"\"\n",
    "s3_csv_prefix = 'catalog_test/complextable/'\n",
    "s3_json_prefix = 'catalog_test/complex_json/'\n",
    "for s3_key in get_all_s3_keys(s3_bucket, s3_csv_prefix):\n",
    "    file_path = 's3://{}/{}'.format(s3_bucket, s3_key)\n",
    "    print(file_path)\n",
    "    \n",
    "    # Read the s3 csv file and create the DataFrame for BoardInfo with first 2 lines (L1 as header, L2 as data)\n",
    "    # Option1：use the native pandas\n",
    "    #raw_board_csv_df = pd.read_csv(s3.open(file_path,mode='rb'), index_col=None, nrows=1, skipinitialspace=True, delim_whitespace=True)\n",
    "    # Option2: use the aws-data-wrangler https://aws-data-wrangler.readthedocs.io/en/stable/\n",
    "    raw_board_csv_df = wr.s3.read_csv(file_path, index_col=None, nrows=1, skipinitialspace=True, delim_whitespace=True, keep_default_na=True, na_filter=True)\n",
    "    #print(raw_board_csv_df.head())\n",
    "    \n",
    "    # Read the s3 csv file and create the DataFrame for ComponetInfo with rest lines (L3 as header, others as data)\n",
    "    # Option1：use the native pandas\n",
    "    #raw_component_csv_df = pd.read_csv(s3.open(file_path,mode='rb'), index_col=None, header=2, delimiter=',', skipinitialspace=True)\n",
    "    # Option2: use the aws-data-wrangler https://aws-data-wrangler.readthedocs.io/en/stable/\n",
    "    raw_component_csv_df = wr.s3.read_csv(file_path, index_col=None, header=2, delimiter=',', skipinitialspace=True, keep_default_na=True, na_filter=True)\n",
    "    #print(raw_component_csv_df.head())\n",
    "    \n",
    "    \n",
    "    # Iterate the DataFrame and convert to json\n",
    "    raw_data_json = {}\n",
    "    # BoardInfo\n",
    "    for index, data in raw_board_csv_df.iterrows():\n",
    "        for column_name in raw_board_csv_df.columns:\n",
    "            column_name_str = column_name.replace(\" \", \"\")\n",
    "            column_name_str = column_name_str.replace(\"(\", \"_\")\n",
    "            column_name_str = column_name_str.replace(\")\", \"_\")\n",
    "            column_name_str = column_name_str.replace(\"%\", \"percentage\")\n",
    "            column_name_str = column_name_str.replace(\":\", \"_\")\n",
    "            raw_data_json[column_name_str] = data[column_name]\n",
    "    #print('converted_raw_data:', json.dumps(raw_data_json))\n",
    "    \n",
    "    # ComponentInfo\n",
    "    component_json_array = []\n",
    "    for index, data in raw_component_csv_df.iterrows():\n",
    "        component_json = {}\n",
    "        for column_name in raw_component_csv_df.columns:\n",
    "            column_name_str = column_name.replace(\" \", \"\")\n",
    "            column_name_str = column_name_str.replace(\"(\", \"_\")\n",
    "            column_name_str = column_name_str.replace(\")\", \"_\")\n",
    "            column_name_str = column_name_str.replace(\"%\", \"percentage\")\n",
    "            column_name_str = column_name_str.replace(\":\", \"_\")\n",
    "            component_json[column_name_str] = data[column_name]\n",
    "        component_json_array.append(component_json)\n",
    "    raw_data_json['aggrate_component_info']=component_json_array\n",
    "    \n",
    "    # Convert the dict to DataFrame\n",
    "    raw_data_json_df = pd.DataFrame.from_dict(raw_data_json)\n",
    "    print(raw_data_json_df.head())\n",
    "    print('raw_data_json_df size: ', raw_data_json_df.shape)\n",
    "    print(raw_data_json_df.at[0, 'aggrate_component_info'])\n",
    "    \n",
    "    # upload to S3\n",
    "    s3_json_key = s3_json_prefix + os.path.basename(s3_key) + '.json'\n",
    "    upload_file_path = 's3://{}/{}'.format(s3_bucket, s3_json_key)\n",
    "    print('upload to S3 {}'.format(upload_file_path))\n",
    "    # json.dump can NOT support flatten array to each line\n",
    "    #with open(\"aggrate_component_info.json\", \"w\") as outfile: \n",
    "    #    json.dump(raw_data_json, outfile)\n",
    "    # upload to S3 # Option1: Save to temporay file and uplaod to S3, then Use the S3 client, here orient=\"records\", lines=True to flatten array to each line\n",
    "    #raw_data_json_df.to_json(\"aggrate_component_info.json\", orient=\"records\", lines=True)\n",
    "    #s3_client.upload_file(Filename='aggrate_component_info.json', Bucket=s3_bucket, Key=s3_json_key)\n",
    "    \n",
    "    # Option2: use the aws-data-wrangler https://aws-data-wrangler.readthedocs.io/en/stable/, here orient=\"records\", lines=True to flatten array to each line\n",
    "    wr.s3.to_json(\n",
    "        df=raw_data_json_df,\n",
    "        path=upload_file_path,\n",
    "        orient=\"records\", \n",
    "        lines=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the external table (AWS Glue Catalog table)\n",
    "\n",
    "- You can run crawler to create the table\n",
    "- You can also create the external table on Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      CREATE EXTERNAL TABLE IF NOT EXISTS `sampledb`.`complex_json_structure` (\n",
      "      `BARCODE` string, \n",
      "      `INDEX` int,\n",
      "      `DATE` string,\n",
      "      `S.TIME` string,\n",
      "      `E.TIME` string,\n",
      "      `JOB` string,\n",
      "      `RESULT` string,\n",
      "      `USER` string,\n",
      "      `LOTINFO` string,\n",
      "      `MACHINE` string,\n",
      "      `SIDE` string,\n",
      "      `aggrate_component_info` struct<ComponentID:string,Volume_percentage_:float,Height_um_:float,Area_percentage_:float,OffsetX_percentage_:float,OffsetY_percentage_:float,Volume_um3_:int,Area_um2_:int,Result:string,PinNumber:string,PadVerification:string,Shape:string,Library_Name:string,Vol_Min_percentage_:int,Vol_Max_percentage_:int,Height_Low_um_:int,Height_High_um_:int,Area_Min_percentage_:int,Area_Max_percentage_:int,OffsetX_Error_mm_:float,OffsetY_Error_mm_:float,Unnamed_21:string>\n",
      "      )\n",
      "    ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\n",
      "    WITH SERDEPROPERTIES (\n",
      "      'ignore.malformed.json'='true',\n",
      "      'paths'='BARCODE,INDEX,DATE,S.TIME,E.TIME,JOB,RESULT,USER,LOTINFO,MACHINE,SIDE,aggrate_component_info'\n",
      "    )\n",
      "    STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' \n",
      "    OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
      "    LOCATION 's3://ray-glue-streaming/catalog_test/complex_json/'\n",
      "    \n",
      "Athena query 566ff54a-e030-4fac-baf8-4a574b171a3d result: SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "json_file_path = 's3://{}/{}'.format(s3_bucket, s3_json_prefix)\n",
    "\n",
    "query = r'''\n",
    "      CREATE EXTERNAL TABLE IF NOT EXISTS `sampledb`.`complex_json_structure` (\n",
    "      `BARCODE` string, \n",
    "      `INDEX` int,\n",
    "      `DATE` string,\n",
    "      `S.TIME` string,\n",
    "      `E.TIME` string,\n",
    "      `JOB` string,\n",
    "      `RESULT` string,\n",
    "      `USER` string,\n",
    "      `LOTINFO` string,\n",
    "      `MACHINE` string,\n",
    "      `SIDE` string,\n",
    "      `aggrate_component_info` struct<ComponentID:string,Volume_percentage_:float,Height_um_:float,Area_percentage_:float,OffsetX_percentage_:float,OffsetY_percentage_:float,Volume_um3_:int,Area_um2_:int,Result:string,PinNumber:string,PadVerification:string,Shape:string,Library_Name:string,Vol_Min_percentage_:int,Vol_Max_percentage_:int,Height_Low_um_:int,Height_High_um_:int,Area_Min_percentage_:int,Area_Max_percentage_:int,OffsetX_Error_mm_:float,OffsetY_Error_mm_:float,Unnamed_21:string>\n",
    "      )\n",
    "    ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\n",
    "    WITH SERDEPROPERTIES (\n",
    "      'ignore.malformed.json'='true',\n",
    "      'paths'='BARCODE,INDEX,DATE,S.TIME,E.TIME,JOB,RESULT,USER,LOTINFO,MACHINE,SIDE,aggrate_component_info'\n",
    "    )\n",
    "    STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' \n",
    "    OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
    "    LOCATION '{}'\n",
    "    '''\n",
    "query = query.format(json_file_path)\n",
    "print(query)\n",
    "    \n",
    "query_exec_id = wr.athena.start_query_execution(sql=query, database=\"sampledb\")\n",
    "wr.athena.wait_query(query_execution_id=query_exec_id)\n",
    "res = wr.athena.get_query_execution(query_execution_id=query_exec_id)\n",
    "print(\"Athena query {} result: {}\".format(query_exec_id, res[\"Status\"][\"State\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Athena Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    barcode  index        date    s.time    e.time                 job result  \\\n",
      "0  MN635582  24566  11/10/2020  11:54:09  11:54:15  A5E41637164-04-BOT   FAIL   \n",
      "1  MN635582  24566  11/10/2020  11:54:09  11:54:15  A5E41637164-04-BOT   FAIL   \n",
      "2  MN635582  24566  11/10/2020  11:54:09  11:54:15  A5E41637164-04-BOT   FAIL   \n",
      "3  MN635582  24566  11/10/2020  11:54:09  11:54:15  A5E41637164-04-BOT   FAIL   \n",
      "4  MN635582  24566  11/10/2020  11:54:09  11:54:15  A5E41637164-04-BOT   FAIL   \n",
      "\n",
      "  user   lotinfo machine  side  \\\n",
      "0   SV  KOHYOUNG    <NA>  <NA>   \n",
      "1   SV  KOHYOUNG    <NA>  <NA>   \n",
      "2   SV  KOHYOUNG    <NA>  <NA>   \n",
      "3   SV  KOHYOUNG    <NA>  <NA>   \n",
      "4   SV  KOHYOUNG    <NA>  <NA>   \n",
      "\n",
      "                              aggrate_component_info  \n",
      "0  {'componentid': '1:', 'volume_percentage_': 10...  \n",
      "1  {'componentid': '1:', 'volume_percentage_': 10...  \n",
      "2  {'componentid': '1:R615_04', 'volume_percentag...  \n",
      "3  {'componentid': '1:R615_04', 'volume_percentag...  \n",
      "4  {'componentid': '1:X161_04', 'volume_percentag...  \n",
      "1047.8994140625\n",
      "    barcode  index        date    s.time    e.time                 job result  \\\n",
      "0  MN634850  24857  11/10/2020  16:43:32  16:43:40  A5E41637164-04-TOP   FAIL   \n",
      "1  MN634850  24857  11/10/2020  16:43:32  16:43:40  A5E41637164-04-TOP   FAIL   \n",
      "2  MN634850  24857  11/10/2020  16:43:32  16:43:40  A5E41637164-04-TOP   FAIL   \n",
      "3  MN634850  24857  11/10/2020  16:43:32  16:43:40  A5E41637164-04-TOP   FAIL   \n",
      "4  MN634850  24857  11/10/2020  16:43:32  16:43:40  A5E41637164-04-TOP   FAIL   \n",
      "\n",
      "  user   lotinfo machine  side  \\\n",
      "0   SV  KOHYOUNG    <NA>  <NA>   \n",
      "1   SV  KOHYOUNG    <NA>  <NA>   \n",
      "2   SV  KOHYOUNG    <NA>  <NA>   \n",
      "3   SV  KOHYOUNG    <NA>  <NA>   \n",
      "4   SV  KOHYOUNG    <NA>  <NA>   \n",
      "\n",
      "                              aggrate_component_info  \n",
      "0  {'componentid': '1:', 'volume_percentage_': 12...  \n",
      "1  {'componentid': '1:', 'volume_percentage_': 10...  \n",
      "2  {'componentid': '1:C600_04', 'volume_percentag...  \n",
      "3  {'componentid': '1:', 'volume_percentage_': 10...  \n",
      "4  {'componentid': '1:', 'volume_percentage_': 98...  \n",
      "3464.009765625\n",
      "    barcode  index        date    s.time    e.time                 job result  \\\n",
      "0  MN635582  24566  11/10/2020  11:54:09  11:54:15  A5E41637164-04-BOT   FAIL   \n",
      "1  MN635582  24566  11/10/2020  11:54:09  11:54:15  A5E41637164-04-BOT   FAIL   \n",
      "\n",
      "  user   lotinfo machine  side  \\\n",
      "0   SV  KOHYOUNG    <NA>  <NA>   \n",
      "1   SV  KOHYOUNG    <NA>  <NA>   \n",
      "\n",
      "                              aggrate_component_info  \n",
      "0  {'componentid': '1:C82_01', 'volume_percentage...  \n",
      "1  {'componentid': '1:C82_01', 'volume_percentage...  \n",
      "4956.1591796875\n"
     ]
    }
   ],
   "source": [
    "# Retrieving the data from Amazon Athena\n",
    "query = r'''SELECT * FROM \"sampledb\".\"complex_json_structure\" limit 10;\n",
    "    '''\n",
    "df = wr.athena.read_sql_query(sql=query, database=\"sampledb\")\n",
    "print(df.head())\n",
    "scanned_bytes = df.query_metadata[\"Statistics\"][\"DataScannedInBytes\"]\n",
    "print(scanned_bytes/1024)\n",
    "\n",
    "query = r'''\n",
    "SELECT * FROM \"sampledb\".\"complex_json_structure\" as t \n",
    "where t.barcode='MN634850' and t.index=24857 and t.JOB='A5E41637164-04-TOP' limit 10;\n",
    "'''\n",
    "df = wr.athena.read_sql_query(sql=query, database=\"sampledb\")\n",
    "print(df.head())\n",
    "# Check how many date athen scanned to get the result\n",
    "scanned_bytes = df.query_metadata[\"Statistics\"][\"DataScannedInBytes\"]\n",
    "print(scanned_bytes/1024)\n",
    "\n",
    "query = r'''\n",
    "SELECT * FROM \"sampledb\".\"complex_json_structure\" as t \n",
    "where t.barcode='MN635582' and t.index=24566 and t.JOB='A5E41637164-04-BOT' and t.aggrate_component_info.ComponentID='1:C82_01' \n",
    "limit 10;\n",
    "'''\n",
    "df = wr.athena.read_sql_query(sql=query, database=\"sampledb\")\n",
    "print(df.head())\n",
    "# Check how many date athen scanned to get the result\n",
    "scanned_bytes = df.query_metadata[\"Statistics\"][\"DataScannedInBytes\"]\n",
    "print(scanned_bytes/1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
