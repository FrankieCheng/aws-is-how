{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6b5b9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting awswrangler\n",
      "  Downloading awswrangler-2.14.0-py3-none-any.whl (226 kB)\n",
      "     |████████████████████████████████| 226 kB 644 kB/s            \n",
      "\u001b[?25hCollecting numpy<1.19.0,>=1.18.0\n",
      "  Downloading numpy-1.18.5-cp36-cp36m-manylinux1_x86_64.whl (20.1 MB)\n",
      "     |████████████████████████████████| 20.1 MB 4.9 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: boto3<2.0.0,>=1.20.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awswrangler) (1.21.42)\n",
      "Collecting progressbar2<4.0.0,>=3.53.3\n",
      "  Downloading progressbar2-3.55.0-py2.py3-none-any.whl (26 kB)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\",)': /simple/requests-aws4auth/\u001b[0m\n",
      "Collecting requests-aws4auth<2.0.0,>=1.1.1\n",
      "  Downloading requests_aws4auth-1.1.2-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.23.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awswrangler) (1.24.42)\n",
      "Collecting jsonpath-ng<2.0.0,>=1.5.3\n",
      "  Downloading jsonpath_ng-1.5.3-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: openpyxl<3.1.0,>=3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awswrangler) (3.0.6)\n",
      "Collecting pg8000<1.23.0,>=1.16.0\n",
      "  Downloading pg8000-1.22.1-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: pandas<1.2.0,>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awswrangler) (1.1.5)\n",
      "Requirement already satisfied: xlrd<3.0.0,>=2.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awswrangler) (2.0.1)\n",
      "Requirement already satisfied: xlwt<2.0.0,>=1.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awswrangler) (1.3.0)\n",
      "Collecting pymysql<1.1.0,>=0.9.0\n",
      "  Downloading PyMySQL-1.0.2-py3-none-any.whl (43 kB)\n",
      "     |████████████████████████████████| 43 kB 331 kB/s            \n",
      "\u001b[?25hCollecting redshift-connector<2.1.0,>=2.0.889\n",
      "  Downloading redshift_connector-2.0.908-py3-none-any.whl (112 kB)\n",
      "     |████████████████████████████████| 112 kB 825 kB/s            \n",
      "\u001b[?25hCollecting opensearch-py<2.0.0,>=1.0.0\n",
      "  Downloading opensearch_py-1.1.0-py2.py3-none-any.whl (207 kB)\n",
      "     |████████████████████████████████| 207 kB 660 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: pyarrow<6.1.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awswrangler) (6.0.1)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3<2.0.0,>=1.20.17->awswrangler) (0.5.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3<2.0.0,>=1.20.17->awswrangler) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<2.0.0,>=1.23.17->awswrangler) (1.26.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<2.0.0,>=1.23.17->awswrangler) (2.8.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from jsonpath-ng<2.0.0,>=1.5.3->awswrangler) (1.15.0)\n",
      "Requirement already satisfied: ply in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from jsonpath-ng<2.0.0,>=1.5.3->awswrangler) (3.11)\n",
      "Requirement already satisfied: decorator in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from jsonpath-ng<2.0.0,>=1.5.3->awswrangler) (4.4.2)\n",
      "Requirement already satisfied: jdcal in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from openpyxl<3.1.0,>=3.0.0->awswrangler) (1.4.1)\n",
      "Requirement already satisfied: et-xmlfile in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from openpyxl<3.1.0,>=3.0.0->awswrangler) (1.0.1)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from opensearch-py<2.0.0,>=1.0.0->awswrangler) (2021.5.30)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas<1.2.0,>=1.1.0->awswrangler) (2021.1)\n",
      "Collecting scramp>=1.4.1\n",
      "  Downloading scramp-1.4.1-py3-none-any.whl (8.5 kB)\n",
      "Collecting python-utils>=2.3.0\n",
      "  Downloading python_utils-3.3.3-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: requests<2.28.1,>=2.23.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from redshift-connector<2.1.0,>=2.0.889->awswrangler) (2.26.0)\n",
      "Collecting lxml>=4.6.5\n",
      "  Downloading lxml-4.9.1-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
      "     |████████████████████████████████| 6.4 MB 414 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from redshift-connector<2.1.0,>=2.0.889->awswrangler) (21.3)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from redshift-connector<2.1.0,>=2.0.889->awswrangler) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from beautifulsoup4<5.0.0,>=4.7.0->redshift-connector<2.1.0,>=2.0.889->awswrangler) (2.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<2.28.1,>=2.23.0->redshift-connector<2.1.0,>=2.0.889->awswrangler) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<2.28.1,>=2.23.0->redshift-connector<2.1.0,>=2.0.889->awswrangler) (2.0.9)\n",
      "Requirement already satisfied: asn1crypto>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scramp>=1.4.1->pg8000<1.23.0,>=1.16.0->awswrangler) (1.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging->redshift-connector<2.1.0,>=2.0.889->awswrangler) (2.4.7)\n",
      "Installing collected packages: scramp, python-utils, numpy, lxml, requests-aws4auth, redshift-connector, pymysql, progressbar2, pg8000, opensearch-py, jsonpath-ng, awswrangler\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Attempting uninstall: lxml\n",
      "    Found existing installation: lxml 4.6.4\n",
      "    Uninstalling lxml-4.6.4:\n",
      "      Successfully uninstalled lxml-4.6.4\n",
      "Successfully installed awswrangler-2.14.0 jsonpath-ng-1.5.3 lxml-4.9.1 numpy-1.18.5 opensearch-py-1.1.0 pg8000-1.22.1 progressbar2-3.55.0 pymysql-1.0.2 python-utils-3.3.3 redshift-connector-2.0.908 requests-aws4auth-1.1.2 scramp-1.4.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f439da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import boto3\n",
    "import pickle\n",
    "import io\n",
    "from io import StringIO\n",
    "import awswrangler as wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "736f94d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need the AWSGlueServiceNotebookRole, AthenaAccess, S3Access policy add to your role\n",
    "df_r = wr.athena.read_sql_query(\"select * from reseller\", database=\"implementationdb\")\n",
    "df =  wr.athena.read_sql_query(\"select * from billing\",database=\"implementationdb\")\n",
    "bucket = 'ray-glue-streaming'\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61615bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe (467729, 3)\n",
      "dataframer (12358, 3)\n"
     ]
    }
   ],
   "source": [
    "print('dataframe',df.shape)\n",
    "print('dataframer',df_r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afd7144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---FUNCTIONS-------------------------------\n",
    "\n",
    "def write_dataframe_to_csv_on_s3(dataframe, bucket, filename):\n",
    "    \"\"\" Write a dataframe to a CSV on S3 \"\"\"\n",
    "    # Create buffer\n",
    "    csv_buffer = StringIO()\n",
    "    # Write dataframe to buffer\n",
    "    dataframe.to_csv(csv_buffer, sep=\",\", header=None,index=None)\n",
    "    # Create S3 object\n",
    "    s3_resource = boto3.resource(\"s3\")\n",
    "    # Write buffer to S3 object\n",
    "    s3_resource.Object(bucket, filename).put(Body=csv_buffer.getvalue())\n",
    "    print(\"Writing {} records to {}\".format(len(dataframe), filename))\n",
    "\n",
    "#--------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a349f7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Filter the last 4 months of data\n",
    "max_date = df['date'].max()\n",
    "min_date = max_date - pd.to_timedelta(120, unit='d')\n",
    "\n",
    "df = df[df['date'] > min_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc3e34b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---FUNCTIONS-------------------------------\n",
    "def completeItem(dfItem):\n",
    "    min_date = dfItem['date'].min()\n",
    "    max_date = dfItem['date'].max()\n",
    "    if min_date == max_date:\n",
    "        #only one data point\n",
    "        return\n",
    "    r = pd.date_range(start=min_date, end=max_date)\n",
    "    dfItemNew = dfItem.set_index('date').reindex(r).rename_axis('date').reset_index()\n",
    "\n",
    "    dfItemNew['mean-last-30'] = dfItemNew['bill'].rolling(30,min_periods=1).mean().reset_index()['bill']\n",
    "    dfItemNew['mean-last-7'] = dfItemNew['bill'].rolling(7,min_periods=1).mean().reset_index()['bill']\n",
    "    dfItemNew['std-last-30'] = dfItemNew['bill'].rolling(30,min_periods=1).std().reset_index()['bill']\n",
    "    dfItemNew['bill'] = dfItemNew['bill'].fillna(0)\n",
    "    dfItemNew['id_reseller'] = dfItem['id_reseller'].max()\n",
    "    dfItemNew['std-last-30'].fillna(method='ffill',inplace=True)\n",
    "    dfItemNew['mean-last-7'].fillna(method='ffill',inplace=True)\n",
    "    dfItemNew['std-last-30'].fillna(method='ffill',inplace=True)\n",
    "    resp = []\n",
    "    counter = 0\n",
    "    for index,row in dfItemNew.iterrows():\n",
    "        resp.append(counter)\n",
    "        if row['bill'] == 0:\n",
    "            counter += 1\n",
    "        else:\n",
    "            counter = 0\n",
    "    dfItemNew['days_without_purchase'] = pd.Series(resp)\n",
    "    return dfItemNew\n",
    "#--------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "badae995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 200 resellers\n",
      "processed 400 resellers\n",
      "processed 600 resellers\n",
      "processed 800 resellers\n",
      "processed 1000 resellers\n",
      "processed 1200 resellers\n",
      "processed 1400 resellers\n",
      "processed 1600 resellers\n",
      "processed 1800 resellers\n",
      "processed 2000 resellers\n",
      "processed 2200 resellers\n",
      "processed 2400 resellers\n",
      "processed 2600 resellers\n",
      "processed 2800 resellers\n",
      "processed 3000 resellers\n",
      "processed 3200 resellers\n",
      "processed 3400 resellers\n",
      "processed 3600 resellers\n",
      "processed 3800 resellers\n",
      "processed 4000 resellers\n",
      "processed 4200 resellers\n",
      "processed 4400 resellers\n",
      "processed 4600 resellers\n",
      "processed 4800 resellers\n",
      "processed 5000 resellers\n",
      "processed 5200 resellers\n",
      "processed 5400 resellers\n",
      "processed 5600 resellers\n",
      "processed 5800 resellers\n",
      "processed 6000 resellers\n",
      "processed 6200 resellers\n",
      "processed 6400 resellers\n",
      "processed 6600 resellers\n",
      "processed 6800 resellers\n",
      "processed 7000 resellers\n",
      "processed 7200 resellers\n",
      "processed 7400 resellers\n",
      "processed 7600 resellers\n",
      "processed 7800 resellers\n",
      "processed 8000 resellers\n",
      "processed 8200 resellers\n",
      "processed 8400 resellers\n",
      "processed 8600 resellers\n",
      "processed 8800 resellers\n",
      "processed 9000 resellers\n",
      "processed 9200 resellers\n",
      "processed 9400 resellers\n",
      "processed 9600 resellers\n",
      "processed 9800 resellers\n",
      "processed 10000 resellers\n",
      "processed 10200 resellers\n",
      "processed 10400 resellers\n",
      "processed 10600 resellers\n",
      "processed 10800 resellers\n",
      "processed 11000 resellers\n",
      "processed 11200 resellers\n",
      "processed 11400 resellers\n",
      "processed 11600 resellers\n",
      "processed 11800 resellers\n",
      "processed 12000 resellers\n",
      "processed 12200 resellers\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "dfCompletedList = []\n",
    "for nid,item in df.groupby('id_reseller'):\n",
    "    i = i+1\n",
    "    if i%200 == 0:\n",
    "        print ('processed {} resellers'.format(str(i)))\n",
    "    dfCompletedList.append(completeItem(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1491ea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfCompletedList).copy()\n",
    "df['weekday']  = df['date'].dt.day_name()\n",
    "del dfCompletedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce1d9b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Compute next bill\n",
    "df['next_bill'] = df.replace(0,np.nan).groupby('id_reseller')['bill'].fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cd10f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Compute last bill\n",
    "df['last_bill'] = df.replace(0,np.nan).groupby('id_reseller')['bill'].fillna(method='ffill').copy()\n",
    "different_zero = df['last_bill'].shift(1)\n",
    "df.loc[df['bill'] != 0,'last_bill'] = np.nan\n",
    "df['last_bill'] = df['last_bill'].fillna(different_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58cc9833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Merge Bill and Reseller bill\n",
    "df = df.merge(df_r,how='inner',on='id_reseller')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3d1b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Drop the na value\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "314db822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Deal with categorical variables\n",
    "#\n",
    "# To deal with categorical variables (reseller's cluster and reseller's zone), \n",
    "# we will use a combination of sklearn's Label Encoder, a preprocessing module that transforms strings in numeric lables, \n",
    "# and One Hot Encoder, that takes this numerical variables and creates dummy (0/1 state) variables.\n",
    "#\n",
    "# This modules are python objects that keep in their internal variables the information necessary to transform new data.  \n",
    "# So, in the Glue ETL we are going to store this objects in pkl format\n",
    "#\n",
    "le_cluster = LabelEncoder()\n",
    "ohe_cluster = OneHotEncoder(handle_unknown='ignore')\n",
    "df_cluster = pd.DataFrame(ohe_cluster.fit_transform(le_cluster.fit_transform(df['cluster'].fillna('')).reshape(-1, 1)).todense())\n",
    "df_cluster = df_cluster.add_prefix('cluster_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2896ef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "le_zone = LabelEncoder()\n",
    "ohe_zone = OneHotEncoder(handle_unknown='ignore')\n",
    "df_zone = pd.DataFrame(ohe_zone.fit_transform(le_zone.fit_transform(df['zone'].fillna('')).reshape(-1, 1)).todense())\n",
    "df_zone = df_zone.add_prefix('zone_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25e9a00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "le_weekday = LabelEncoder()\n",
    "ohe_weekday = OneHotEncoder(handle_unknown='ignore')\n",
    "df_weekday = pd.DataFrame(ohe_weekday.fit_transform(le_weekday.fit_transform(df['weekday']).reshape(-1, 1)).todense())\n",
    "df_weekday = df_weekday.add_prefix('weekday_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c22dcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Upload to S3\n",
    "client = boto3.client('s3')\n",
    "client.put_object(Body=pickle.dumps(le_cluster), Bucket=bucket, Key='glue_python_module/preprocessing/le_cluster.pkl');\n",
    "client.put_object(Body=pickle.dumps(ohe_cluster), Bucket=bucket, Key='glue_python_module/preprocessing/ohe_cluster.pkl')\n",
    "client.put_object(Body=pickle.dumps(le_zone), Bucket=bucket, Key='glue_python_module/preprocessing/le_zone.pkl')\n",
    "client.put_object(Body=pickle.dumps(ohe_zone), Bucket=bucket, Key='glue_python_module/preprocessing/ohe_zone.pkl')\n",
    "client.put_object(Body=pickle.dumps(le_weekday), Bucket=bucket, Key='glue_python_module/preprocessing/le_weekday.pkl')\n",
    "client.put_object(Body=pickle.dumps(ohe_weekday), Bucket=bucket, Key='glue_python_module/preprocessing/ohe_weekday.pkl');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4de31ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Write to S3 resulting ETL\n",
    "#\n",
    "# Now we have to write to S3 all the relevant columns. \n",
    "# We will perform a train/validation split of the customers so we can train on a group and get relevant metrics on the other.\n",
    "df = df[['next_bill', 'bill', 'date', 'id_reseller', 'mean-last-30', 'mean-last-7',\n",
    "       'std-last-30', 'days_without_purchase', 'weekday',\n",
    "       'last_bill', 'zone', 'cluster']]\n",
    "\n",
    "df = pd.concat([df,df_cluster,df_zone,df_weekday],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf6d4c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 139883 records to glue_python_module/validation/validation.csv\n",
      "Writing 1301725 records to glue_python_module/train/train.csv\n"
     ]
    }
   ],
   "source": [
    "#Take a random 10% sample of the resellers\n",
    "val_resellers = list(pd.Series(df['id_reseller'].unique()).sample(frac=0.1))\n",
    "\n",
    "df_train = df[~df['id_reseller'].isin(val_resellers)].sample(frac=1)\n",
    "\n",
    "df_validation = df[df['id_reseller'].isin(val_resellers)].sample(frac=1)\n",
    "\n",
    "df_train.drop(['date','id_reseller','bill','zone','cluster','weekday'],axis=1,inplace=True)\n",
    "df_validation.drop(['date','id_reseller','bill','zone','cluster','weekday'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "write_dataframe_to_csv_on_s3(df_validation, bucket, 'glue_python_module/validation/validation.csv')\n",
    "write_dataframe_to_csv_on_s3(df_train, bucket, 'glue_python_module/train/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a4d477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# Preprocessing Pipeline\n",
    "#####\n",
    "df_r = wr.athena.read_sql_query(\"select * from reseller\", database=\"implementationdb\")\n",
    "df = wr.athena.read_sql_query(\"select * from billing\", database=\"implementationdb\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "max_date = df['date'].max()\n",
    "min_date = max_date - pd.Timedelta(days=30)\n",
    "df = df[(df['date'] > min_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "043ae1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeItem(dfItem,max_date,min_date):\n",
    "    r = pd.date_range(start=min_date, end=max_date)\n",
    "    dfItemNew = dfItem.set_index('date').reindex(r).fillna(0.0).rename_axis('date').reset_index()\n",
    "    dfItemNew['id_reseller'] = dfItem['id_reseller'].max()\n",
    "    return dfItemNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "363609e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCompletedList = []\n",
    "for nid,item in df.groupby('id_reseller'):\n",
    "    dfCompletedList.append(completeItem(item,max_date,min_date))\n",
    "dfCompleted = pd.concat(dfCompletedList).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6173be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfCompleted\n",
    "del dfCompleted\n",
    "del dfCompletedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc42f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_info(group):\n",
    "    weekday = (max_date + pd.Timedelta(days=1)).day_name()\n",
    "    mean_last_30 = group['bill'].replace(0,np.nan).mean()\n",
    "    std_last_30 = group['bill'].replace(0,np.nan).std()\n",
    "    date_last_bill = group[group['bill'] != 0]['date'].max()\n",
    "    days_without_purchase = (max_date + pd.Timedelta(days=1) - date_last_bill).days\n",
    "\n",
    "    mean_last_7 = group[(group['date'] >= max_date - pd.Timedelta(days=6))]['bill'].replace(0,np.nan).mean()\n",
    "    last_bill = group[group['bill'] > 0].sort_values('date',ascending=False).head(1)['bill'].values[0]\n",
    "    return {'weekday':weekday,'mean-last-30':mean_last_30,\n",
    "           'std-last-30':std_last_30,'mean-last-7':mean_last_7,'last_bill':last_bill,\n",
    "           'id_reseller':group['id_reseller'].max(), 'days_without_purchase':days_without_purchase}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd766bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Prepare feature\n",
    "features = []\n",
    "for index,group in df.groupby('id_reseller'):\n",
    "    features.append(complete_info(group))\n",
    "\n",
    "df_features = pd.DataFrame(features)\n",
    "\n",
    "df_features = df_features.merge(df_r,how='inner',on='id_reseller')\n",
    "\n",
    "pipe_list = [le_cluster,ohe_cluster,le_zone,ohe_zone,le_weekday,ohe_weekday]\n",
    "\n",
    "df_cluster = pd.DataFrame(\n",
    "    pipe_list[1].transform(pipe_list[0].transform(df_features['cluster']).reshape(-1, 1)).todense()\n",
    ")\n",
    "df_cluster = df_cluster.add_prefix('cluster_')\n",
    "df_zone = pd.DataFrame(\n",
    "    pipe_list[3].transform(pipe_list[2].transform(df_features['zone']).reshape(-1, 1)).todense()\n",
    ")\n",
    "df_zone = df_zone.add_prefix('zone_')\n",
    "df_weekday = pd.DataFrame(\n",
    "    pipe_list[5].transform(pipe_list[4].transform(df_features['weekday']).reshape(-1, 1)).todense()\n",
    ")\n",
    "df_weekday = df_weekday.add_prefix('weekday_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "007ed0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Predict\n",
    "df_to_predict = pd.concat([df_features,df_cluster,df_zone,df_weekday],axis=1)\n",
    "df_to_predict_feats = df_to_predict[['mean-last-30', 'mean-last-7', 'std-last-30',\n",
    "       'days_without_purchase', 'last_bill', 'cluster_0', 'cluster_1',\n",
    "       'cluster_2', 'cluster_3', 'cluster_4', 'zone_0', 'zone_1', 'zone_2',\n",
    "       'zone_3', 'zone_4', 'zone_5', 'zone_6', 'zone_7', 'zone_8', 'zone_9',\n",
    "       'zone_10', 'zone_11', 'zone_12', 'zone_13', 'zone_14', 'zone_15',\n",
    "       'zone_16', 'zone_17', 'zone_18', 'zone_19', 'zone_20', 'zone_21',\n",
    "       'zone_22', 'zone_23', 'zone_24', 'zone_25', 'zone_26', 'zone_27',\n",
    "       'zone_28', 'zone_29', 'zone_30', 'zone_31', 'zone_32', 'zone_33',\n",
    "       'zone_34', 'zone_35', 'zone_36', 'zone_37', 'zone_38', 'zone_39',\n",
    "       'zone_40', 'zone_41', 'zone_42', 'zone_43', 'zone_44', 'zone_45',\n",
    "       'zone_46', 'zone_47', 'zone_48', 'zone_49', 'zone_50', 'zone_51',\n",
    "       'weekday_0', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4',\n",
    "       'weekday_5', 'weekday_6']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "535ee5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 12347 records to glue_python_module/predict/to_predict.csv\n",
      "Writing 12347 records to glue_python_module/predict/id_reseller_to_predict.csv\n"
     ]
    }
   ],
   "source": [
    "write_dataframe_to_csv_on_s3(df_to_predict_feats,bucket,'glue_python_module/predict/to_predict.csv')\n",
    "write_dataframe_to_csv_on_s3(df_to_predict[['id_reseller']],bucket,'glue_python_module/predict/id_reseller_to_predict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdbad21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
