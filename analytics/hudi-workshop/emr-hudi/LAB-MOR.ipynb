{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab : Batch Updates to a S3 Datalake using Apache Hudi\n",
    "\n",
    "# Merge On Read\n",
    "\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "2. [Merge on Read](#Merge-On-Read) \n",
    "   2.1 [Bulk Insert the Initial Dataset](#Bulk-Insert-the-Initial-Dataset)\n",
    "   2.2 [Batch Upsert some records](#Batch-Upsert-some-records)\n",
    "   2.3 [Deleting Records](#Deleting-Records.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates using PySpark on [Apache Hudi](https://aws.amazon.com/emr/features/hudi/) on Amazon EMR to insert/upsert/delete records to an S3 data lake.\n",
    "\n",
    "Here is a good reference link to read later:\n",
    "\n",
    "* [How Hudi Works](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi-how-it-works.html)\n",
    "\n",
    "This notebook covers the following concepts when writing  Merge-On-Read tables to an S3 Datalake:\n",
    "\n",
    "- Write Hudi Spark jobs in PySpark.\n",
    "- Bulk Insert the Initial Dataset.\n",
    "- Sync the Hudi tables to the Hive/Glue Catalog.\n",
    "- Upsert some records to a Hudi MOR table.\n",
    "- Delete records from a Hudi MOR table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-requisites\n",
    "\n",
    "### This demo is based on Hudi version 0.5.0-incubating and runs fine on Jupyter Notebooks connected to a 1 node (r5.4xlarge) EMR cluster with configuration listed below \n",
    "\n",
    " - EMR versions 5.29.0 or 6.0.0 \n",
    " \n",
    " - Software configuration\n",
    "\n",
    "       - Hadoop 2.8.5\n",
    "       - Hive 2.3.6\n",
    "       - Livy 0.6.0\n",
    "       - JupyterHub 1.0.0\n",
    "       - Spark 2.4.4\n",
    "       \n",
    "       \n",
    " - AWS Glue Data Catalog settings - Select the below listed check boxes\n",
    "       - Use for Hive table metadata  \n",
    "       - Use for Spark table metadata\n",
    "\n",
    "\n",
    "\n",
    "### Connect to the Master Node of EMR cluster Using SSH :\n",
    "    - ssh -i ~/xxxx.pem hadoop@<ec2-xx-xxx-xx-xx.us-west-2.compute.amazonaws.com>\n",
    "\n",
    "    - Ensure  the below listed files are copied into HDFS.\n",
    "\n",
    "    - hadoop fs -copyFromLocal /usr/lib/hudi/hudi-spark-bundle.jar hdfs:///user/hadoop/\n",
    "\n",
    "    - hadoop fs -copyFromLocal /usr/lib/spark/external/lib/spark-avro.jar hdfs:///user/hadoop/\n",
    "\n",
    "    - hadoop fs -copyFromLocal /usr/lib/spark/jars/httpclient-4.5.9.jar hdfs:///user/hadoop/\n",
    "\n",
    "Let's start by initializing the Spark Session to connect this notebook to our Spark EMR cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///user/hadoop/httpclient-4.5.9.jar, hdfs:///user/hadoop/hudi-spark-bundle.jar,hdfs:///user/hadoop/spark-avro.jar', 'spark.sql.hive.convertMetastoreParquet': 'false', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.dynamicAllocation.executorIdleTimeout': 3600, 'spark.executor.memory': '7G', 'spark.executor.cores': 1, 'spark.dynamicAllocation.initialExecutors': 16}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":  { \n",
    "             \"spark.jars\":\"hdfs:///user/hadoop/httpclient-4.5.9.jar, hdfs:///user/hadoop/hudi-spark-bundle.jar,hdfs:///user/hadoop/spark-avro.jar\",\n",
    "             \"spark.sql.hive.convertMetastoreParquet\":\"false\",     \n",
    "             \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "             \"spark.dynamicAllocation.executorIdleTimeout\": 3600,\n",
    "             \"spark.executor.memory\": \"7G\",\n",
    "             \"spark.executor.cores\": 1,\n",
    "             \"spark.dynamicAllocation.initialExecutors\":16\n",
    "           } \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constants for Python to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54fa2cafdee94f66878779fc5a7fee97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>47</td><td>application_1593789203399_0081</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-23-24.us-west-2.compute.internal:20888/proxy/application_1593789203399_0081/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-17-12.us-west-2.compute.internal:8042/node/containerlogs/container_1593789203399_0081_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# General Constants\n",
    "HUDI_FORMAT = \"org.apache.hudi\"\n",
    "TABLE_NAME = \"hoodie.table.name\"\n",
    "RECORDKEY_FIELD_OPT_KEY = \"hoodie.datasource.write.recordkey.field\"\n",
    "PRECOMBINE_FIELD_OPT_KEY = \"hoodie.datasource.write.precombine.field\"\n",
    "OPERATION_OPT_KEY = \"hoodie.datasource.write.operation\"\n",
    "BULK_INSERT_OPERATION_OPT_VAL = \"bulk_insert\"\n",
    "UPSERT_OPERATION_OPT_VAL = \"upsert\"\n",
    "BULK_INSERT_PARALLELISM = \"hoodie.bulkinsert.shuffle.parallelism\"\n",
    "UPSERT_PARALLELISM = \"hoodie.upsert.shuffle.parallelism\"\n",
    "S3_CONSISTENCY_CHECK = \"hoodie.consistency.check.enabled\"\n",
    "HUDI_CLEANER_POLICY = \"hoodie.cleaner.policy\"\n",
    "KEEP_LATEST_COMMITS = \"KEEP_LATEST_COMMITS\"\n",
    "HUDI_COMMITS_RETAINED = \"hoodie.cleaner.commits.retained\"\n",
    "PAYLOAD_CLASS_OPT_KEY = \"hoodie.datasource.write.payload.class\"\n",
    "EMPTY_PAYLOAD_CLASS_OPT_VAL = \"org.apache.hudi.EmptyHoodieRecordPayload\"\n",
    "\n",
    "# Hive Constants\n",
    "HIVE_SYNC_ENABLED_OPT_KEY=\"hoodie.datasource.hive_sync.enable\"\n",
    "HIVE_PARTITION_FIELDS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_fields\"\n",
    "HIVE_ASSUME_DATE_PARTITION_OPT_KEY=\"hoodie.datasource.hive_sync.assume_date_partitioning\"\n",
    "HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_extractor_class\"\n",
    "HIVE_TABLE_OPT_KEY=\"hoodie.datasource.hive_sync.table\"\n",
    "\n",
    "# Partition Constants\n",
    "NONPARTITION_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.NonPartitionedExtractor\"\n",
    "MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.MultiPartKeysValueExtractor\"\n",
    "KEYGENERATOR_CLASS_OPT_KEY=\"hoodie.datasource.write.keygenerator.class\"\n",
    "NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.NonpartitionedKeyGenerator\"\n",
    "COMPLEX_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.ComplexKeyGenerator\"\n",
    "PARTITIONPATH_FIELD_OPT_KEY=\"hoodie.datasource.write.partitionpath.field\"\n",
    "\n",
    "#Incremental Constants\n",
    "VIEW_TYPE_OPT_KEY=\"hoodie.datasource.view.type\"\n",
    "BEGIN_INSTANTTIME_OPT_KEY=\"hoodie.datasource.read.begin.instanttime\"\n",
    "VIEW_TYPE_INCREMENTAL_OPT_VAL=\"incremental\"\n",
    "END_INSTANTTIME_OPT_KEY=\"hoodie.datasource.read.end.instanttime\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to create JSON data and Spark dataframe from this data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge On Read \n",
    "\n",
    "For near real-time applications that mandate quick upserts, MERGE_ON_READ table type would be better suited. MOR table stores incoming upserts for each file group, onto a row based delta log (In Avro file format). This log is then merged with the existing Parquet file using a compactor during reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea25d03ef1d402a8405600829c6afd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## CHANGE ME ##\n",
    "config = {\n",
    "    \"table_name\": \"hudi_mor_trips_table\",\n",
    "    \"target\": \"s3://<Your S3 Bucket Here>/tmp/hudi/hudi_mor_trips_table\",\n",
    "    \"primary_key\": \"trip_id\",\n",
    "    \"sort_key\": \"tstamp\",\n",
    "    \"commits_to_retain\": \"2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69351e1ab90e43c7ab584bc47f4bb777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "STORAGE_TYPE_OPT_KEY=\"hoodie.datasource.write.storage.type\"\n",
    "COMPACTION_INLINE_OPT_KEY=\"hoodie.compact.inline\"\n",
    "COMPACTION_MAX_DELTA_COMMITS_OPT_KEY=\"hoodie.compact.inline.max.delta.commits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e614702708a4bf0a6d398f94dc36925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Generates Data\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def get_json_data(start, count, dest):\n",
    "    time_stamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    data = [{\"trip_id\": i, \"tstamp\": time_stamp, \"route_id\": chr(65 + (i % 10)), \"destination\": dest[i%10]} for i in range(start, start + count)]\n",
    "    return data\n",
    "\n",
    "# Creates the Dataframe\n",
    "def create_json_df(spark, data):\n",
    "    sc = spark.sparkContext\n",
    "    return spark.read.json(sc.parallelize(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulk Insert the Initial Dataset\n",
    "\n",
    "Let's generate 2M records to load into our Data Lake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c81e7043c640f1b7e5cdb4810aba02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+-------+-------------------+\n",
      "|  destination|route_id|trip_id|             tstamp|\n",
      "+-------------+--------+-------+-------------------+\n",
      "|      Seattle|       A|      0|2020-07-16 10:39:11|\n",
      "|     New York|       B|      1|2020-07-16 10:39:11|\n",
      "|   New Jersey|       C|      2|2020-07-16 10:39:11|\n",
      "|  Los Angeles|       D|      3|2020-07-16 10:39:11|\n",
      "|    Las Vegas|       E|      4|2020-07-16 10:39:11|\n",
      "|       Tucson|       F|      5|2020-07-16 10:39:11|\n",
      "|Washington DC|       G|      6|2020-07-16 10:39:11|\n",
      "| Philadelphia|       H|      7|2020-07-16 10:39:11|\n",
      "|        Miami|       I|      8|2020-07-16 10:39:11|\n",
      "|San Francisco|       J|      9|2020-07-16 10:39:11|\n",
      "|      Seattle|       A|     10|2020-07-16 10:39:11|\n",
      "|     New York|       B|     11|2020-07-16 10:39:11|\n",
      "|   New Jersey|       C|     12|2020-07-16 10:39:11|\n",
      "|  Los Angeles|       D|     13|2020-07-16 10:39:11|\n",
      "|    Las Vegas|       E|     14|2020-07-16 10:39:11|\n",
      "|       Tucson|       F|     15|2020-07-16 10:39:11|\n",
      "|Washington DC|       G|     16|2020-07-16 10:39:11|\n",
      "| Philadelphia|       H|     17|2020-07-16 10:39:11|\n",
      "|        Miami|       I|     18|2020-07-16 10:39:11|\n",
      "|San Francisco|       J|     19|2020-07-16 10:39:11|\n",
      "+-------------+--------+-------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "mor_dest = [\"Seattle\", \"New York\", \"New Jersey\", \"Los Angeles\", \"Las Vegas\", \"Tucson\",\"Washington DC\",\"Philadelphia\",\"Miami\",\"San Francisco\"]\n",
    "df2 = create_json_df(spark, get_json_data(0, 2000000, mor_dest))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Bulk insert will take the same time as COW as this is the first write \n",
    "\n",
    "We will be using the Copy on write storage option(STORAGE_TYPE_OPT_KEY, \"MERGE_ON_READ\") which  is to be explicitly set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df2.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL)\n",
    "      .option(BULK_INSERT_PARALLELISM, 3)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .option(STORAGE_TYPE_OPT_KEY, \"MERGE_ON_READ\")\n",
    "      .option(COMPACTION_INLINE_OPT_KEY, \"false\")\n",
    "      .option(COMPACTION_MAX_DELTA_COMMITS_OPT_KEY, \"20\")\n",
    "      .mode(\"Overwrite\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the number of files \n",
    "\n",
    "Let us check the contents of S3 path. Bulk insert operation on Copy-On-Write and Merge-On-Read tables is identical in terms of performance. \n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://<Your S3 Bucket Here>/tmp/hudi/hudi_mor_trips_table/\n",
    "                           PRE .hoodie/\n",
    "2020-04-28 23:30:20          0 .hoodie_$folder$\n",
    "2020-04-28 23:30:26         93 .hoodie_partition_metadata\n",
    "2020-04-28 23:30:33    4378000 45b1ce07-f9ac-496d-8b03-20af011a0c44-0_1-194-3566_20200428233020.parquet\n",
    "2020-04-28 23:30:34    5048941 932d5e97-c5f0-4c91-a7f6-f65d487a5e2b-0_2-194-3567_20200428233020.parquet\n",
    "2020-04-28 23:30:34    5065824 ea6e8bfa-e70c-4f7e-90ec-37d018fb0acf-0_0-194-3565_20200428233020.parquet\n",
    "```\n",
    "\n",
    "Notice the delta commits \n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://<Your S3 Bucket Here>/tmp/hudi/hudi_mor_trips_table/.hoodie/\n",
    "2020-04-28 23:30:21          0 .aux_$folder$\n",
    "2020-04-28 23:30:21          0 .temp_$folder$\n",
    "2020-04-28 23:30:37       1077 20200428233020.clean\n",
    "2020-04-28 23:30:36       4929 20200428233020.deltacommit\n",
    "2020-04-28 23:30:21          0 archived_$folder$\n",
    "2020-04-28 23:30:21        264 hoodie.properties\n",
    "```\n",
    "\n",
    "This is the first commit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Upsert some records\n",
    "\n",
    "\n",
    "Now let us try to upsert some records into MOR table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_dest = [\"San Diego\", \"San Diego\", \"San Diego\", \"San Diego\", \"San Diego\",\"San Diego\",\"San Diego\",\"San Diego\",\"San Diego\",\"San Diego\"]\n",
    "df3 = create_json_df(spark, get_json_data(1000000, 10, upsert_dest))\n",
    "df3.count()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df3.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .option(STORAGE_TYPE_OPT_KEY, \"MERGE_ON_READ\")\n",
    "      .option(COMPACTION_INLINE_OPT_KEY, \"false\")\n",
    "      .option(COMPACTION_MAX_DELTA_COMMITS_OPT_KEY, \"20\")\n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the number of tables created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can observe that there are two tables created for  Merge On Read storage option. \n",
    "\n",
    "1.hudi_mor_trips_table     \n",
    "2.hudi_mor_trips_table_rt  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets query the table  -> hudi_mor_trips_table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select trip_id, route_id, destination, tstamp from \"+config['table_name']+\" where trip_id between 999996 and 1000010\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets query the real-time table -> hudi_mor_trips_table_rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select trip_id, route_id, destination, tstamp from \"+config['table_name']+\"_rt where trip_id between 999996 and 1000010\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the S3 path again. There is no change in number of Parquet files after upsert operation unlike Copy-On-Write tables. \n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://<Your S3 Bucket Here>/tmp/hudi/hudi_mor_trips_table/\n",
    "                           PRE .hoodie/\n",
    "2020-04-28 23:33:22       2071 .ea6e8bfa-e70c-4f7e-90ec-37d018fb0acf-0_20200428233020.log.1_0-227-3837\n",
    "2020-04-28 23:30:20          0 .hoodie_$folder$\n",
    "2020-04-28 23:30:26         93 .hoodie_partition_metadata\n",
    "2020-04-28 23:30:33    4378000 45b1ce07-f9ac-496d-8b03-20af011a0c44-0_1-194-3566_20200428233020.parquet\n",
    "2020-04-28 23:30:34    5048941 932d5e97-c5f0-4c91-a7f6-f65d487a5e2b-0_2-194-3567_20200428233020.parquet\n",
    "2020-04-28 23:30:34    5065824 ea6e8bfa-e70c-4f7e-90ec-37d018fb0acf-0_0-194-3565_20200428233020.parquet\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting Records.\n",
    "\n",
    "Apache Hudi supports implementing two types of deletes on data stored in Hudi datasets, by enabling the user to specify a different record payload implementation.\n",
    "\n",
    "* **Soft Deletes** : With soft deletes, user wants to retain the key but just null out the values for all other fields. This can be simply achieved by ensuring the appropriate fields are nullable in the dataset schema and simply upserting the dataset after setting these fields to null.\n",
    "    \n",
    "* **Hard Deletes** : A stronger form of delete is to physically remove any trace of the record from the dataset. \n",
    "\n",
    "Let's now execute some hard delete operations on our dataset which will remove the records from our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's delete the 10 records with the \"San Diego\" destination we upserted to the table. Note that the only change is the single line that set the hoodie.datasource.write.payload.class to org.apache.hudi.EmptyHoodieRecordPayload to delete the records.\n",
    "\n",
    "```\n",
    ".option(PAYLOAD_CLASS_OPT_KEY, EMPTY_PAYLOAD_CLASS_OPT_VAL)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df3.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .option(STORAGE_TYPE_OPT_KEY, \"MERGE_ON_READ\")\n",
    "      .option(PAYLOAD_CLASS_OPT_KEY, EMPTY_PAYLOAD_CLASS_OPT_VAL)\n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Query and observe that the updated records do not exist in the two tables after deletion. \n",
    "\n",
    "1.hudi_mor_trips_table     \n",
    "2.hudi_mor_trips_table_rt  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select trip_id, route_id, destination, tstamp from \"+config['table_name'] +\" where trip_id between 999996 and 1000010\").show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select trip_id, route_id, destination, tstamp from \"+config['table_name']+\"_rt where trip_id between 999996 and 1000010\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Check the count of records in the two tables after deletion. \n",
    "\n",
    "1.hudi_mor_trips_table     \n",
    "2.hudi_mor_trips_table_rt  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observe that total record count indicates that 10 upserted records have been deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from \"+config['table_name']+\"_rt\" ).show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from \"+config['table_name']).show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
