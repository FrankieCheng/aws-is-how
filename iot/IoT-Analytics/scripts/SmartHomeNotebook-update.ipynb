{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Smart Home Sensor Analysis</h1>\n",
    "<p>The ‘Household Power Consumption‘ dataset is a multivariate time series dataset that describes the electricity consumption for a single household for last few months. The  dataset is modeled after household consumption dataset available here - \n",
    "https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+consumption\n",
    "\n",
    "\n",
    "It is a multivariate series comprised of seven variables (besides the date and time); they are:\n",
    "\n",
    "<b>global_active_power:</b> The total active power consumed by the household (kilowatts).<br>\n",
    "<b>global_reactive_power:</b> The total reactive power consumed by the household (kilowatts).<br>\n",
    "<b>voltage:</b> Average voltage (volts).<br>\n",
    "<b>global_intensity:</b> Average current intensity (amps).<br>\n",
    "<b>sub_metering_1:</b> Active energy for kitchen (watt-hours of active energy).<br>\n",
    "<b>sub_metering_2:</b> Active energy for laundry (watt-hours of active energy).<br>\n",
    "<b>sub_metering_3:</b> Active energy for climate control systems (watt-hours of active energy).<br>\n",
    "\n",
    "<p> In the following section, we will analyze and predict hourly power consumption using DeepAR on SageMaker. The purpose of this exercise is to demonstrate Sagemaker integration with IoT Analytics and not to focus on training the right model to generate accurate prediction.\n",
    "<p>For more information see the DeepAR [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html) or [paper](https://arxiv.org/abs/1704.04110), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "from random import shuffle\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider, FloatSlider, Checkbox\n",
    "\n",
    "from sagemaker import get_execution_role\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, we can override the default values for the following:\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = sagemaker.Session().default_bucket()# replace with an existing bucket if needed\n",
    "\n",
    "s3_prefix = 'iot-analytics-demo-notebook'    # prefix used for all data stored within the bucket\n",
    "\n",
    "role = get_execution_role()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker_session.boto_region_name\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we configure the container image to be used for the region that we are running in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: latest.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522234722520.dkr.ecr.us-east-1.amazonaws.com/forecasting-deepar:1\n"
     ]
    }
   ],
   "source": [
    "image_uri = sagemaker.image_uris.retrieve(\"forecasting-deepar\", region, \"latest\")\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import  dataset from the IotAnalytics database and upload it to S3 to make it available for Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reading from the IotAnalytics database\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from numpy import isnan\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from numpy import isnan\n",
    "\n",
    "\n",
    "bucket='iotareinvent18'\n",
    "data_key = 'inputdata.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "\n",
    "\n",
    "# Function to convert date columns into a single timestamp field\n",
    "#def parse(x):\n",
    "#    return datetime.strptime(x, '%Y %m %d %H')\n",
    "def parse(x):\n",
    "    t= pd.to_datetime(str(x)) \n",
    "    timestring = t.strftime('%Y.%m.%d %H:%M:%S')\n",
    "    return t\n",
    "\n",
    "def fill_missing(values):\n",
    "    one_day = 60 * 24\n",
    "    for row in range(values.shape[0]):\n",
    "        for col in range(values.shape[1]):\n",
    "            if np.isnan(values[row, col]):\n",
    "                values[row, col] = values[row - one_day, col]\n",
    " \n",
    "# load sample data\n",
    "\n",
    "#n = sum(1 for line in open(data_location)) - 1 #number of records in file (excludes header)\n",
    "n=244207 #total observations\n",
    "s = 10000 #desired sample size\n",
    "sampling = sorted(random.sample(range(1,n+1),n-s)) #the 0-indexed header will not be included in the skip list\n",
    "\n",
    "dataset = pd.read_csv(data_location,  header=0, skiprows=sampling ,low_memory=False, infer_datetime_format=True, date_parser = parse)\n",
    "dataset['__dt'] = dataset['timestamp']\n",
    "dataset['cost'] = (dataset['sub_metering_1'] +dataset['sub_metering_2'] + dataset['sub_metering_3'])*1.5\n",
    "dataset['timestamp'] = pd.to_datetime(dataset['timestamp'])\n",
    "dataset.set_index('timestamp', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####Examine the dataset\n",
      "dataset shape  (10000, 9)\n",
      "                     global_active_power  global_reactive_power  voltage  \\\n",
      "timestamp                                                                  \n",
      "2018-09-21 11:28:00                 2.05                   0.75   237.85   \n",
      "2018-08-26 00:40:00                 0.29                   0.09   242.73   \n",
      "2018-06-30 05:14:00                 1.54                   0.32   238.85   \n",
      "2018-08-26 20:16:00                 0.38                   0.09   243.18   \n",
      "2018-08-23 07:05:00                 0.43                   0.09   242.36   \n",
      "\n",
      "                     global_intensity  sub_metering_1  sub_metering_2  \\\n",
      "timestamp                                                               \n",
      "2018-09-21 11:28:00               9.8               0               0   \n",
      "2018-08-26 00:40:00               1.4               0               0   \n",
      "2018-06-30 05:14:00               6.6               0               1   \n",
      "2018-08-26 20:16:00               1.6               0               0   \n",
      "2018-08-23 07:05:00               1.8               0               0   \n",
      "\n",
      "                     sub_metering_3                 __dt  cost  \n",
      "timestamp                                                       \n",
      "2018-09-21 11:28:00              11  2018-09-21T11:28:00  16.5  \n",
      "2018-08-26 00:40:00               1   2018-08-26T0:40:00   1.5  \n",
      "2018-06-30 05:14:00              21   2018-06-30T5:14:00  33.0  \n",
      "2018-08-26 20:16:00               1  2018-08-26T20:16:00   1.5  \n",
      "2018-08-23 07:05:00               0   2018-08-23T7:05:00   0.0  \n"
     ]
    }
   ],
   "source": [
    "print(\"#####Examine the dataset\")\n",
    "print(\"dataset shape \",dataset.shape)\n",
    "print(dataset.sample(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Explore the data</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeUAAAGfCAYAAABhicrFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABCUElEQVR4nO3deZxcVZ3+8c9D2HdZVAwgiLigQJCAIqCgqIgCo6IRlwF1RFQEd/HHqHEbZMQNN8w4LC4soiggKCAQFhVIgIQAsomgLKNGFEExQPL8/ri3k6LorbordU91P+/Xq15dd6l7nypCf/uce+pc2SYiIiKat0LTASIiIqKSohwREVGIFOWIiIhCpChHREQUIkU5IiKiECnKERERhUhRjoiIKESKckRERCFSlKMvSZoi6X1N54iI6KYU5ehLthcD+zadIyKim5RpNqNfSfossA5wKvCPgfW2r24sVETEOKQoR9+SdNEgq237RT0PExHRBSnKERERhcg15ehbkp4g6X8l/axe3krS25rOFRExVinK0c9OAM4FnlQv3wy8t6kwERHjlaIc/WwD2z8AlgDYfgRY3GykiIixS1GOfvYPSesDBpD0POC+ZiNFRIzdik0HiBiHDwBnAltI+iWwIbBfs5EiIsYuo69j1CRNAQ61/aWmswyQtCLwdEDATbYfbjhSRMSYpfs6Rq20WbQkXQp8EtgEuCMFOSL6XVrK0ZGSZtGS9BRgF2BX4HnAIuBS25kTOyL6Uq4pR6eeX//8VMs6Az2fRcv2bZIeBB6qH7sDz+x1joiIbklLOfqWpN8CC4GTgEuBebaXNJsqImLsck05OlLYLFrHAL8H9gcOBQ6QtEVDWSIixi0t5ehIXYyPB46wvW09+vka21s3mGlN4C3AB4GNbU9pKktExHikpRydKmYWLUlfkHQFcAUwDfg4sGUTWSIiuiEDvaJTJc2idTnw37b/2ND5IyK6Kt3X0RFJ21Ndy302cB31LFq2r20ozz7AC+rFi22f1USOiIhuSFGOjpUyi5akI4Edge/Xq/YH5tr+aBN5IiLGK0U5OlLPonUJ1VeQfmn7/gazXAtMG/gaVD0N6DW2t2kqU0TEeGSgV3TqAOAm4DXAryTNldTkXNjrtjxfp6kQERHdkIFe0ZHCZtE6ErhG0kVUXekvANJ1HRF9K93X0ZHSZtGStBGwQ714pe3/aypLRMR4paUcnTqG6iYQ+wPbARdLusT2bxvKs1Odx8AU4McN5YiIGLe0lGNMSphFS9I3gKcCJ9erZgC/tf3uXmeJiOiGFOXoiKQvULVM16SavOMSqtsl3tZAluuBZ7v+RyxpBWCB7Wf1OktERDek+zo6VdIsWjcBmwJ31MubAI1MYhIR0Q1pKUfHSplFS9LFVIO8rqxX7QD8GvgngO19msgVETFWKcrRkZJm0ZL0wuG22764V1kiIrohRTk60k+zaEn6te2dms4RETFamdErxmLdluclz6K1atMBIiI6kaIcnRqYResESScCVwH/1XCmofSsG0jS0ZIy6nsUJP2sx+dbW9KRkr4r6Q1t277RwxxPlPRNSV+XtL6kmZIWSPpBPQlORLqvo3P9MouWpKttP6dH5/oPqu9trwgcD5xsu6n7TDdO0lCfu4Cf2u5ZEZL0I+AWqm8OvBV4GHiD7UU9/jfyc+BsYA3gDVTjMk4G9gX2sL1vL3JE2VKUo2OSXs2yWbQus13kLFqSrrG9XY/P+XSq4rw/8Evgf2xf1MsMJZC0GLiYqgi3e57t1XqYZZ7taS3LRwB7AfsA5/ewKC/99yjp97Y3HSpjTF75nnJ0ZJBZtN4haY+mZtGS9GRgS9u/kLQasGLL7STf3OMsU4Bn1I+FwHzg/ZLeYfv1vcxSgN8A77B9S/sGSX/ocZZVJK0wMDjR9mcl3Uk18c2aPczRernwO8Nsi0ksRTk69UIePYvWicCCJoJIejtwELAesAWwMXAs8GIA29f1MMsXgb2BC4H/sj3w3emjJN3UqxwFmcnQheY9PcwBcBbwIuAXAytsnyjpj8BXe5jjDElr2n7A9n8OrJT0VODmHuaIgqX7Ojoi6XTgfbbvqJefDHzO9v4NZJlH9Z3pK1q6BRfY3rqBLG8FTrH9z0G2rTOZry8PR9IBtk9sOgeUk6WUHNGMdJlEp9YHfiNptqTZwA3AhpLOlHRmj7Mssv3QwIKkFenhiOs2b2wvyJIuAEhBHtZhTQdoUUqWUnJEA9J9HZ36eNMBWlws6f8Bq0l6CfAuqq7KnpG0KrA6sIGkx7FsYNPawJN6maVPDTYQrCmlZCklRzQgRTk6MtLUlT2eRetw4G1U17TfAZwDfLtH5x7wDuC9VAX46pb1fwe+3uMs/aik62elZCklRzQgRTm6rZezaO0LfMf2//TwnI9i+yvAVyS9x3YvBw1NFCW1CkvJUkqOaECKcnRbL//K3wf4sqRLgFOAc20/0sPzI+lFti8E7qq/v/0otk/vZZ4+9MumA7QoJUspOaIBGX0dXdXLGZLq860EvByYQTWhyfm2/6OH5/+k7U9IOn6Qzbb91l5lKZGk9w+y+j7gKtvzJmOWUnJEmVKUo6samkVrJWBPqpm0drW9YS/PH0OTdBIwnWUD8F4BzKGaYOU02/892bKUkiPKlK9ERcckPVnSHvXz1SSt1bK5Z7NoSdpT0gnArcB+VIO8GpnYX9Jh9Y0PJOnbkq6W9NImshRmfeA5tj9g+wNUxWhD4AXAgZM0Syk5okApytGRehatHwLfqldtDPxkYHsvZ9Gi+gX2E+Bptg+wfU6vrym3eKvtvwMvBR5P1Wr/XENZSrIp8FDL8sPAk20/CCyapFlKyREFykCv6NS7qWfRArB9i6THNxGksPmkB0bM7gUcb3u+pIyihZOAyyWdUS/vDZwsaQ2qiWcmY5ZSckSBck05OiLpCtvPHbh2XM+idbXtbXqY4TLbu0i6n0eP9hbV4Kq1e5WlJdPxwFRgc2BbYAow2/b2vc5SGknTgZ2p/vtcZnvuZM9SSo4oT4pydETSfwN/A/6d6sYC7wJusH1Ek7maJmkFYBpwm+2/SVofmGr72maTNa++e9YTaOmZs/37yZyllBxRnhTl6EhdfN5Gde1UwLnAt93APyRJ37X95pHW9TDPVODJPPoX7SVNZCmFpPcAnwD+CCxmWW9Gz3pWSstSSo4oU4pydETSq4BzbDc+IKX9O9F1V/q1trdqIMtRVN+VvoHqFy1Uv2j36XWWkki6FXiu7b8kS1k5okwZ6BWdKmEWrY8CAzei+PvAaqoRrbN6maXFvwFPL+GPlcL8gWpijBKUkqWUHFGgtJSjY03PotWS40jbH+31eQcj6WfAa20/0HSWkkj6X+DpwNm0fN3H9hcna5ZSckSZ0lKOjtl+uC5CBlajujFEz4sycKWkdQbuVyxpXWA32z9pIMs/gXn1PZRbf9Ee2kCWkvy+fqxcP5KlnBxRoLSUoyOS9gReD+wOzAZOBc5rYtIOSfNsT2tb1/NpPuvzHjDYetsn9jpLRPSvtJSjUwdSXUt+RwHXTwebka6Rf9O2T5S0GrCp7ZuayFASSV+2/V5JZzHIncN6OQCulCyl5IiypShHRwqbRWuupC8CX6f6Jfce4KomgkjaGziaqjtyc0nTgE9N4l+0361/Ht1oikopWUrJEQVL93WMSqGzaK0BfAzYo85xHvAZ2/9oIMtVwIuoZvHarl63wPbWvc5SinqCjBNtvylZysoR5UpLOUbF9i71z7VG2rdX6uJ7eNM5ao/Yvq9tuutJ/Rev7cWSNpS0su2HRn7FxM9SSo4oV4pydKSkWbQkbQh8GHgWsOrAetsv6nUW4DpJbwCmSNoSOBT4VQM5SnM78EtJZwJLezAa+vpPKVlKyREFyq0bo1PPal2oZ9Fq6qYL3wdupLoJxCepftnNaSjLe6g+m0VUdwG6DzisoSwluRv4KdXvmrVaHpM5Syk5okC5phyj0jqLFtV3cqFlFq0mJvGQdJXt7SVdOzBvsKSLbb+wgSyvtX3aSOsmK0lrNHGtfzClZCklR5QlLeUYFdtH1teTP2977fqxlu31G5xV6+H65z2SXiFpO2DjhrIM9hkUMdtYkyTtJOkG4Df18raSvjGZs5SSI8qUa8rRqZJm0fqMpHWADwBfBdYG3tfLAJJeDuwFTJV0TMumtYGeT6hSoC8DLwPOBLA9X9ILJnmWUnJEgVKUo1OfsP3jgYX63sGfAH7S6yC2f1o/vY9qhrEm3A3MpbpRR+t3pO+nx38glMr2H9pGpS8eat/JkqWUHFGeFOXoVDGzaEl6GvBN4Am2ny1pG2Af25/pVQbb84H5kk6y/fCIL5h8/iDp+YAlrUw1Kv03kzxLKTmiQLmmHJ2aK+mLkraQ9BRJX6KhWbSA/6G6bvswgO1rqeblbsKOks6XdLOk2yT9TtJtDWUpycHAu4GpwJ3ANOBdkzxLKTmiQGkpR6feQzWL1qksm0Xr3Q1lWd32lW3dgE1dx/1fqu7qq0hXZKun235j6wpJOwO/nMRZSskRBcpXoqJv1bePPAQ4zfZzJO0HvM32yxvIcoXt5/b6vKWTdLXt54y0bjJlKSVHlCkt5ehIYbNovRuYBTxD0l3A74A3Dv+S5eYiSZ8HTufR91O+uqE8jZK0E/B8YENJ72/ZtDYwZTJmKSVHlC1FOTr1faqu61dSXRs7APhzr0PUE/u/0/Ye9Y0pVrB9f69ztBhoJU9vWWeqm1RMRisDa1L9jmmdrervwH6TNEspOaJg6b6OjhQ2i9aFDbXQY5QkPdn2HSXMXlVKllJyRJnSUo5OPWoWLarv6TY1i9Y19aT+p/Hoif1P71UASW+y/b227silcpMBnlRf+18T2FTStsA7bDcx2riULKXkiAKlKEenGp9Fq8V6wF94dBexqa7r9soa9c8ibiggaXWq/zab2n57fceqp7dMtNJrX6ac2atKyVJKjihQinJ0ZKRZtCR91PaRPcryluG29yKL7W/VPz/ZdJba8VRfy9qpXr6TqiehqaJc1OxVpWQpJUeUJ5OHRLe9tukALSZjli1s/zfLJlR5kOr75E151OxVkj5IITN6NZillBxRoBTl6LYmC0C7yZjlIUmrUXXjI2kLWr6i1YDBZq9qarKZUrKUkiMKlO7r6LaShvNPxiyfAH4ObCLp+8DOwIE9Ovdj2F5Ic98df5RSspSSI8qUohzdNhlbp6PRkyy2z5d0NfC8+pyH1UWgEZI2p5qadTNaft/Y3meyZiklR5QpRTm67bSmA7SYdFkkDUzVeE/9c9N6tPwdtpuYF/wnVPOCnwUsaeD8rX5CGVlKyREFyuQhMSqSvsowXbC2D02WZrMASLoceA5wLVVL+dn18/WBg22f1+M8xcwJXkqWUnJEmdJSjtGa23SAFskytNupbspxPYCkrYAPAZ+m+v52T4sy8BVJn6jP2/Sc4KVkKSVHFCgt5YgJRNI829MGWzfYth7kORJ4M/BblnXVuonpUUvJUkqOKFNaytGR+i5RHwG2ouG7RCXLoG6S9E3glHp5BnCzpFVYNkVqL70KeIrthxo4d7tSspSSIwqU7ylHp75PNdHB5sAnqbpL5yRLMVkOBG4F3ks1/elt9bqHGWQGth6YD6zbwHkHU0qWUnJEgdJ9HR0p7C5RyVI4SbOBbaj+QGm9ftrEV6KKyFJKjihTuq+jUyXdJSpZ2tQ3oDiSx3ajP6XXWWqfaOi8gyklSyk5okBpKUdHJL0SuBTYhGV3ifqk7TOTpfkski6j+qX/JWBv4C1U/58XWQgk/dr2TiPvufyVkqWUHNGMFOWICaSlG32B7a3rdZfa3rXpbIORdI3t7ZrOAeVkKSVHNCMDvaIjkp4i6SxJCyX9SdIZkhrpGk2WQf1L0grALZIOkfQq4PEN5BitkloFpWTpWQ5Jj7l72WDrondSlKNTJwE/AJ4IPIlq+siTk6WYLO8FVgcOBbYH3gT8ewM5oj98dJTrokcy0Cs6JdvfbVn+nqRDkqWYLJvZngM8QHU9eaDlc0UDWUZj0t00ZBSWew5JLwf2AqZKOqZl09pAE3OkRy3XlGNUJK1XP/0w8DeqySlMNTnFKrY/nSzNZqnzXG37OSOt63GmJwI7Un0uc2z/X8u2Z9u+brJlaTqHpG2p7uP8KeDjLZvuBy6y/dflef4YWopyjIqk31H9Ahnsr3j38is3yTJojoGWz+uAU1s2rQ1sZXvHXuQYJNd/UP3Sv5DqM3oh8Cnbx03WLKXkqLOsZPvh+vnjgE1sX9vrHLFMinLEBFC3fLajmk2smJaPpJuA59v+S728PvAr20+frFlKyVGfezawD9WlzHnAn4GLbb+/11mikmvK0RFJKwHvBF5Qr5oNfGvgr+1kaSaL7fnAfEnfa+i+yUO5k+oPgwH3A3+Y5FlKyQGwju2/1633421/QlJayg1KUY5OfRNYCfhGvfzmet1/JEtzWSQtoP4qjfTYnvSBqT97RdJAS+su4ApJZ1Dl2xe4cjJmKSVHmxUlbUR12eOIhjJEixTl6NQOtrdtWb5Q0vxkaTzLK3t4rtFYq/752/ox4IxJnKWUHK0+BZwL/NL2nPq79bc0mGfSyzXl6Iikq4HX2v5tvfwU4IdNjO5NliGzPAHYoV680vafep0hIsYmLeXo1IeAiyTdRjVy9MnU34dNluazSHod8Hmqa9oCvirpQ7Z/2OssdZ6LGGSGqobueV1EllJy1Fk2ppqrfec602XAYbbv7HWWqKSlHB2TtArwdKpf+jfaXjTCS5KldxnmAy8ZaB1L2hD4RVvXei/zbN+yuCrwGuAR2x+erFlKyVFnOZ9qNrqBiW/eBLzR9kt6nSUqKcoxKpJePdx226cnS7NZoBrwNXAjinp5BWB+67qmqaD7TJeSpakckubZnjbSuuiddF/HaO3dtjzw15zq570sPskytJ9JOpdl827PAM7pcYalWmY8g2qu/elU84NP2iyl5KgtlPQmlv172R/4S0NZgrSUo0OSPsCjZ7AycB9wle15ydJslvprN3+mmkJRwKW2f9yr8w+Sp3XGs4eB26lmr7pssmYpJUedZVPga8BOdaZfAYfa/n2vs0Qld4mKTm0PHAxsRHU3pIOA3YD/kdTra2LJ8lhrAYdTzav8W6pfsk36CDDN9uZU1y3/AfxzkmcpJQfAp4EDbG9o+/HAW4GZDWUJANt55DHqB9V3GtdsWV4T+DmwGnBDsjSfpT7/NsBngRupBno19e/l2vrnLsAlVBNlXDGZs5SSo85wzWjW5dG7R1rK0alNgYdalh8Gnmz7QaDXo42TZWh/Av6P6vrg4xs4/4DF9c9XAMfaPgNYeZJnKSUHwAr1jSiApde7M9aoQfnwo1MnAZfXUwRCNdDpZElrADckS7NZJL2TanDXhsAPgbfb7vVn0eouSd8C9gCOqr821lRjoJQspeQA+ALwK0k/pLqm/DqqHpZoSAZ6Rcfq71nuQjVQ5TLbc5OljCySPgec4h4PdBuKpNWBPYEFtm+p51ne2vZ5kzVLKTla8mwFvIjq3+0FDf8RN+mlKEdERBQi15QjIiIKkaIcYybpoKYzDEiWwSXL4ErJUkoOKCtLCSQdJ+lPkq4bYrskHSPpVknXSurKzWdSlGM8SvqfOFkGlyyDKyVLKTmgrCwlOIHq2v9QXg5sWT8Oorp/+rilKEdERLSxfQlw7zC77At8x5XLgXXrQXvjkq9ETVIbrDfFm22y0riOsenUFZm+7arjHim44N4Nx3sIVlz3cayyySbjzrL1en8ed5ZufS43X7v6uLOsyuqsrfXGlWXxU1cZdw6AlR+/NmtsudG4sjzyyJSuZJmy/rqssvnG48qyzmoPjjvHmk9cncdvtf64/62sv+ID487yxKlTeOY2q4w7y40LHlpoe/z/U4/Sy3Zfw3+5d/HIO7a56tpF1wP/alk1y/asDg4xFfhDy/Kd9bp7Og7TIkV5ktpsk5W48txNmo4BwJbfe2fTEZa68k1d6YHqipdN3a7pCAD89ZinNh1hqYUL12o6wlKvfNaCpiMs9ab1m55NdZmdNrvjjl6e7y/3LubKczft+HVTNrrlX7anj+PUGmTduP+oSVGOiIi+ZWAJS5o49Z1Aa8tmY+Du8R4015QjIqKPmcVe0vGjC84E/r0ehf084D7b4+q6hrSUIyKij1Ut5e5PgiXpZKo7vW0g6U7gE8BKALaPpbpP+V7ArVR3+XpLN86bohwREX1teXRf295/hO0G3t3t86YoR0RE3zJm8QSaLjpFOSIi+try6L5uSopyRET0LQOLJ1BRzujriIiIQqSlHBERfS3d1xEREQUwZKBXREREKRqZz2s5SVGOiIi+ZTyhBnqlKEdERP8yLJ44NXn5j76WdIKk/UbY53ZJG3RwzAMlfW386ZYe7/+1LZdzy5WIiBhSNc1m549S5StRlUcVZdvPbypIt0hKL0hETAJi8RgepepqUZb0MUk3Sjpf0smSPti2/cWSrpG0QNJxklrvnv4hSVfWj6fW++8t6Yr6Nb+Q9IRR5hj0dZLWlHR8ff5rJb1G0ueA1STNk/T9er8H6p+nStqr5bgn1K+ZIunzkubUx3nHMFl2k3SJpB9LukHSsZJWqLftX2e5TtJR9brXSfpi/fwwSbfVz7eQdFn9fHtJF0u6StK5kjaq18+W9F+SLgYOG81nFRHRzwwsceePUnWtKEuaDrwG2A54NTC9bfuqwAnADNtbU13Pbr27/d9t7wh8Dfhyve4y4Hm2twNOAT48yjhDve5jVLfX2tr2NsCFtg8HHrQ9zfYb245zCjCjzr8y8GKqO4O8rT7ODsAOwNslbT5Mnh2BDwBbA1sAr5b0JOAo4EXANGAHSf8GXALsWr9uV+AvkqYCuwCXSloJ+Cqwn+3tgeOAz7aca13bL7T9hfYQkg6SNFfS3D//ZfEwcSMi+sdEail3s4tzF+AM2w8CSDqrbfvTgd/ZvrlePpHqDhtfrpdPbvn5pfr5xsCpdUtwZeB3o8wy1Ov2AF4/sJPtv45wnJ8Bx9Qt+j2BS2w/KOmlwDYt18rXAbYcJt+VtgdavCdTfVYPA7Nt/7le/33gBbZ/Urfo16K6gfZJwAuoCvTpVJ/js4HzJQFMAVrv4XnqUG/G9ixgFsD0bVct+G/FiIjRqabZLLfIdqqb3dcjfSojbfcgz78KfK1uWb8DWHWUWYZ6ndrOM3wg+1/AbOBlVC3mU1qO8566dT3N9ua2zxvuUIMsD/d5/Jrq3pw3AZdSFeSdgF/Wr7u+5dxb235py2v/Mbp3FxExMSyxOn6UqptF+TJgb0mrSloTeEXb9huBzQauFwNvBi5u2T6j5eev6+frAHfVzw/oIMtQrzsPOGRgQdLj6qcP193CgzmFqkDuCpxbrzsXeOfAayQ9TdIaw+TZUdLm9bXkGVSf1RXACyVtIGkKsD/LPo9LgA/WP68BdgcW2b6PqlBvKGmn+twrSXrWMOeOiIg+0bWibHsOcCYwn6qbdS5wX8v2f1EVt9MkLaAalX5syyFWkXQF1QCl99XrZtb7Xwos7CDOUK/7DPC4emDVfKpiB1WX7rUDA73anEfVffwL2w/V674N3ABcLek64FsMfyng18DngOuourh/bPse4KPARVSf2dW2z6j3v5Sq6/oS24uBP1AVcuoM+wFH1e9hHtD3o8UjIsZioPs615QHd7TtmZJWp2rlfcH2/wxstH0B1UCwR7G9Wf30k23rzwDOGGT/E6gGjQ1qmNc9wCAtbtsfAT7Ssrxmy/OHgfXb9l9C9TWqR32Vahj/tD2jfaXtk6iuGbev/y0t3dtt3dPYnkf1h0L763YbZZ6IiAnBiMUT6Nu93S7KsyRtRXUN90TbV3f5+BEREY9S8jXiTnW1KNt+QzePNxJJRwCvbVt9mu3PDrb/cs6yNfDdttWLbD+XarBYRER02UQbfd3Xsz7VxbfnBXgwthdQfd84IiJ6Rix2uq8jIiIaV819naIcERFRhHRfR0REFMBO93VEREQxlqSlHBER0bxq9HVayhEREQVI93VEREQRJtro64nzTiIiIvpcWsqT1IJ7N2TL772z6RgA3PKmbzYdYaktTjm46QhLTd1nSdMRAFh0Xjm/JtZYpekEy1xw4w5NR1jqvFXLyQLv7/kZF0+gaTbTUo6IiL41cEOKTh8jkbSnpJsk3Srp8EG2ryPpLEnzJV0v6S3deD/l/AkcERExBku6PNCrvsf914GXAHcCcySdafuGlt3eDdxge29JGwI3Sfp+yy1+xyRFOSIi+tZy+krUjsCttm8DkHQKsC/QWpQNrCVJwJrAvcAj4z1xinJERPQto7FeU95A0tyW5Vm2Z9XPpwJ/aNl2J/Dcttd/DTgTuBtYC5hhe9wDQVKUIyKir43xK1ELbU8fYttgVd5tyy8D5gEvArYAzpd0qe2/jyXMgAz0ioiIvmXDYq/Q8WMEdwKbtCxvTNUibvUW4HRXbgV+BzxjvO8nRTkiIvqYWDKGxwjmAFtK2lzSysDrqbqqW/0eeDGApCcATwduG++7Sfd1RET0LUPXp9m0/YikQ4BzgSnAcbavl3Rwvf1Y4NPACZIWUHV3f8T2wvGeO0U5IiL62vK4IYXtc4Bz2tYd2/L8buCl3T5vinJERPQtI5ZkRq+IiIjotrSUIyKir+V+yhEREQUw3Z9ms0kpyhER0cfE4pG/4tQ3xvznhaQTJO03wj63S9qgg2MeKOlrY800VpL+X9vyr3qdISIiOjfQUu70Uapyk41AUjdb+Y8qyraf38VjN6LLn09ERLEW163lTh6lGlVRlvQxSTdKOl/SyZI+2Lb9xZKukbRA0nGSWm9F/iFJV9aPp9b77y3pivo1v6hnQxlNjhMkfVHSRcBRkraQ9HNJV0m6VNIzhju+pDUlHV/nvFbSayR9DlhN0jxJ36/3e6D+eaqkvdrO/xpJUyR9XtKc+jjvGCbzbpIukfRjSTdIOlbSCvW2/ess10k6ql73OklfrJ8fJmngLiVbSLqsfr69pIvr932upI3q9bMl/Zeki4HDRvOZRkT0M1uTq6UsaTrwGmA74NXA9LbtqwInUN0hY2uq69TvbNnl77Z3pLqjxpfrdZcBz7O9HXAK8OEOMj8N2MP2B4BZwHtsbw98EPjGCMf/GHCf7a1tbwNcaPtw4EHb02y/se1cpwAz6ve5MtWUaucAb6uPswOwA/B2SZsPk3lH4APA1lQTl79a0pOAo6gmM58G7CDp34BLgF3r1+0K/EXSVGAX4FJJKwFfBfar3/dxwGdbzrWu7Rfa/kJ7CEkHSZorae6SB/4xTNyIiP6xHOa+bsxoujh3Ac6w/SCApLPatj8d+J3tm+vlE6lu/vzlevnklp9fqp9vDJxat/BWpprIe7ROs71Y0prA84HTqttZAjDQQh/q+HtQzWEKgO2/jnCunwHH1C3/PYFLbD8o6aXANi3X1NcBthzmfVzZcl/Ok6k+04eB2bb/XK//PvAC2z+pW/RrUU2IfhLwAqoCfTrV5/1sqjuSQDUF3D0t5zp1qDdT35ZsFsAqm2zSfseTiIi+YxjNXNZ9YzRFeaR3O9J2D/L8q8AXbZ8paTdg5ihyDBho4q0A/M32tEH2Ger44rG33xqS7X9Jmk11i64ZLPsDQ1Qt9HNHe6hBlof73H5NdQeSm4BLgbcCO1G1tjcFrre90xCvTRM4IiYRFd3y7dRo3sllwN6SVq1bp69o234jsNnA9WLgzcDFLdtntPz8df18HeCu+vkBHacG6ntW/k7SawFU2XaE458HHDKwIOlx9dOH627hwZxCVSB3pZqcnPrnOwdeI+lpktYYJu6Oqu42sgLV53AZcAXwQkkbSJoC7M+yz+0Squ74S4BrgN2BRbbvoyrUG0raqT73SpKeNcy5IyImrGr0tTp+lGrEomx7DtUtq+ZTdZ/OBe5r2f4vqqJ1mqq7ZSwBjm05xCqSrqAaePS+et3Mev9LgfHcVeONwNskzQeuB/Yd4fifAR5XD6yaT1XsoOrSvXZgoFeb86i6j39h+6F63beBG4CrJV0HfIvhex1+DXwOuI6qi/vHtu8BPgpcRPXZXm37jHr/S6m6ri+xvRj4A1Uhp86wH9VAt/lUN9nu+9HiERFjtZgVOn6USvbIvbmS1rT9gKTVqVpvB9m+ermnmwDq7vMP2n5lw1EeZZVNNvHGh71v5B174JY3fbPpCEttccrBTUdYaurFS5qOAMB9m5Xz7bolq4y8T68smdJ0gmUWr9p0gmVu+fj7r7I9feQ9u2OjZz3OB5z84o5fd9S2P+ppztEa7f9tsyRtBawKnJiCHBERJZhod4kaVVG2/YblHaSVpCOA17atPs32ZwfbvwSStga+27Z6ke3nArN7nygiYnJYUnB3dKfK6ZdqURffYgvwYGwvoPq+cURE9IgNiydbSzkiIqJUk677OiIiokTVNeV0X0dERBSh5BtMdCpFOSIi+tbA5CETRYpyRET0sXRfR0REFGOy3ZAiIiKiSPlKVEREREHSfR19b+v1/syVhcw5XdJ80799/bEj79QjL3v/tKYjAPDwz7ZoOsJSf/zTOk1HWGrPrW5oOsJSB25wadMRltrp400n6G8pyhER0bcm5dzXERERpcpAr4iIiAJMtO8pT5yr4xERMSkt8QodP0YiaU9JN0m6VdLhQ+yzm6R5kq6XdHE33ktayhER0b/c/WvKkqYAXwdeAtwJzJF0pu0bWvZZF/gGsKft30t6fDfOnZZyRET0LVNdU+70MYIdgVtt32b7IeAUYN+2fd4AnG779wC2/9SN95OiHBERfW1J3Vru5DGCqcAfWpbvrNe1ehrwOEmzJV0l6d+78V7SfR0REX1rHAO9NpA0t2V5lu1Z9fPBDui25RWB7YEXA6sBv5Z0ue2bxxKm9aARERF9a4xFeaHt6UNsuxPYpGV5Y+DuQfZZaPsfwD8kXQJsC4yrKKf7OiIi+tbA5CFd7r6eA2wpaXNJKwOvB85s2+cMYFdJK0paHXgu8Jvxvp+0lCMioq91e/IQ249IOgQ4F5gCHGf7ekkH19uPtf0bST8HrgWWAN+2fd14z52iHBER0cb2OcA5beuObVv+PPD5bp43RTkiIvqXJ9aMXinKERHRtzLNZiEknSBpvxH2uV3SBh0c80BJXxtm+8EjfRdN0jRJe432nB1k+5SkPern760HFkRETHrLYaBXY9JS7kD79YQhTAOm03Ytogvnbr1L6XuB7wH/7OY5IiL6zUS7dWNftJQlfUzSjZLOl3SypA+2bX+xpGskLZB0nKRVWjZ/SNKV9eOp9f57S7qifs0vJD1hlDlmDpy7nsXlqPq4N0vatR46/ylgRj1J+QxJa9SZ5tTn27d+/YGSTpf0c0m3SPrvev2Uuhfguvr9vK9ef4Kk/SQdCjwJuEjSRZLeJulLLRnfLumLQ+Q/SNJcSXP//JfFo/z0IyLKZqvjR6mKL8qSpgOvAbYDXk3VCm3dvipwAjDD9tZUrf93tuzyd9s7Al8Dvlyvuwx4nu3tqOY0/fAY461YH/u9wCfqOVI/Dpxqe5rtU4EjgAtt7wDsDnxe0hr166cBM4CtqQr5JvW6qbafXb+f41tPaPsYqi+x72579zr/PpJWqnd5S/trWl47y/Z029M3XH/KGN9yRERZlsPc140pvigDuwBn2H7Q9v3AWW3bnw78rmVqsxOBF7RsP7nl5071842BcyUtAD4EPGuM2U6vf14FbDbEPi8FDpc0D5gNrApsWm+7wPZ9tv8F3AA8GbgNeIqkr0raE/j7cAHq2WQuBF4p6RnASrYXjPH9RET0FXtiXVPuh6I80qc30nYP8vyrwNfqlug7qArlWCyqfy5m6OvzAl5Tt5yn2d7U9sCsL4ta9ltM1fL+K9VUbbOBdwPfHkWObwMHMkwrOSJiokr3dW9dBuwtaVVJawKvaNt+I7DZwPVi4M1A682mZ7T8/HX9fB3grvr5AV3Oez+wVsvyucB7JAlA0nbDvbgeLb6C7R8BHwOeM9I5bF9BNU/rG1jWMxARMQksl2k2G1P86GvbcySdCcwH7gDmAve1bP+XpLcAp0lakWrO0tZR0qtIuoLqD5D963Uz6/3vAi4HNu9i5ItY1l19JPBpqmvZ19aF+XbglcO8fipwvKSBP5g+Osg+s4CfSbqnvq4M8ANgWt3SjoiYNEpu+Xaq+KJcO9r2zPq7uZcAX7D9PwMbbV9ANRDsUWxvVj/9ZNv6M6gmE2/f/wSqQWODsj2z5fluLc8XUl9Ttn0vsEPbS98x0rlstxbqx7SObR/Y8vyrVF3wrXYBvkRExCSSyUOaMatueV4N/Mj21Q3nKYakdSXdDDxY/3ESERF9qi9ayrbf0MvzSToCeG3b6tNsf7aXOUbD9t+ApzWdIyKiEa5GYE8UfVGUe60uvsUV4IiIeKySv3fcqRTliIjoWyYDvSIiIgpR9lecOpWiHBERfS3XlCMiIgqR7uuIiIgC2CnKERERxcg15YiIiELkmnJEREQh0n0dfe/ma1fnZVOHvWFVz0zdZ0nTEZZ62funNR1hqXPvntd0BABe+tppTUdYarXVyvnle8f16zYdYamZ/3hx0xFaHNfTs5myb8XYqX6Z+zoiImLCS0s5IiL62gS6pJyiHBERfSxfiYqIiCjIBGoqpyhHRERfS0s5IiKiEPmeckRERAEm2q0b85WoiIjoXwaszh8jkLSnpJsk3Srp8GH220HSYkn7dePtpChHRERfq25K0dljOJKmAF8HXg5sBewvaash9jsKOLdb7yVFOSIi+pvH8BjejsCttm+z/RBwCrDvIPu9B/gR8Kdxv4dainJERPSxaprNTh8jmAr8oWX5znrdsrNKU4FXAcd2891koFdERPS3sY2+3kDS3JblWbZn1c8Hq9rtZ/ky8BHbi6XuDTRLUY6IiMlooe3pQ2y7E9ikZXlj4O62faYDp9QFeQNgL0mP2P7JeEKl+7oBkjaTdF39fJqkvZrOFBHRl+ppNrvcfT0H2FLS5pJWBl4PnPmo09qb297M9mbAD4F3jbcgQ4pyCaYBKcoREWPV5YFeth8BDqEaVf0b4Ae2r5d0sKSDl8+bqKT7ukskHQXcYfsb9fJM4H7giVTD6g18xvapLa9ZGfgUsJqkXYAjgd9RXatYDXgQeIvtmyStDpwAPIPqH8lmwLttz5X0UuCTwCrAb+vXPLCc33JERCG6P3mI7XOAc9rWDTqoy/aB3TpvWsrdcwowo2X5dcBCqpbwtsAewOclbTSwQz3U/uPAqban1QX7RuAFtrert/1Xvfu7gL/a3gb4NLA9gKQNgP8E9rD9HGAu8P7BAko6SNJcSXMfZlF33nVERNO6/5WoxqSl3CW2r5H0eElPAjYE/kpVkE+2vRj4o6SLgR2Aa4c51DrAiZK2pPqns1K9fhfgK/W5rpM0cIznUX25/Zf1gIOVgV8PkXEWMAtgba1X8D/LiIgOTKDfZinK3fVDYD+qLutTgC3GcIxPAxfZfpWkzYDZ9fqh+mcEnG97/zGcKyKivw1MszlBpPu6u06hGqW3H1WBvgSYIWmKpA2BFwBXtr3mfmCtluV1gLvq5we2rL+Mqkucerq3rev1lwM7S3pqvW11SU/r1huKiChdt6fZbFKKchfZvp6qwN5l+x7gx1Rd1fOBC4EP2/6/tpddBGwlaZ6kGcB/A0dK+iUwpWW/bwAb1t3WH6mPe5/tP1MV75PrbZdTDQaLiJgcck05hmJ765bnBj5UP1r3uR14dv38XqrrzK1aW7ofq3/+C3iT7X9J2gK4ALijPsaFgxwjImJymEDd1ynK/WN14CJJK1FdR35nPXo7ImJSU8Et306lKPcJ2/dTTesWEREDCu+O7lSKckRE9DFNqO7rDPSKiIgoRFrKERHR39J9HRERUYgU5YiIiEKkKEdERBRggk2zmaIcERF9Ld9TjoiIKMUEKsr5SlREREQh0lKepBY/dRX+esxTm44BwKLzyvln+PDPxnK3zeXjpa+d1nQEAM477YSmIyy13Wfe1XSEpf76jKc0HWGp9f7tzqYjLPPi3p8y3dcRERGlyECviIiIAmTu64iIiIJMoKKcgV4RERGFSEs5IiL6WgZ6RURElCJFOSIiohApyhEREc2T030dERFRjnxPOSIiohBpKUdERJQh3dcRERGlmEBFOZOHRERE//KywV6dPEYiaU9JN0m6VdLhg2x/o6Rr68evJG3bjbeTlnJERPS3LreUJU0Bvg68BLgTmCPpTNs3tOz2O+CFtv8q6eXALOC54z13WsoREdHfPIbH8HYEbrV9m+2HgFOAfR91SvtXtv9aL14ObNyNtzKhirKkmZI+2KNz7Sbp+WN43XRJx3Q5y2slXS9piaTp3Tx2RMQkNBX4Q8vynfW6obwN+Fk3Tpzu67HbDXgA+NVoXyBpRdtzgbldznId8GrgW10+bkRE8cY4+noDSa2/i2fZnjVwyEH2H/QsknanKsq7jClFm+JbypLWkHS2pPmSrpM0Q9Ltkjaot0+XNLvlJdtKulDSLZLePsxxd5N0saQfSLpZ0ufqC/dXSlogaYt6vw0l/UjSnPqxs6TNgIOB90maJ2nXwfarXz9T0ixJ5wHfqc/705Ztx0maLek2SYe25PuYpBslnS/p5OF6AGz/xvZNo/gsD5I0V9LcR/7+z5F2j4iYyBbant7ymNWy7U5gk5bljYG72w8gaRvg28C+tv/SjVD90FLeE7jb9isAJK0DHDXM/tsAzwPWAK6RdLbtx3yYtW2BZwL3ArcB37a9o6TDgPcA7wW+AnzJ9mWSNgXOtf1MSccCD9g+us51Uvt+9bEBtgd2sf2gpN3aMjwD2B1YC7hJ0jfrXK8BtqP6b3Q1cNUIn9OI6n90swDW2HKjCfQlgoiY1Lr/22wOsKWkzYG7gNcDb2jdof49fzrwZts3d+vE/VCUFwBHSzoK+KntS6Vhp1Q7w/aDwIOSLqK6YP+TIfadY/seAEm/Bc5rOefu9fM9gK1azrm2pLUGOdZw+51ZZxrM2bYXAYsk/Ql4AlU3yMD7QNJZw73hiIhJaznMfW37EUmHUDWupgDH2b5e0sH19mOBjwPrA9+of+8/YnvcY3qKL8q2b5a0PbAXcGTdDfwIy7reV21/yQjLrRa1PF/SsryEZZ/NCsBO7UV1kD8MhtvvH6PMsLg+78SZyDUiYnlbDv1+ts8Bzmlbd2zL8/8A/qPb5+2Ha8pPAv5p+3vA0cBzgNupuoSh6uZtta+kVSWtTzUYa844I5wHHNKSZ1r99H6qLueR9huLy4C96/exJvCKcRwrImJi6/5XohpTfFEGtgaulDQPOAL4DPBJ4CuSLqVqXba6Ejib6ntjnx7mevJoHQpMr2dtuYFqgBfAWcCrBgZ6DbNfx2zPAc4E5lNds5gL3DfU/pJeJelOYCfgbEnnjvXcERH9RCyfGb2a0g/d1+dS9eu3e9og+87s4Lizgdkty7sNts32QmDGIK+/mWpQWavB9pvZttx67PZtz25ZPNr2TEmrA5cAXxjmvfwY+PFQ2yMiJrSCi2ynii/Kk9gsSVtRXTM/0fbVTQeKiChO4S3fTk34oixpa+C7basX2R73HKXLk+03tK+T9HVg57bVX7F9fG9SRUQUKEW5f9heAExrOkc32H530xkiIoozgYpyPwz0ioiImBQmfEs5IiImtlxTjoiIKEWKckRERAEKnwykUynKERHR19J9HRERUYoU5YiIiDKkpRx975FHprBw4WB3oOy9NVZpOsEyf/zTOk1HWGq11cq4Wdh2n3lX0xGWuuY/v9F0hKVK+lz+774y/l9uTIpyREREATLQKyIiogxiYt2APkU5IiL6W1rKERERZZhIA70y93VEREQh0lKOiIj+NoFayinKERHR31KUIyIiCuCJdU05RTkiIvpbinJEREQZ0lKOiIgoRYpyREREGdJSjoiIKEHmvo6IiCjIBCrKE2pGL0kzJX2wR+faTdLzx/C66ZKO6XKWz0u6UdK1kn4sad1uHj8iolSi6r7u9FGqCVWUe2w3oKOiLGlF23NtH9rlLOcDz7a9DXAz8NEuHz8iolwew6NQxRdlSWtIOlvSfEnXSZoh6XZJG9Tbp0ua3fKSbSVdKOkWSW8f5ri7SbpY0g8k3Szpc5LeKOlKSQskbVHvt6GkH0maUz92lrQZcDDwPknzJO062H7162dKmiXpPOA79Xl/2rLtOEmzJd0m6dCWfB+rW7/nSzp5uB4A2+fZfqRevBzYeCyfdUREVCTtKekmSbdKOnyQ7ZJ0TL39WknP6cZ5++Ga8p7A3bZfASBpHeCoYfbfBngesAZwjaSzbd89xL7bAs8E7gVuA75te0dJhwHvAd4LfAX4ku3LJG0KnGv7mZKOBR6wfXSd66T2/epjA2wP7GL7QUm7tWV4BrA7sBZwk6Rv1rleA2xH9d/oauCqET6nAW8FTh1sg6SDgIMApqy/7igPFxFRNrm7TV9JU4CvAy8B7gTmSDrT9g0tu70c2LJ+PBf4Zv1zXPqhKC8AjpZ0FPBT25dKw97S+gzbDwIPSroI2BH4yRD7zrF9D4Ck3wLntZxz9/r5HsBWLedcW9JagxxruP3OrDMN5mzbi4BFkv4EPAHYpeV9IOms4d7wAElHAI8A3x9su+1ZwCyAVTbfuOAOnIiIUVo+3dE7Arfavg1A0inAvkBrUd4X+I5tA5dLWlfSRgM1ZayKL8q2b5a0PbAXcGTdDfwIy7reV21/yQjLrRa1PF/SsryEZZ/NCsBO7UV1kD8MhtvvH6PMsLg+77B/dQxG0gHAK4EX1/9IIiImhTEO3NpA0tyW5Vl1wwVgKvCHlm138thW8GD7TAXGVZT74Zryk4B/2v4ecDTwHOB2qi5hqLp5W+0raVVJ61MNxpozzgjnAYe05JlWP72fqst5pP3G4jJg7/p9rAm8YridJe0JfATYx/Y/x3HeiIj+M7aBXgttT295zGo54mANo/bSP5p9OlZ8SxnYGvi8pCXAw8A7gdWA/5X0/4Ar2va/Ejgb2BT49DDXk0frUODrkq6l+rwuoRrkdRbwQ0n7Ul1/Hmq/jtmeI+lMYD5wBzAXuG+Yl3wNWAU4v26ZX257TOeOiOg3y+ErTncCm7Qsbwy015LR7NOx4ouy7XOpBk21e9og+87s4Lizgdkty7sNts32QmDGIK+/mWpQWavB9pvZttx67PZtz25ZPNr2TEmrUxX4LwzzXp461LaIiAmv+0V5DrClpM2Bu4DXA29o2+dM4JD6evNzgfvGez0Z+qAoT2KzJG1Fdc38RNtXNx0oIqI4y2EyENuPSDqEqkE4BTjO9vWSDq63HwucQzXW6Vbgn8BbunHuCV+UJW0NfLdt9SLb4x66vjzZbv+rDElfB3ZuW/0V28f3JlVERIGWw9BW2+dQFd7Wdce2PDfw7m6fd8IXZdsLgGlN5+gG213/BxAR0c8GptmcKCZ8UY6IiAluAn0LNEU5IiL62kRqKRf/PeWIiIjJIi3liIjoX4Xf9alTKcoREdHXtKTpBN2TohwREf0tLeWIiIgyTKSBXinKERHRv0y+EhX9b53VHuSVz1rQdAwALrhxh6YjLLXnVjeMvFOP3HH9uk1HAOCvz3hK0xGW2u4z72o6wlLX/Oc3mo6w1LF/m9p0hKWamOEoLeWIiIhSpChHREQ0L9NsRkRElMLONeWIiIhSpKUcERFRihTliIiIMkyklnJuSBEREVGItJQjIqJ/GVgycZrKKcoREdHfJk5NTlGOiIj+NpGuKacoR0REf8v3lCMiIsqQlnJEREQJTK4pR0RElKCa+3riVOUJ9T1lSTMlfbBH59pN0vPH8Lrpko7pcpZPS7pW0jxJ50l6UjePHxFRtCVjeBRqQhXlHtsN6KgoS1rR9lzbh3Y5y+dtb2N7GvBT4ONdPn5ERLFkd/woVfFFWdIaks6WNF/SdZJmSLpd0gb19umSZre8ZFtJF0q6RdLbhznubpIulvQDSTdL+pykN0q6UtICSVvU+20o6UeS5tSPnSVtBhwMvK9une462H7162dKmiXpPOA79Xl/2rLtOEmzJd0m6dCWfB+TdKOk8yWdPFwPgO2/tyyuwYS6whIRMQyP8VGofrimvCdwt+1XAEhaBzhqmP23AZ5HVZyukXS27buH2Hdb4JnAvcBtwLdt7yjpMOA9wHuBrwBfsn2ZpE2Bc20/U9KxwAO2j65zndS+X31sgO2BXWw/KGm3tgzPAHYH1gJukvTNOtdrgO2o/htdDVw13Ick6bPAvwP31ccbbJ+DgIMA1nzi6sMdLiKiT0ysWzcW31IGFgB7SDpK0q627xth/zNsP2h7IXARsOMw+86xfY/tRcBvgfNazrlZ/XwP4GuS5gFnAmtLWmuQYw2335m2Hxwiw9m2F9V5/wQ8Adil5X3cD5w1wnvG9hG2NwG+DxwyxD6zbE+3PX21x6060iEjIqLHim8p275Z0vbAXsCRdTfwIyz7g6K9urT/yTTcn1CLWp4vaVlewrLPZgVgp/aiKqn9WMPt949RZlhcn/cxB+/AScDZwCfGcYyIiL4xkb6nXHxLuR5J/E/b3wOOBp4D3E7VJQxVN2+rfSWtKml9qsFYc8YZ4TxaWp6SptVP76fqch5pv7G4DNi7fh9rAq8YbmdJW7Ys7gPcOI5zR0T0F7vzxzhIWq8e73NL/fNxg+yziaSLJP1G0vX1ZdERFV+Uga2BK+tu4SOAzwCfBL4i6VKq1mWrK6laipcDnx7mevJoHQpMr79ydAPVAC+oupRfNTDQa5j9OmZ7DlUX+HzgdGAu1bXioXyuHgR3LfBSYFT/8SMi+p5BSzp/jNPhwAW2twQuqJfbPQJ8wPYzqcY5vVvSViMduB+6r8+lGjTV7mmD7Duzg+POBma3LO822Lb6Wu+MQV5/M9WgslaD7Tezbbn12O3bnt2yeLTtmZJWBy4BvjDMe2nvLYiImDx6P9BrX6qeWIATqX6nf6R1B9v3APfUz++X9BtgKnDDcAcuvihPYrPqv6pWBU60fXXTgSIiijS2mryBpLkty7Nszxrla59QF11s3yPp8cPtXH+NdjvgipEOPOGLsqStge+2rV5k+7lN5Bkt229oXyfp68DObau/Yvv43qSKiCjPGCcDWWh7+pDHlH4BPHGQTUd0cpJ6XNCPgPe2zSkxqAlflG0vAKY1naMbbL+76QwREcVZDt3XtvcYapukP0raqG4lb0T1ddbB9luJqiB/3/bpozlvPwz0ioiIGJxpYu7rM4ED6ucHAGe076Dq+7D/C/zG9hdHe+AU5YiI6Fui83mvuzD39eeAl0i6BXhJvYykJ0k6p95nZ+DNwIvqb+nMk7TXSAee8N3XERExwfV49LXtvwAvHmT93VQTXWH7MsYwEVSKckRE9LfMfR0RERHdlpZyRET0r4GBXhNEinJERPS1LgzcKkaKckRE9LcU5eh366/4AG9a/1dNxwDgvFV3aDrCUgducGnTEZaa+Y/HDO5sxHr/dmfTEZb6v/sGu5V5M47929SmIyx18Lp3NR1hqd7PcDT+uz6VJEU5IiL6l0lRjoiIKEYGekVERJQhA70iIiJKkaIcERFRAANLUpQjIiIKkNHXERER5ZhARTlzX0dERBQiLeWIiOhvE6ilnKIcERH9KwO9IiIiSmHwxJk9JEU5IiL6W7qvIyIiCjDBuq8z+noCkTRN0l5N54iI6Cm780ehUpQnlmlAinJETC4pyrG8SPp3SddKmi/pu5KeLOmCet0Fkjat93utpOvq/S6RtDLwKWCGpHmSZjT7TiIiemEMBbngopxrygWR9CzgCGBn2wslrQecCHzH9omS3gocA/wb8HHgZbbvkrSu7YckfRyYbvuQpt5DRERPGVgycUZfp6VclhcBP7S9EMD2vcBOwEn19u8Cu9TPfwmcIOntwJTRHFzSQZLmSpr7t3sXdzd5RERTJlBLOUW5LKL6u284BrB9MPCfwCbAPEnrj3Rw27NsT7c9fd31RlXHIyLKl6Icy8kFwOsGCmzdff0r4PX19jcCl9XbtrB9he2PAwupivP9wFo9Tx0REV2Ra8oFsX29pM8CF0taDFwDHAocJ+lDwJ+Bt9S7f17SllSt6wuA+cDvgcMlzQOOtH1qr99DRERveUJ9TzlFuTC2T6Qa3NXqRYPs9+pBXn4vsMPyyBURUSSDM81mREREISZQSznXlCMior/1eKCXpPUknS/plvrn44bZd4qkayT9dDTHTlGOiIj+ZVffU+70MT6HAxfY3pJqTM/hw+x7GPCb0R44RTkiIvpb778StS/Lxv6cSDWh02NI2hh4BfDt0R4415QjIqKvufczej3B9j0Atu+R9Pgh9vsy8GE6+KpqinJERPSxMbd8N5A0t2V5lu1ZAwuSfgE8cZDXHTGag0t6JfAn21dJ2m20oVKUIyKif439fsoLbU8f8rD2HkNtk/RHSRvVreSNgD8NstvOwD717XRXBdaW9D3bbxouVK4pR0REf/OSzh/jcyZwQP38AOCMx0SyP2p7Y9ubUc3KeOFIBRlSlCMioo8Z8BJ3/BinzwEvkXQL8JJ6GUlPknTOeA6c7uuIiIgO2P4L8OJB1t8N7DXI+tnA7NEcO0U5IiL6l92N7uhipChHRERf60J3dDHkgu8rGcuPpD8Dd4zzMBtQ3TayBMkyuGQZXClZSskB3cvyZNsbduE4oyLp51TZO7XQ9p7dzjNeKcoxZpLmDveVgl5KlsEly+BKyVJKDigry2SW0dcRERGFSFGOiIgoRIpyjMeskXfpmWQZXLIMrpQspeSAsrJMWrmmHBERUYi0lCMiIgqRohwREVGIFOWIiIhCpChHREQUIkU5IiKiEP8fSaZpMEMLaRkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "cor_cols = [\"global_active_power\",\"global_reactive_power\", \"global_intensity\",\"voltage\",\"sub_metering_1\",\"sub_metering_2\",\"sub_metering_3\",\"cost\"]\n",
    "\n",
    "\n",
    "# plot correlation matrix\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.matshow(dataset.loc[:, cor_cols].corr(), fignum=1)\n",
    "plt.xticks(range(len(cor_cols)), cor_cols, rotation=90)\n",
    "plt.yticks(range(len(cor_cols)), cor_cols)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Correlation Plot</b>\n",
    "<p>The correlation plot, we can observe that Sub Meters 1 and 2 show some correlation with each other. Sub meter 3 has less correlation with Sub meter 1 and 2. Since cost is derived by aggregating Sub Meter 1, 2 and 3, we will retain only cost column to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##updating data to dataset\n",
    "analysis_cols = [\"global_active_power\",\"global_reactive_power\", \"global_intensity\",\"voltage\",\"cost\"]\n",
    "\n",
    "# select the value columns in the DataFrame to compare\n",
    "dataset= dataset.loc[:, analysis_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load and parse the dataset and convert it to a collection of Pandas time series, which makes common time series operations such as indexing by time periods or resampling much easier. Here we want to forecast longer periods (one week) and resample the data to a granularity of every hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timeseries = dataset.shape[1]\n",
    "data_kw = dataset.resample('2H').sum()\n",
    "timeseries = []\n",
    "for i in range(num_timeseries):\n",
    "    timeseries.append(np.trim_zeros(data_kw.iloc[:,i], trim='f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test splits\n",
    "\n",
    "Often times one is interested in evaluating the model or tuning its hyperparameters by looking at error metrics on a hold-out test set. Here we split the available data into train and test sets for evaluating the trained model. For standard machine learning tasks such as classification and regression, one typically obtains this split by randomly separating examples into train and test sets. However, in forecasting it is important to do this train/test split based on time rather than by time series.\n",
    "\n",
    "In this example, we will reserve the last section of each of the time series for evalutation purpose and use only the first part as training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use 1 (changed 2 to 1) hour frequency for the time series\n",
    "freq = '2H' \n",
    "\n",
    "# we predict for 7 days\n",
    "prediction_length = 7 * 12  ##original 7*12\n",
    "\n",
    "# we also use 7 days as context length, this is the number of state updates accomplished before making predictions\n",
    "context_length = 7 * 12 ##original 7*12 due to 2H sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we assume that the minimum date is the first day sensors started recording the dataset. We split the dataset 70, 30 with 70% of the data retained for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>global_active_power</th>\n",
       "      <th>global_reactive_power</th>\n",
       "      <th>global_intensity</th>\n",
       "      <th>voltage</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-05-30</th>\n",
       "      <td>8.26</td>\n",
       "      <td>0.83</td>\n",
       "      <td>34.8</td>\n",
       "      <td>1446.99</td>\n",
       "      <td>103.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>42.40</td>\n",
       "      <td>4.73</td>\n",
       "      <td>179.8</td>\n",
       "      <td>11091.73</td>\n",
       "      <td>607.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-01</th>\n",
       "      <td>63.75</td>\n",
       "      <td>4.59</td>\n",
       "      <td>269.8</td>\n",
       "      <td>17089.58</td>\n",
       "      <td>975.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-02</th>\n",
       "      <td>35.31</td>\n",
       "      <td>4.13</td>\n",
       "      <td>150.8</td>\n",
       "      <td>12753.60</td>\n",
       "      <td>513.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-03</th>\n",
       "      <td>64.33</td>\n",
       "      <td>8.01</td>\n",
       "      <td>277.8</td>\n",
       "      <td>15117.75</td>\n",
       "      <td>1069.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-21</th>\n",
       "      <td>32.85</td>\n",
       "      <td>4.52</td>\n",
       "      <td>138.4</td>\n",
       "      <td>13795.45</td>\n",
       "      <td>262.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-22</th>\n",
       "      <td>79.28</td>\n",
       "      <td>4.30</td>\n",
       "      <td>336.0</td>\n",
       "      <td>13942.78</td>\n",
       "      <td>1036.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-23</th>\n",
       "      <td>61.35</td>\n",
       "      <td>5.11</td>\n",
       "      <td>263.6</td>\n",
       "      <td>14487.99</td>\n",
       "      <td>619.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-24</th>\n",
       "      <td>54.30</td>\n",
       "      <td>4.70</td>\n",
       "      <td>228.0</td>\n",
       "      <td>12511.39</td>\n",
       "      <td>712.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-25</th>\n",
       "      <td>48.81</td>\n",
       "      <td>4.66</td>\n",
       "      <td>204.2</td>\n",
       "      <td>12809.79</td>\n",
       "      <td>235.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            global_active_power  global_reactive_power  global_intensity  \\\n",
       "timestamp                                                                  \n",
       "2018-05-30                 8.26                   0.83              34.8   \n",
       "2018-05-31                42.40                   4.73             179.8   \n",
       "2018-06-01                63.75                   4.59             269.8   \n",
       "2018-06-02                35.31                   4.13             150.8   \n",
       "2018-06-03                64.33                   8.01             277.8   \n",
       "...                         ...                    ...               ...   \n",
       "2018-11-21                32.85                   4.52             138.4   \n",
       "2018-11-22                79.28                   4.30             336.0   \n",
       "2018-11-23                61.35                   5.11             263.6   \n",
       "2018-11-24                54.30                   4.70             228.0   \n",
       "2018-11-25                48.81                   4.66             204.2   \n",
       "\n",
       "             voltage    cost  \n",
       "timestamp                     \n",
       "2018-05-30   1446.99   103.5  \n",
       "2018-05-31  11091.73   607.5  \n",
       "2018-06-01  17089.58   975.0  \n",
       "2018-06-02  12753.60   513.0  \n",
       "2018-06-03  15117.75  1069.5  \n",
       "...              ...     ...  \n",
       "2018-11-21  13795.45   262.5  \n",
       "2018-11-22  13942.78  1036.5  \n",
       "2018-11-23  14487.99   619.5  \n",
       "2018-11-24  12511.39   712.5  \n",
       "2018-11-25  12809.79   235.5  \n",
       "\n",
       "[180 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dailyGroups = dataset.resample('D').sum() ## Gathering unique days for dataset split\n",
    "dailyGroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Confirm the datasets\n",
      "startTrainDate, maxDate, traingSplit, splitDate 2018-05-30 00:00:00 2018-11-25 00:00:00 126 2018-10-03 00:00:00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "startTrainDate = dailyGroups.index.min()\n",
    "\n",
    "maxDate=dailyGroups.index.max()\n",
    "\n",
    "\n",
    "traingSplit=round(dailyGroups.shape[0]*.7)\n",
    "\n",
    "\n",
    "splitDate=dailyGroups.index[traingSplit]\n",
    "\n",
    "\n",
    "start_dataset = pd.Timestamp(startTrainDate, freq=freq)\n",
    "end_training = pd.Timestamp(splitDate, freq=freq)\n",
    "\n",
    "print(\"###### Confirm the datasets\")\n",
    "print(\"startTrainDate, maxDate, traingSplit, splitDate\", startTrainDate, maxDate, traingSplit, splitDate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DeepAR JSON input format represents each time series as a JSON object. In the simplest case each time series just consists of a start time stamp (``start``) and a list of values (``target``). For more complex cases, DeepAR also supports the fields ``dynamic_feat`` for time-series features and ``cat`` for categorical features, which we will use  later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training_data = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": ts[start_dataset:end_training - pd.Timedelta(1, unit='D')].tolist()\n",
    "         # We use -1 days, because pandas indexing includes the upper bound \n",
    "    }\n",
    "    for ts in timeseries\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As test data, we will consider time series extending beyond the training range: these will be used for computing test scores, by using the trained model to forecast their trailing 7 days, and comparing predictions with actual values.\n",
    "To evaluate our model performance on more than one week, we generate test data that extends to 1, 2, 3, 4 weeks beyond the training range. This way we perform *rolling evaluation* of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_windows = 4\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": ts[start_dataset:end_training + pd.Timedelta(k, unit='D') * prediction_length].tolist()\n",
    "    }\n",
    "    for k in range(1, num_test_windows + 1) \n",
    "    for ts in timeseries\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now write the dictionary to the `jsonlines` file format that DeepAR understands (it also supports gzipped jsonlines and parquet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, 'wb') as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.2 ms, sys: 0 ns, total: 25.2 ms\n",
      "Wall time: 24.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_dicts_to_file(\"train.json\", training_data)\n",
    "write_dicts_to_file(\"test.json\", test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data files locally, let us copy them to S3 where DeepAR can access them. Depending on your connection, this may take a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "def copy_to_s3(local_file, s3_path, override=True):\n",
    "    assert s3_path.startswith('s3://')\n",
    "    split = s3_path.split('/')\n",
    "    bucket = split[2]\n",
    "    path = '/'.join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "    \n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print('File s3://{}/{} already exists.\\nSet override to upload anyway.\\n'.format(s3_bucket, s3_path))\n",
    "            return\n",
    "        else:\n",
    "            print('Overwriting existing file')\n",
    "    with open(local_file, 'rb') as data:\n",
    "        print('Uploading file to {}'.format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file to s3://sagemaker-us-east-1-710299592439/iot-analytics-demo-notebook/output/train/train.json\n",
      "Uploading file to s3://sagemaker-us-east-1-710299592439/iot-analytics-demo-notebook/output/test/test.json\n",
      "CPU times: user 36.5 ms, sys: 9.37 ms, total: 45.9 ms\n",
      "Wall time: 267 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "copy_to_s3(\"train.json\", s3_output_path + \"/train/train.json\")\n",
    "copy_to_s3(\"test.json\", s3_output_path + \"/test/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look to what we just wrote to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"start\": \"2018-05-30 00:00:00\", \"target\": [7.15, 1.1099999999999999, 1.32, 0.42000000000000004, 0.9...\n"
     ]
    }
   ],
   "source": [
    "s3filesystem = s3fs.S3FileSystem()\n",
    "with s3filesystem.open(s3_output_path + \"/train/train.json\", 'rb') as fp:\n",
    "    print(fp.readline().decode(\"utf-8\")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are all set with our dataset processing, we can now call DeepAR to train a model and generate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model\n",
    "\n",
    "Here we define the estimator that will launch the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_uri=image_uri,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='deepar-electricity-demo',\n",
    "    output_path=s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to set the hyperparameters for the training job. For example frequency of the time series used, number of data points the model will look at in the past, number of predicted data points. The other hyperparameters concern the model to train (number of layers, number of cells per layer, likelihood function) and the training options (number of epochs, batch size, learning rate...). We use default parameters for every optional parameter in this case (you can always use [Sagemaker Automated Model Tuning](https://aws.amazon.com/blogs/aws/sagemaker-automatic-model-tuning/) to tune them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"40\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"1E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to launch the training job. SageMaker will start an EC2 instance, download the data from S3, start training the model and save the trained model.\n",
    "\n",
    "If you provide the `test` data channel as we do in this example, DeepAR will also calculate accuracy metrics for the trained model on this test. This is done by predicting the last `prediction_length` points of each time-series in the test set and comparing this to the actual value of the time-series. \n",
    "\n",
    "**Note:** the next cell may take a few minutes to complete, depending on data size, model complexity, training options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-21 13:40:51 Starting - Starting the training job...\n",
      "2021-03-21 13:41:14 Starting - Launching requested ML instancesProfilerReport-1616334050: InProgress\n",
      "......\n",
      "2021-03-21 13:42:15 Starting - Preparing the instances for training...\n",
      "2021-03-21 13:42:50 Downloading - Downloading input data...\n",
      "2021-03-21 13:43:15 Training - Downloading the training image...\n",
      "2021-03-21 13:43:42 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:45 INFO 140029106595200] Reading default configuration from /opt/amazon/lib/python3.6/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:45 INFO 140029106595200] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'prediction_length': '84', 'time_freq': '2H', 'context_length': '84', 'epochs': '40', 'learning_rate': '1E-4', 'early_stopping_patience': '40', 'mini_batch_size': '64'}\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:45 INFO 140029106595200] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '40', 'embedding_dimension': '10', 'learning_rate': '1E-4', 'likelihood': 'student-t', 'mini_batch_size': '64', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'prediction_length': '84', 'time_freq': '2H', 'context_length': '84', 'epochs': '40'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:45 INFO 140029106595200] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:46 INFO 140029106595200] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:46 INFO 140029106595200] random_seed is None\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:46 INFO 140029106595200] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:46 INFO 140029106595200] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:46 INFO 140029106595200] Training set statistics:\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:46 INFO 140029106595200] Real time series\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:46 INFO 140029106595200] number of time series: 5\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:46 INFO 140029106595200] number of observations: 7455\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:46 INFO 140029106595200] mean target length: 1491.0\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:46 INFO 140029106595200] min/mean/max target: 0.0/230.08847036483223/3136.090087890625\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:46 INFO 140029106595200] mean abs(target): 230.08847036483223\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:46 INFO 140029106595200] contains missing values: no\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:46 INFO 140029106595200] Small number of time series. Doing 128 passes over dataset with prob 1.0 per epoch.\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:46 INFO 140029106595200] Test set statistics:\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:46 INFO 140029106595200] Real time series\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:46 INFO 140029106595200] number of time series: 20\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:46 INFO 140029106595200] number of observations: 43000\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:46 INFO 140029106595200] mean target length: 2150.0\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:46 INFO 140029106595200] min/mean/max target: 0.0/240.3857703488372/3136.090087890625\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:46 INFO 140029106595200] mean abs(target): 240.3857703488372\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:46 INFO 140029106595200] contains missing values: no\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:46 INFO 140029106595200] #memory_usage::<batchbuffer> = 20.827598571777344 mb\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:46 INFO 140029106595200] nvidia-smi took: 0.025310516357421875 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:46 INFO 140029106595200] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:46 INFO 140029106595200] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334226.0843868, \"EndTime\": 1616334226.7001154, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 613.5210990905762, \"count\": 1, \"min\": 613.5210990905762, \"max\": 613.5210990905762}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:46 INFO 140029106595200] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:47 INFO 140029106595200] #memory_usage::<model> = 55 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334226.7002366, \"EndTime\": 1616334227.615779, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 1531.2340259552002, \"count\": 1, \"min\": 1531.2340259552002, \"max\": 1531.2340259552002}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:48 INFO 140029106595200] Epoch[0] Batch[0] avg_epoch_loss=5.020197\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:48 INFO 140029106595200] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=5.02019739151001\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:49 INFO 140029106595200] Epoch[0] Batch[5] avg_epoch_loss=4.743582\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:49 INFO 140029106595200] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=4.743582407633464\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:49 INFO 140029106595200] Epoch[0] Batch [5]#011Speed: 313.04 samples/sec#011loss=4.743582\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.6/lib/python3.6/contextlib.py:99: DeprecationWarning: generator 'local_timer' raised StopIteration\n",
      "  self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:50 INFO 140029106595200] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334227.6158679, \"EndTime\": 1616334230.1654546, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 40.0, \"count\": 1, \"min\": 40, \"max\": 40}, \"update.time\": {\"sum\": 2549.4885444641113, \"count\": 1, \"min\": 2549.4885444641113, \"max\": 2549.4885444641113}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:50 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=250.6214203605946 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:50 INFO 140029106595200] #progress_metric: host=algo-1, completed 2.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:50 INFO 140029106595200] #quality_metric: host=algo-1, epoch=0, train loss <loss>=4.60196213722229\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:50 INFO 140029106595200] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:50 INFO 140029106595200] Saved checkpoint to \"/opt/ml/model/state_ba554a6a-d2aa-4b2b-a720-77430f49a101-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334230.1655774, \"EndTime\": 1616334230.2341201, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 67.89398193359375, \"count\": 1, \"min\": 67.89398193359375, \"max\": 67.89398193359375}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:50 INFO 140029106595200] Epoch[1] Batch[0] avg_epoch_loss=4.684524\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:50 INFO 140029106595200] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=4.684523582458496\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:51 INFO 140029106595200] Epoch[1] Batch[5] avg_epoch_loss=4.410304\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:51 INFO 140029106595200] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=4.410304069519043\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:51 INFO 140029106595200] Epoch[1] Batch [5]#011Speed: 261.97 samples/sec#011loss=4.410304\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:53 INFO 140029106595200] Epoch[1] Batch[10] avg_epoch_loss=4.416176\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:53 INFO 140029106595200] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=4.423223161697388\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:53 INFO 140029106595200] Epoch[1] Batch [10]#011Speed: 255.95 samples/sec#011loss=4.423223\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:53 INFO 140029106595200] processed a total of 691 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334230.2342172, \"EndTime\": 1616334233.2191167, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2984.811544418335, \"count\": 1, \"min\": 2984.811544418335, \"max\": 2984.811544418335}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:53 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=231.49410510600075 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:53 INFO 140029106595200] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:53 INFO 140029106595200] #quality_metric: host=algo-1, epoch=1, train loss <loss>=4.4161763841455635\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:53 INFO 140029106595200] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:53 INFO 140029106595200] Saved checkpoint to \"/opt/ml/model/state_60e113df-54f5-4227-82cf-d2057fbd9b31-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334233.219218, \"EndTime\": 1616334233.2950528, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 75.28901100158691, \"count\": 1, \"min\": 75.28901100158691, \"max\": 75.28901100158691}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:53 INFO 140029106595200] Epoch[2] Batch[0] avg_epoch_loss=4.322834\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:53 INFO 140029106595200] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=4.32283353805542\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:54 INFO 140029106595200] Epoch[2] Batch[5] avg_epoch_loss=3.909598\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:54 INFO 140029106595200] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=3.909598469734192\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:54 INFO 140029106595200] Epoch[2] Batch [5]#011Speed: 315.24 samples/sec#011loss=3.909598\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:55 INFO 140029106595200] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334233.2951472, \"EndTime\": 1616334235.7561738, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2460.932493209839, \"count\": 1, \"min\": 2460.932493209839, \"max\": 2460.932493209839}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:55 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=242.57447207346289 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:55 INFO 140029106595200] #progress_metric: host=algo-1, completed 7.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:55 INFO 140029106595200] #quality_metric: host=algo-1, epoch=2, train loss <loss>=3.8987839460372924\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:55 INFO 140029106595200] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:55 INFO 140029106595200] Saved checkpoint to \"/opt/ml/model/state_e0bbe680-5683-42a3-b5d2-60c4bc31e43c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334235.7562842, \"EndTime\": 1616334235.8446229, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 87.74614334106445, \"count\": 1, \"min\": 87.74614334106445, \"max\": 87.74614334106445}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:56 INFO 140029106595200] Epoch[3] Batch[0] avg_epoch_loss=3.960658\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:56 INFO 140029106595200] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=3.960658073425293\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:57 INFO 140029106595200] Epoch[3] Batch[5] avg_epoch_loss=3.869241\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:57 INFO 140029106595200] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=3.8692413171132407\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:57 INFO 140029106595200] Epoch[3] Batch [5]#011Speed: 307.82 samples/sec#011loss=3.869241\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:58 INFO 140029106595200] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334235.8447077, \"EndTime\": 1616334238.2436874, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2398.8871574401855, \"count\": 1, \"min\": 2398.8871574401855, \"max\": 2398.8871574401855}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:58 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=254.26180586397322 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:58 INFO 140029106595200] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:58 INFO 140029106595200] #quality_metric: host=algo-1, epoch=3, train loss <loss>=3.9711297512054444\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:58 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:58 INFO 140029106595200] Epoch[4] Batch[0] avg_epoch_loss=4.293914\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:58 INFO 140029106595200] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=4.293913841247559\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:59 INFO 140029106595200] Epoch[4] Batch[5] avg_epoch_loss=4.181584\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:59 INFO 140029106595200] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=4.1815842390060425\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:43:59 INFO 140029106595200] Epoch[4] Batch [5]#011Speed: 314.30 samples/sec#011loss=4.181584\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:00 INFO 140029106595200] Epoch[4] Batch[10] avg_epoch_loss=4.270471\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:00 INFO 140029106595200] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=4.377135610580444\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:00 INFO 140029106595200] Epoch[4] Batch [10]#011Speed: 298.79 samples/sec#011loss=4.377136\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:00 INFO 140029106595200] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334238.2438533, \"EndTime\": 1616334240.8506083, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2606.168031692505, \"count\": 1, \"min\": 2606.168031692505, \"max\": 2606.168031692505}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:00 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=245.94136557044462 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:00 INFO 140029106595200] #progress_metric: host=algo-1, completed 12.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:00 INFO 140029106595200] #quality_metric: host=algo-1, epoch=4, train loss <loss>=4.270471226085316\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:00 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:01 INFO 140029106595200] Epoch[5] Batch[0] avg_epoch_loss=4.303827\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:01 INFO 140029106595200] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=4.303827285766602\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:02 INFO 140029106595200] Epoch[5] Batch[5] avg_epoch_loss=4.055635\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:02 INFO 140029106595200] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=4.0556349356969195\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:02 INFO 140029106595200] Epoch[5] Batch [5]#011Speed: 311.72 samples/sec#011loss=4.055635\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:03 INFO 140029106595200] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334240.8507073, \"EndTime\": 1616334243.3818932, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2530.6644439697266, \"count\": 1, \"min\": 2530.6644439697266, \"max\": 2530.6644439697266}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:03 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=241.818480091229 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:03 INFO 140029106595200] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:03 INFO 140029106595200] #quality_metric: host=algo-1, epoch=5, train loss <loss>=4.018557095527649\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:03 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:04 INFO 140029106595200] Epoch[6] Batch[0] avg_epoch_loss=3.829182\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:04 INFO 140029106595200] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=3.8291823863983154\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:05 INFO 140029106595200] Epoch[6] Batch[5] avg_epoch_loss=4.078047\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:05 INFO 140029106595200] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=4.078047474225362\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:05 INFO 140029106595200] Epoch[6] Batch [5]#011Speed: 277.34 samples/sec#011loss=4.078047\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:06 INFO 140029106595200] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334243.3820047, \"EndTime\": 1616334246.1841002, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2801.3150691986084, \"count\": 1, \"min\": 2801.3150691986084, \"max\": 2801.3150691986084}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:06 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=215.9602529238695 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:06 INFO 140029106595200] #progress_metric: host=algo-1, completed 17.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:06 INFO 140029106595200] #quality_metric: host=algo-1, epoch=6, train loss <loss>=3.9229601621627808\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:06 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:06 INFO 140029106595200] Epoch[7] Batch[0] avg_epoch_loss=4.242205\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:06 INFO 140029106595200] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=4.2422051429748535\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:07 INFO 140029106595200] Epoch[7] Batch[5] avg_epoch_loss=4.162435\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:07 INFO 140029106595200] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=4.162435213724772\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:07 INFO 140029106595200] Epoch[7] Batch [5]#011Speed: 312.95 samples/sec#011loss=4.162435\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:08 INFO 140029106595200] Epoch[7] Batch[10] avg_epoch_loss=4.059008\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:08 INFO 140029106595200] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=3.934895420074463\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:08 INFO 140029106595200] Epoch[7] Batch [10]#011Speed: 301.48 samples/sec#011loss=3.934895\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:08 INFO 140029106595200] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334246.1841815, \"EndTime\": 1616334248.863143, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2678.3502101898193, \"count\": 1, \"min\": 2678.3502101898193, \"max\": 2678.3502101898193}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:08 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=243.41883355798382 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:08 INFO 140029106595200] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:08 INFO 140029106595200] #quality_metric: host=algo-1, epoch=7, train loss <loss>=4.0590080347928135\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:08 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:09 INFO 140029106595200] Epoch[8] Batch[0] avg_epoch_loss=3.877455\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:09 INFO 140029106595200] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=3.877455234527588\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:10 INFO 140029106595200] Epoch[8] Batch[5] avg_epoch_loss=4.146388\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:10 INFO 140029106595200] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=4.146387934684753\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:10 INFO 140029106595200] Epoch[8] Batch [5]#011Speed: 314.18 samples/sec#011loss=4.146388\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:11 INFO 140029106595200] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334248.8632536, \"EndTime\": 1616334251.2013416, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2337.5518321990967, \"count\": 1, \"min\": 2337.5518321990967, \"max\": 2337.5518321990967}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:11 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=260.08410076973627 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:11 INFO 140029106595200] #progress_metric: host=algo-1, completed 22.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:11 INFO 140029106595200] #quality_metric: host=algo-1, epoch=8, train loss <loss>=4.302655410766602\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:11 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:11 INFO 140029106595200] Epoch[9] Batch[0] avg_epoch_loss=3.309226\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:11 INFO 140029106595200] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=3.3092260360717773\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:12 INFO 140029106595200] Epoch[9] Batch[5] avg_epoch_loss=3.760742\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:12 INFO 140029106595200] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=3.760742465655009\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:12 INFO 140029106595200] Epoch[9] Batch [5]#011Speed: 310.64 samples/sec#011loss=3.760742\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:13 INFO 140029106595200] Epoch[9] Batch[10] avg_epoch_loss=3.779813\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:13 INFO 140029106595200] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=3.8026973724365236\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:13 INFO 140029106595200] Epoch[9] Batch [10]#011Speed: 301.27 samples/sec#011loss=3.802697\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:13 INFO 140029106595200] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334251.2014503, \"EndTime\": 1616334253.8226001, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2620.595932006836, \"count\": 1, \"min\": 2620.595932006836, \"max\": 2620.595932006836}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:13 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=260.9950874149031 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:13 INFO 140029106595200] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:13 INFO 140029106595200] #quality_metric: host=algo-1, epoch=9, train loss <loss>=3.779812877828425\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:13 INFO 140029106595200] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:13 INFO 140029106595200] Saved checkpoint to \"/opt/ml/model/state_a5754695-9eec-4745-9748-a8eec249f54a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334253.8226995, \"EndTime\": 1616334253.88643, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 63.16065788269043, \"count\": 1, \"min\": 63.16065788269043, \"max\": 63.16065788269043}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:14 INFO 140029106595200] Epoch[10] Batch[0] avg_epoch_loss=3.843411\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:14 INFO 140029106595200] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=3.8434109687805176\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:15 INFO 140029106595200] Epoch[10] Batch[5] avg_epoch_loss=4.017089\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:15 INFO 140029106595200] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=4.017089366912842\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:15 INFO 140029106595200] Epoch[10] Batch [5]#011Speed: 317.30 samples/sec#011loss=4.017089\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:16 INFO 140029106595200] Epoch[10] Batch[10] avg_epoch_loss=4.237006\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:16 INFO 140029106595200] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=4.500905752182007\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:16 INFO 140029106595200] Epoch[10] Batch [10]#011Speed: 306.04 samples/sec#011loss=4.500906\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:16 INFO 140029106595200] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334253.8865159, \"EndTime\": 1616334256.4571161, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2570.521831512451, \"count\": 1, \"min\": 2570.521831512451, \"max\": 2570.521831512451}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:16 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=249.3491964210856 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:16 INFO 140029106595200] #progress_metric: host=algo-1, completed 27.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:16 INFO 140029106595200] #quality_metric: host=algo-1, epoch=10, train loss <loss>=4.237005905671553\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:16 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:16 INFO 140029106595200] Epoch[11] Batch[0] avg_epoch_loss=4.142846\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:16 INFO 140029106595200] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=4.14284610748291\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:17 INFO 140029106595200] Epoch[11] Batch[5] avg_epoch_loss=4.029554\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:17 INFO 140029106595200] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=4.029554128646851\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:17 INFO 140029106595200] Epoch[11] Batch [5]#011Speed: 314.59 samples/sec#011loss=4.029554\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:19 INFO 140029106595200] Epoch[11] Batch[10] avg_epoch_loss=3.901047\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:19 INFO 140029106595200] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=3.7468385219573976\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:19 INFO 140029106595200] Epoch[11] Batch [10]#011Speed: 310.01 samples/sec#011loss=3.746839\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:19 INFO 140029106595200] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334256.4572375, \"EndTime\": 1616334259.009135, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2551.283597946167, \"count\": 1, \"min\": 2551.283597946167, \"max\": 2551.283597946167}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:19 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=256.3253604272658 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:19 INFO 140029106595200] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:19 INFO 140029106595200] #quality_metric: host=algo-1, epoch=11, train loss <loss>=3.901047034697099\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:19 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:19 INFO 140029106595200] Epoch[12] Batch[0] avg_epoch_loss=4.059619\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:19 INFO 140029106595200] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=4.059618949890137\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:20 INFO 140029106595200] Epoch[12] Batch[5] avg_epoch_loss=4.200552\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:20 INFO 140029106595200] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=4.200551509857178\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:20 INFO 140029106595200] Epoch[12] Batch [5]#011Speed: 318.67 samples/sec#011loss=4.200552\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:21 INFO 140029106595200] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334259.009256, \"EndTime\": 1616334261.3205047, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2310.7635974884033, \"count\": 1, \"min\": 2310.7635974884033, \"max\": 2310.7635974884033}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:21 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=274.7824511588467 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:21 INFO 140029106595200] #progress_metric: host=algo-1, completed 32.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:21 INFO 140029106595200] #quality_metric: host=algo-1, epoch=12, train loss <loss>=4.232312870025635\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:21 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:21 INFO 140029106595200] Epoch[13] Batch[0] avg_epoch_loss=4.215644\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:21 INFO 140029106595200] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=4.215643882751465\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:22 INFO 140029106595200] Epoch[13] Batch[5] avg_epoch_loss=4.132415\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:22 INFO 140029106595200] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=4.1324147780736284\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:22 INFO 140029106595200] Epoch[13] Batch [5]#011Speed: 318.59 samples/sec#011loss=4.132415\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:23 INFO 140029106595200] Epoch[13] Batch[10] avg_epoch_loss=3.967403\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:23 INFO 140029106595200] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=3.769388437271118\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:23 INFO 140029106595200] Epoch[13] Batch [10]#011Speed: 305.84 samples/sec#011loss=3.769388\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:23 INFO 140029106595200] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334261.3206115, \"EndTime\": 1616334263.9306111, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2609.447240829468, \"count\": 1, \"min\": 2609.447240829468, \"max\": 2609.447240829468}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:23 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=250.99718621984104 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:23 INFO 140029106595200] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:23 INFO 140029106595200] #quality_metric: host=algo-1, epoch=13, train loss <loss>=3.9674028049815786\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:23 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:24 INFO 140029106595200] Epoch[14] Batch[0] avg_epoch_loss=3.631428\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:24 INFO 140029106595200] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=3.631427526473999\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:25 INFO 140029106595200] Epoch[14] Batch[5] avg_epoch_loss=4.000258\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:25 INFO 140029106595200] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=4.000257611274719\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:25 INFO 140029106595200] Epoch[14] Batch [5]#011Speed: 319.55 samples/sec#011loss=4.000258\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:26 INFO 140029106595200] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334263.9307108, \"EndTime\": 1616334266.3082106, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2376.974105834961, \"count\": 1, \"min\": 2376.974105834961, \"max\": 2376.974105834961}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:26 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=267.97299098863806 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:26 INFO 140029106595200] #progress_metric: host=algo-1, completed 37.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:26 INFO 140029106595200] #quality_metric: host=algo-1, epoch=14, train loss <loss>=4.0146321773529055\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:26 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:26 INFO 140029106595200] Epoch[15] Batch[0] avg_epoch_loss=4.365823\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:26 INFO 140029106595200] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=4.365822792053223\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:27 INFO 140029106595200] Epoch[15] Batch[5] avg_epoch_loss=3.854138\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:27 INFO 140029106595200] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=3.8541380564371743\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:27 INFO 140029106595200] Epoch[15] Batch [5]#011Speed: 316.22 samples/sec#011loss=3.854138\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:28 INFO 140029106595200] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334266.3083038, \"EndTime\": 1616334268.64448, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2335.615396499634, \"count\": 1, \"min\": 2335.615396499634, \"max\": 2335.615396499634}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:28 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=261.1542361480414 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:28 INFO 140029106595200] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:28 INFO 140029106595200] #quality_metric: host=algo-1, epoch=15, train loss <loss>=4.0213217496871945\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:28 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:29 INFO 140029106595200] Epoch[16] Batch[0] avg_epoch_loss=4.150332\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:29 INFO 140029106595200] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=4.150331974029541\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:30 INFO 140029106595200] Epoch[16] Batch[5] avg_epoch_loss=3.928570\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:30 INFO 140029106595200] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=3.928569515546163\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:30 INFO 140029106595200] Epoch[16] Batch [5]#011Speed: 321.15 samples/sec#011loss=3.928570\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:30 INFO 140029106595200] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334268.644595, \"EndTime\": 1616334270.956563, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2311.3794326782227, \"count\": 1, \"min\": 2311.3794326782227, \"max\": 2311.3794326782227}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:30 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=271.68532470549115 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:30 INFO 140029106595200] #progress_metric: host=algo-1, completed 42.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:30 INFO 140029106595200] #quality_metric: host=algo-1, epoch=16, train loss <loss>=3.9457976102828978\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:30 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:31 INFO 140029106595200] Epoch[17] Batch[0] avg_epoch_loss=4.218556\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:31 INFO 140029106595200] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=4.2185564041137695\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:32 INFO 140029106595200] Epoch[17] Batch[5] avg_epoch_loss=4.179475\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:32 INFO 140029106595200] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=4.179474592208862\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:32 INFO 140029106595200] Epoch[17] Batch [5]#011Speed: 325.27 samples/sec#011loss=4.179475\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:33 INFO 140029106595200] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334270.956641, \"EndTime\": 1616334273.2440634, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2286.858320236206, \"count\": 1, \"min\": 2286.858320236206, \"max\": 2286.858320236206}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:33 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=279.40353580712303 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:33 INFO 140029106595200] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:33 INFO 140029106595200] #quality_metric: host=algo-1, epoch=17, train loss <loss>=4.190499019622803\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:33 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:33 INFO 140029106595200] Epoch[18] Batch[0] avg_epoch_loss=3.973765\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:33 INFO 140029106595200] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=3.9737648963928223\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:34 INFO 140029106595200] Epoch[18] Batch[5] avg_epoch_loss=3.942426\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:34 INFO 140029106595200] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=3.942425847053528\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:34 INFO 140029106595200] Epoch[18] Batch [5]#011Speed: 315.70 samples/sec#011loss=3.942426\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:35 INFO 140029106595200] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334273.244172, \"EndTime\": 1616334275.5643413, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2319.563865661621, \"count\": 1, \"min\": 2319.563865661621, \"max\": 2319.563865661621}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:35 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=274.6017367166501 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:35 INFO 140029106595200] #progress_metric: host=algo-1, completed 47.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:35 INFO 140029106595200] #quality_metric: host=algo-1, epoch=18, train loss <loss>=4.011763691902161\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:35 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:36 INFO 140029106595200] Epoch[19] Batch[0] avg_epoch_loss=3.879777\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:36 INFO 140029106595200] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=3.879777431488037\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:37 INFO 140029106595200] Epoch[19] Batch[5] avg_epoch_loss=4.004056\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:37 INFO 140029106595200] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=4.004056294759114\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:37 INFO 140029106595200] Epoch[19] Batch [5]#011Speed: 320.55 samples/sec#011loss=4.004056\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:37 INFO 140029106595200] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334275.5644503, \"EndTime\": 1616334277.9013271, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2336.2443447113037, \"count\": 1, \"min\": 2336.2443447113037, \"max\": 2336.2443447113037}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:37 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=269.6446418718628 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:37 INFO 140029106595200] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:37 INFO 140029106595200] #quality_metric: host=algo-1, epoch=19, train loss <loss>=3.9421341180801392\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:37 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:38 INFO 140029106595200] Epoch[20] Batch[0] avg_epoch_loss=4.050661\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:38 INFO 140029106595200] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=4.050661087036133\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:39 INFO 140029106595200] Epoch[20] Batch[5] avg_epoch_loss=3.870738\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:39 INFO 140029106595200] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=3.8707383076349893\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:39 INFO 140029106595200] Epoch[20] Batch [5]#011Speed: 314.57 samples/sec#011loss=3.870738\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:40 INFO 140029106595200] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334277.901439, \"EndTime\": 1616334280.2441046, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2342.0701026916504, \"count\": 1, \"min\": 2342.0701026916504, \"max\": 2342.0701026916504}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:40 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=271.9657473024789 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:40 INFO 140029106595200] #progress_metric: host=algo-1, completed 52.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:40 INFO 140029106595200] #quality_metric: host=algo-1, epoch=20, train loss <loss>=3.9230648040771485\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:40 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:40 INFO 140029106595200] Epoch[21] Batch[0] avg_epoch_loss=4.783178\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:40 INFO 140029106595200] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=4.783178329467773\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:41 INFO 140029106595200] Epoch[21] Batch[5] avg_epoch_loss=4.157700\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:41 INFO 140029106595200] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=4.157700260480245\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:41 INFO 140029106595200] Epoch[21] Batch [5]#011Speed: 312.83 samples/sec#011loss=4.157700\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:42 INFO 140029106595200] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334280.2441974, \"EndTime\": 1616334282.6147285, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2369.9264526367188, \"count\": 1, \"min\": 2369.9264526367188, \"max\": 2369.9264526367188}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:42 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=262.43953455222334 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:42 INFO 140029106595200] #progress_metric: host=algo-1, completed 55.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:42 INFO 140029106595200] #quality_metric: host=algo-1, epoch=21, train loss <loss>=4.202433300018311\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:42 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:43 INFO 140029106595200] Epoch[22] Batch[0] avg_epoch_loss=4.121017\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:43 INFO 140029106595200] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=4.121016502380371\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:44 INFO 140029106595200] Epoch[22] Batch[5] avg_epoch_loss=3.957936\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:44 INFO 140029106595200] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=3.9579356908798218\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:44 INFO 140029106595200] Epoch[22] Batch [5]#011Speed: 306.48 samples/sec#011loss=3.957936\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:45 INFO 140029106595200] Epoch[22] Batch[10] avg_epoch_loss=3.891819\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:45 INFO 140029106595200] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=3.8124786853790282\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:45 INFO 140029106595200] Epoch[22] Batch [10]#011Speed: 301.24 samples/sec#011loss=3.812479\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:45 INFO 140029106595200] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334282.614827, \"EndTime\": 1616334285.2370827, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2621.589183807373, \"count\": 1, \"min\": 2621.589183807373, \"max\": 2621.589183807373}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:45 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=248.69042492020304 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:45 INFO 140029106595200] #progress_metric: host=algo-1, completed 57.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:45 INFO 140029106595200] #quality_metric: host=algo-1, epoch=22, train loss <loss>=3.891818870197643\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:45 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:45 INFO 140029106595200] Epoch[23] Batch[0] avg_epoch_loss=3.476131\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:45 INFO 140029106595200] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=3.476130723953247\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:46 INFO 140029106595200] Epoch[23] Batch[5] avg_epoch_loss=3.893015\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:46 INFO 140029106595200] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=3.8930153052012124\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:46 INFO 140029106595200] Epoch[23] Batch [5]#011Speed: 308.06 samples/sec#011loss=3.893015\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:47 INFO 140029106595200] processed a total of 599 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334285.2371824, \"EndTime\": 1616334287.6522126, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2414.4575595855713, \"count\": 1, \"min\": 2414.4575595855713, \"max\": 2414.4575595855713}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:47 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=248.0766019624449 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:47 INFO 140029106595200] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:47 INFO 140029106595200] #quality_metric: host=algo-1, epoch=23, train loss <loss>=4.215986061096191\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:47 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:48 INFO 140029106595200] Epoch[24] Batch[0] avg_epoch_loss=3.937620\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:48 INFO 140029106595200] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=3.937619686126709\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:49 INFO 140029106595200] Epoch[24] Batch[5] avg_epoch_loss=3.973863\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:49 INFO 140029106595200] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=3.9738625288009644\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:49 INFO 140029106595200] Epoch[24] Batch [5]#011Speed: 307.98 samples/sec#011loss=3.973863\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:50 INFO 140029106595200] Epoch[24] Batch[10] avg_epoch_loss=3.914503\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:50 INFO 140029106595200] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=3.843272495269775\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:50 INFO 140029106595200] Epoch[24] Batch [10]#011Speed: 295.66 samples/sec#011loss=3.843272\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:50 INFO 140029106595200] processed a total of 701 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334287.652288, \"EndTime\": 1616334290.288175, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2635.305881500244, \"count\": 1, \"min\": 2635.305881500244, \"max\": 2635.305881500244}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:50 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=265.9885736100927 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:50 INFO 140029106595200] #progress_metric: host=algo-1, completed 62.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:50 INFO 140029106595200] #quality_metric: host=algo-1, epoch=24, train loss <loss>=3.914503422650424\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:50 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:50 INFO 140029106595200] Epoch[25] Batch[0] avg_epoch_loss=4.203691\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:50 INFO 140029106595200] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=4.203690528869629\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:51 INFO 140029106595200] Epoch[25] Batch[5] avg_epoch_loss=4.002783\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:51 INFO 140029106595200] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=4.002782781918843\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:51 INFO 140029106595200] Epoch[25] Batch [5]#011Speed: 310.46 samples/sec#011loss=4.002783\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:52 INFO 140029106595200] Epoch[25] Batch[10] avg_epoch_loss=3.864467\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:52 INFO 140029106595200] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=3.6984888076782227\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:52 INFO 140029106595200] Epoch[25] Batch [10]#011Speed: 295.89 samples/sec#011loss=3.698489\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:52 INFO 140029106595200] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334290.2882755, \"EndTime\": 1616334292.975999, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2687.1790885925293, \"count\": 1, \"min\": 2687.1790885925293, \"max\": 2687.1790885925293}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:52 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=244.480385198476 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:52 INFO 140029106595200] #progress_metric: host=algo-1, completed 65.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:52 INFO 140029106595200] #quality_metric: host=algo-1, epoch=25, train loss <loss>=3.8644673390821977\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:52 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:53 INFO 140029106595200] Epoch[26] Batch[0] avg_epoch_loss=4.001656\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:53 INFO 140029106595200] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=4.0016560554504395\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:54 INFO 140029106595200] Epoch[26] Batch[5] avg_epoch_loss=4.012746\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:54 INFO 140029106595200] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=4.012745976448059\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:54 INFO 140029106595200] Epoch[26] Batch [5]#011Speed: 280.30 samples/sec#011loss=4.012746\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:55 INFO 140029106595200] Epoch[26] Batch[10] avg_epoch_loss=3.851607\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:55 INFO 140029106595200] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=3.6582395792007447\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:55 INFO 140029106595200] Epoch[26] Batch [10]#011Speed: 285.76 samples/sec#011loss=3.658240\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:55 INFO 140029106595200] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334292.9760993, \"EndTime\": 1616334295.776094, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2799.4587421417236, \"count\": 1, \"min\": 2799.4587421417236, \"max\": 2799.4587421417236}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:55 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=234.67769096160652 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:55 INFO 140029106595200] #progress_metric: host=algo-1, completed 67.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:55 INFO 140029106595200] #quality_metric: host=algo-1, epoch=26, train loss <loss>=3.851606704972007\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:55 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:56 INFO 140029106595200] Epoch[27] Batch[0] avg_epoch_loss=3.957326\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:56 INFO 140029106595200] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=3.9573264122009277\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:57 INFO 140029106595200] Epoch[27] Batch[5] avg_epoch_loss=3.925474\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:57 INFO 140029106595200] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=3.925474484761556\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:57 INFO 140029106595200] Epoch[27] Batch [5]#011Speed: 306.39 samples/sec#011loss=3.925474\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:58 INFO 140029106595200] Epoch[27] Batch[10] avg_epoch_loss=4.060841\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:58 INFO 140029106595200] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=4.223280334472657\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:58 INFO 140029106595200] Epoch[27] Batch [10]#011Speed: 304.18 samples/sec#011loss=4.223280\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:58 INFO 140029106595200] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334295.7761743, \"EndTime\": 1616334298.397018, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2620.2895641326904, \"count\": 1, \"min\": 2620.2895641326904, \"max\": 2620.2895641326904}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:58 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=249.19542595898022 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:58 INFO 140029106595200] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:58 INFO 140029106595200] #quality_metric: host=algo-1, epoch=27, train loss <loss>=4.060840780084783\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:58 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:58 INFO 140029106595200] Epoch[28] Batch[0] avg_epoch_loss=4.270971\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:44:58 INFO 140029106595200] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=4.270970821380615\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:00 INFO 140029106595200] Epoch[28] Batch[5] avg_epoch_loss=3.893970\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:00 INFO 140029106595200] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=3.8939698139826455\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:00 INFO 140029106595200] Epoch[28] Batch [5]#011Speed: 301.69 samples/sec#011loss=3.893970\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:00 INFO 140029106595200] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334298.3971162, \"EndTime\": 1616334300.8546686, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2456.998348236084, \"count\": 1, \"min\": 2456.998348236084, \"max\": 2456.998348236084}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:00 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=257.207932537549 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:00 INFO 140029106595200] #progress_metric: host=algo-1, completed 72.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:00 INFO 140029106595200] #quality_metric: host=algo-1, epoch=28, train loss <loss>=3.8292006969451906\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:00 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:01 INFO 140029106595200] Epoch[29] Batch[0] avg_epoch_loss=4.100357\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:01 INFO 140029106595200] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=4.100356578826904\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:02 INFO 140029106595200] Epoch[29] Batch[5] avg_epoch_loss=3.964063\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:02 INFO 140029106595200] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=3.9640634457270303\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:02 INFO 140029106595200] Epoch[29] Batch [5]#011Speed: 272.04 samples/sec#011loss=3.964063\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:03 INFO 140029106595200] processed a total of 594 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334300.8547764, \"EndTime\": 1616334303.39876, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2543.4112548828125, \"count\": 1, \"min\": 2543.4112548828125, \"max\": 2543.4112548828125}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:03 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=233.53047245879125 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:03 INFO 140029106595200] #progress_metric: host=algo-1, completed 75.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:03 INFO 140029106595200] #quality_metric: host=algo-1, epoch=29, train loss <loss>=3.898428177833557\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:03 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:03 INFO 140029106595200] Epoch[30] Batch[0] avg_epoch_loss=3.984275\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:03 INFO 140029106595200] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=3.9842753410339355\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:05 INFO 140029106595200] Epoch[30] Batch[5] avg_epoch_loss=4.039103\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:05 INFO 140029106595200] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=4.039102832476298\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:05 INFO 140029106595200] Epoch[30] Batch [5]#011Speed: 265.46 samples/sec#011loss=4.039103\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:06 INFO 140029106595200] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334303.398867, \"EndTime\": 1616334306.2365406, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2836.9693756103516, \"count\": 1, \"min\": 2836.9693756103516, \"max\": 2836.9693756103516}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:06 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=221.35031357268147 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:06 INFO 140029106595200] #progress_metric: host=algo-1, completed 77.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:06 INFO 140029106595200] #quality_metric: host=algo-1, epoch=30, train loss <loss>=4.001586627960205\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:06 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:06 INFO 140029106595200] Epoch[31] Batch[0] avg_epoch_loss=3.495108\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:06 INFO 140029106595200] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=3.4951083660125732\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:07 INFO 140029106595200] Epoch[31] Batch[5] avg_epoch_loss=4.161833\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:07 INFO 140029106595200] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=4.161833047866821\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:07 INFO 140029106595200] Epoch[31] Batch [5]#011Speed: 306.87 samples/sec#011loss=4.161833\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:08 INFO 140029106595200] Epoch[31] Batch[10] avg_epoch_loss=4.044336\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:08 INFO 140029106595200] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=3.903338813781738\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:08 INFO 140029106595200] Epoch[31] Batch [10]#011Speed: 306.65 samples/sec#011loss=3.903339\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:08 INFO 140029106595200] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334306.2366545, \"EndTime\": 1616334308.935513, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2698.244333267212, \"count\": 1, \"min\": 2698.244333267212, \"max\": 2698.244333267212}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:08 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=237.92015228342078 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:08 INFO 140029106595200] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:08 INFO 140029106595200] #quality_metric: host=algo-1, epoch=31, train loss <loss>=4.044335668737238\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:08 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:09 INFO 140029106595200] Epoch[32] Batch[0] avg_epoch_loss=4.188236\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:09 INFO 140029106595200] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=4.188235759735107\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:10 INFO 140029106595200] Epoch[32] Batch[5] avg_epoch_loss=4.131644\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:10 INFO 140029106595200] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=4.131643811861674\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:10 INFO 140029106595200] Epoch[32] Batch [5]#011Speed: 306.58 samples/sec#011loss=4.131644\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:11 INFO 140029106595200] Epoch[32] Batch[10] avg_epoch_loss=4.078481\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:11 INFO 140029106595200] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=4.014684963226318\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:11 INFO 140029106595200] Epoch[32] Batch [10]#011Speed: 304.85 samples/sec#011loss=4.014685\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:11 INFO 140029106595200] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334308.9356074, \"EndTime\": 1616334311.5598464, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2623.7094402313232, \"count\": 1, \"min\": 2623.7094402313232, \"max\": 2623.7094402313232}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:11 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=250.014213608296 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:11 INFO 140029106595200] #progress_metric: host=algo-1, completed 82.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:11 INFO 140029106595200] #quality_metric: host=algo-1, epoch=32, train loss <loss>=4.0784806988456035\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:11 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:12 INFO 140029106595200] Epoch[33] Batch[0] avg_epoch_loss=3.303533\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:12 INFO 140029106595200] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=3.303532838821411\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:13 INFO 140029106595200] Epoch[33] Batch[5] avg_epoch_loss=3.883796\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:13 INFO 140029106595200] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=3.883795658747355\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:13 INFO 140029106595200] Epoch[33] Batch [5]#011Speed: 306.71 samples/sec#011loss=3.883796\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:13 INFO 140029106595200] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334311.5599449, \"EndTime\": 1616334313.9960766, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2435.610771179199, \"count\": 1, \"min\": 2435.610771179199, \"max\": 2435.610771179199}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:13 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=246.73963119679908 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:13 INFO 140029106595200] #progress_metric: host=algo-1, completed 85.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:13 INFO 140029106595200] #quality_metric: host=algo-1, epoch=33, train loss <loss>=4.14630389213562\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:13 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:14 INFO 140029106595200] Epoch[34] Batch[0] avg_epoch_loss=3.373212\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:14 INFO 140029106595200] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=3.373211622238159\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:15 INFO 140029106595200] Epoch[34] Batch[5] avg_epoch_loss=3.825281\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:15 INFO 140029106595200] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=3.8252805868784585\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:15 INFO 140029106595200] Epoch[34] Batch [5]#011Speed: 302.07 samples/sec#011loss=3.825281\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:16 INFO 140029106595200] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334313.9961832, \"EndTime\": 1616334316.42349, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2426.7399311065674, \"count\": 1, \"min\": 2426.7399311065674, \"max\": 2426.7399311065674}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:16 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=261.6511378468393 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:16 INFO 140029106595200] #progress_metric: host=algo-1, completed 87.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:16 INFO 140029106595200] #quality_metric: host=algo-1, epoch=34, train loss <loss>=3.874427819252014\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:16 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:16 INFO 140029106595200] Epoch[35] Batch[0] avg_epoch_loss=4.013283\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:16 INFO 140029106595200] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=4.0132832527160645\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:17 INFO 140029106595200] Epoch[35] Batch[5] avg_epoch_loss=3.650509\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:17 INFO 140029106595200] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=3.6505085229873657\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:17 INFO 140029106595200] Epoch[35] Batch [5]#011Speed: 308.05 samples/sec#011loss=3.650509\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:18 INFO 140029106595200] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334316.4235978, \"EndTime\": 1616334318.8266952, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2402.4910926818848, \"count\": 1, \"min\": 2402.4910926818848, \"max\": 2402.4910926818848}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:18 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=257.21901544119936 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:18 INFO 140029106595200] #progress_metric: host=algo-1, completed 90.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:18 INFO 140029106595200] #quality_metric: host=algo-1, epoch=35, train loss <loss>=3.6528796911239625\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:18 INFO 140029106595200] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:18 INFO 140029106595200] Saved checkpoint to \"/opt/ml/model/state_6d1c5b35-9560-4440-b991-da6924ca93f1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334318.8267858, \"EndTime\": 1616334318.887386, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 59.97776985168457, \"count\": 1, \"min\": 59.97776985168457, \"max\": 59.97776985168457}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:19 INFO 140029106595200] Epoch[36] Batch[0] avg_epoch_loss=3.654273\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:19 INFO 140029106595200] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=3.654273271560669\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:20 INFO 140029106595200] Epoch[36] Batch[5] avg_epoch_loss=3.926336\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:20 INFO 140029106595200] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=3.926336129506429\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:20 INFO 140029106595200] Epoch[36] Batch [5]#011Speed: 301.59 samples/sec#011loss=3.926336\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:21 INFO 140029106595200] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334318.8874888, \"EndTime\": 1616334321.2952235, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2407.6454639434814, \"count\": 1, \"min\": 2407.6454639434814, \"max\": 2407.6454639434814}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:21 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=264.97194212448017 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:21 INFO 140029106595200] #progress_metric: host=algo-1, completed 92.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:21 INFO 140029106595200] #quality_metric: host=algo-1, epoch=36, train loss <loss>=3.883797311782837\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:21 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:21 INFO 140029106595200] Epoch[37] Batch[0] avg_epoch_loss=4.273978\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:21 INFO 140029106595200] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=4.273977756500244\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:22 INFO 140029106595200] Epoch[37] Batch[5] avg_epoch_loss=3.988376\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:22 INFO 140029106595200] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=3.9883755445480347\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:22 INFO 140029106595200] Epoch[37] Batch [5]#011Speed: 311.32 samples/sec#011loss=3.988376\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:23 INFO 140029106595200] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334321.295332, \"EndTime\": 1616334323.6933305, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2397.4316120147705, \"count\": 1, \"min\": 2397.4316120147705, \"max\": 2397.4316120147705}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:23 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=256.0899134905637 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:23 INFO 140029106595200] #progress_metric: host=algo-1, completed 95.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:23 INFO 140029106595200] #quality_metric: host=algo-1, epoch=37, train loss <loss>=3.993954801559448\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:23 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:24 INFO 140029106595200] Epoch[38] Batch[0] avg_epoch_loss=4.153577\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:24 INFO 140029106595200] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=4.153576850891113\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:25 INFO 140029106595200] Epoch[38] Batch[5] avg_epoch_loss=3.930937\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:25 INFO 140029106595200] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=3.930936813354492\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:25 INFO 140029106595200] Epoch[38] Batch [5]#011Speed: 289.58 samples/sec#011loss=3.930937\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:26 INFO 140029106595200] Epoch[38] Batch[10] avg_epoch_loss=3.805214\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:26 INFO 140029106595200] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=3.6543468475341796\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:26 INFO 140029106595200] Epoch[38] Batch [10]#011Speed: 297.61 samples/sec#011loss=3.654347\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:26 INFO 140029106595200] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334323.6934469, \"EndTime\": 1616334326.4011393, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2707.1220874786377, \"count\": 1, \"min\": 2707.1220874786377, \"max\": 2707.1220874786377}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:26 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=243.41537538618647 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:26 INFO 140029106595200] #progress_metric: host=algo-1, completed 97.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:26 INFO 140029106595200] #quality_metric: host=algo-1, epoch=38, train loss <loss>=3.8052141016179863\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:26 INFO 140029106595200] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:26 INFO 140029106595200] Epoch[39] Batch[0] avg_epoch_loss=4.027485\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:26 INFO 140029106595200] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=4.027484893798828\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:28 INFO 140029106595200] Epoch[39] Batch[5] avg_epoch_loss=3.745990\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:28 INFO 140029106595200] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=3.7459896008173623\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:28 INFO 140029106595200] Epoch[39] Batch [5]#011Speed: 311.67 samples/sec#011loss=3.745990\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:29 INFO 140029106595200] Epoch[39] Batch[10] avg_epoch_loss=3.539871\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:29 INFO 140029106595200] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=3.2925284147262572\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:29 INFO 140029106595200] Epoch[39] Batch [10]#011Speed: 303.25 samples/sec#011loss=3.292528\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:29 INFO 140029106595200] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334326.401277, \"EndTime\": 1616334329.0605075, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2658.7042808532715, \"count\": 1, \"min\": 2658.7042808532715, \"max\": 2658.7042808532715}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:29 INFO 140029106595200] #throughput_metric: host=algo-1, train throughput=245.21888974204765 records/second\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:29 INFO 140029106595200] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:29 INFO 140029106595200] #quality_metric: host=algo-1, epoch=39, train loss <loss>=3.5398708798668603\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:29 INFO 140029106595200] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:29 INFO 140029106595200] Saved checkpoint to \"/opt/ml/model/state_7554de1d-942e-46fc-81fc-0495ad776d03-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334329.0606067, \"EndTime\": 1616334329.123115, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 61.95712089538574, \"count\": 1, \"min\": 61.95712089538574, \"max\": 61.95712089538574}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:29 INFO 140029106595200] Final loss: 3.5398708798668603 (occurred at epoch 39)\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:29 INFO 140029106595200] #quality_metric: host=algo-1, train final_loss <loss>=3.5398708798668603\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:29 INFO 140029106595200] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:29 WARNING 140029106595200] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:29 INFO 140029106595200] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334329.1232152, \"EndTime\": 1616334329.9144528, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 790.5123233795166, \"count\": 1, \"min\": 790.5123233795166, \"max\": 790.5123233795166}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:30 INFO 140029106595200] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334329.9145572, \"EndTime\": 1616334330.2102833, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 1086.411952972412, \"count\": 1, \"min\": 1086.411952972412, \"max\": 1086.411952972412}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:30 INFO 140029106595200] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:30 INFO 140029106595200] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334330.2103724, \"EndTime\": 1616334330.2488122, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 38.38920593261719, \"count\": 1, \"min\": 38.38920593261719, \"max\": 38.38920593261719}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:30 INFO 140029106595200] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:30 INFO 140029106595200] #memory_usage::<batchbuffer> = 20.827598571777344 mb\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:30 INFO 140029106595200] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334330.2488773, \"EndTime\": 1616334330.2499468, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.bind.time\": {\"sum\": 0.05054473876953125, \"count\": 1, \"min\": 0.05054473876953125, \"max\": 0.05054473876953125}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334330.250011, \"EndTime\": 1616334333.8365755, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.score.time\": {\"sum\": 3586.674451828003, \"count\": 1, \"min\": 3586.674451828003, \"max\": 3586.674451828003}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:33 INFO 140029106595200] #test_score (algo-1, RMSE): 216.042631964649\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:33 INFO 140029106595200] #test_score (algo-1, mean_absolute_QuantileLoss): 118808.83497857659\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:33 INFO 140029106595200] #test_score (algo-1, mean_wQuantileLoss): 0.2911408378141932\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:33 INFO 140029106595200] #test_score (algo-1, wQuantileLoss[0.1]): 0.1496519346888989\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:33 INFO 140029106595200] #test_score (algo-1, wQuantileLoss[0.2]): 0.235987247800647\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:33 INFO 140029106595200] #test_score (algo-1, wQuantileLoss[0.3]): 0.30946328793298783\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:33 INFO 140029106595200] #test_score (algo-1, wQuantileLoss[0.4]): 0.3570949763208455\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:33 INFO 140029106595200] #test_score (algo-1, wQuantileLoss[0.5]): 0.3801846477587413\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:33 INFO 140029106595200] #test_score (algo-1, wQuantileLoss[0.6]): 0.3743851977656273\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:33 INFO 140029106595200] #test_score (algo-1, wQuantileLoss[0.7]): 0.3379790960625166\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:33 INFO 140029106595200] #test_score (algo-1, wQuantileLoss[0.8]): 0.28247208628123194\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:33 INFO 140029106595200] #test_score (algo-1, wQuantileLoss[0.9]): 0.19304906571624228\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:33 INFO 140029106595200] #quality_metric: host=algo-1, test RMSE <loss>=216.042631964649\u001b[0m\n",
      "\u001b[34m[03/21/2021 13:45:33 INFO 140029106595200] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.2911408378141932\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616334333.8367028, \"EndTime\": 1616334333.9037337, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 7.695674896240234, \"count\": 1, \"min\": 7.695674896240234, \"max\": 7.695674896240234}, \"totaltime\": {\"sum\": 108153.83696556091, \"count\": 1, \"min\": 108153.83696556091, \"max\": 108153.83696556091}}}\n",
      "\u001b[0m\n",
      "\n",
      "2021-03-21 13:45:43 Uploading - Uploading generated training model\n",
      "2021-03-21 13:45:43 Completed - Training job completed\n",
      "Training seconds: 173\n",
      "Billable seconds: 173\n",
      "CPU times: user 869 ms, sys: 37.9 ms, total: 906 ms\n",
      "Wall time: 5min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": \"{}/train/\".format(s3_output_path),\n",
    "    \"test\": \"{}/test/\".format(s3_output_path)\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you pass a test set in this example, accuracy metrics for the forecast are computed and logged (see bottom of the log).\n",
    "You can find the definition of these metrics from [our documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html). You can use these to optimize the parameters and tune your model or use SageMaker's [Automated Model Tuning service](https://aws.amazon.com/blogs/aws/sagemaker-automatic-model-tuning/) to tune the model for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint and predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained model, we can use it to perform predictions by deploying it to an endpoint.\n",
    "\n",
    "**Note: Remember to delete the endpoint after running this experiment. A cell at the very bottom of this notebook will do that: make sure you run it at the end.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query the endpoint and perform predictions, we can define the following utility class: this allows making requests using `pandas.Series` objects rather than raw JSON strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, content_type=\"application/json\", **kwargs)\n",
    "        \n",
    "    def predict(self, ts, cat=None, dynamic_feat=None, \n",
    "                num_samples=100, return_samples=False, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + pd.Timedelta(1, unit='D')\n",
    "\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None)\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n",
    "    \n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode('utf-8'))['predictions'][0]\n",
    "        prediction_length = len(next(iter(predictions['quantiles'].values())))\n",
    "        #prediction_index = pd.DatetimeIndex(start=prediction_time, freq=freq, periods=prediction_length)\n",
    "        prediction_index = pd.date_range(start=prediction_time, freq=freq, periods=prediction_length)\n",
    "        if return_samples:\n",
    "            dict_of_samples = {'sample_' + str(i): s for i, s in enumerate(predictions['samples'])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(data={**predictions['quantiles'], **dict_of_samples}, index=prediction_index)\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "        \n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]        \n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can deploy the model and create and endpoint that can be queried using our custom DeepARPredictor class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!"
     ]
    }
   ],
   "source": [
    "job_name = estimator.latest_training_job.name\n",
    "\n",
    "endpoint_name = sagemaker_session.endpoint_from_job(\n",
    "    job_name=job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    image_uri=image_uri,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "# predictor = estimator.deploy(\n",
    "#     initial_instance_count=1,\n",
    "#     instance_type='ml.m4.xlarge',\n",
    "#     predictor_cls=DeepARPredictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class RealTimePredictor has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "content_type is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import json_serializer\n",
    "from sagemaker.predictor import json_deserializer\n",
    "predictor = DeepARPredictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session, serializer=json_serializer, deserializer=json_deserializer)\n",
    "#predictor.set_prediction_parameters(freq, prediction_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `predictor` object to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The json_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type 'bytes' is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-70089bd5e56c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrespDF\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeseries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.90\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-43b370b0d9c0>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mquantiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__encode_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDeepARPredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__decode_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         request_args = self._create_request_args(\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         )\n\u001b[1;32m    131\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36m_create_request_args\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"InferenceId\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Body\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/deprecations.py\u001b[0m in \u001b[0;36mdeprecate\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mrenamed_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeprecate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/serializers.py\u001b[0m in \u001b[0;36mserialize\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mseparators\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         default is None and not sort_keys and not kw):\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m    179\u001b[0m         raise TypeError(\"Object of type '%s' is not JSON serializable\" %\n\u001b[0;32m--> 180\u001b[0;31m                         o.__class__.__name__)\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type 'bytes' is not JSON serializable"
     ]
    }
   ],
   "source": [
    "respDF=predictor.predict(ts=timeseries[4], quantiles=[0.90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing predicted cost for next 10 hours  at 0.9 quantile. Please note, the data is generated using Device simulators and not recorded by actual sensors. Predicted cost can sometime display negative numbers, which will not be the case in case of real life scenario. Power company will never credit money for low consumption!!!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'respDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-3fe8d5fc5d0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrespDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'respDF' is not defined"
     ]
    }
   ],
   "source": [
    "respDF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
